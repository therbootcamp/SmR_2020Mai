---
title: "Linear Models"
author: "Statistics with R<br>
  <a href='https://therbootcamp.github.io'>
    Basel R Bootcamp
  </a>
  <br>
  <a href='https://therbootcamp.github.io/SwR_2019Apr/'>
    <i class='fas fa-clock' style='font-size:.9em;'></i>
  </a>&#8239; 
  <a href='https://therbootcamp.github.io'>
    <i class='fas fa-home' style='font-size:.9em;' ></i>
  </a>&#8239;
  <a href='mailto:therbootcamp@gmail.com'>
    <i class='fas fa-envelope' style='font-size: .9em;'></i>
  </a>&#8239;
  <a href='https://www.linkedin.com/company/basel-r-bootcamp/'>
    <i class='fab fa-linkedin' style='font-size: .9em;'></i>
  </a>"
date: "April 2019"
output:
  xaringan::moon_reader:
    css: ["default", "baselrbootcamp.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
---

layout: true

<div class="my-footer">
  <span style="text-align:center">
    <span> 
      <img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/by-sa.png" height=14 style="vertical-align: middle"/>
    </span>
    <a href="https://therbootcamp.github.io/">
      <span style="padding-left:82px"> 
        <font color="#7E7E7E">
          www.therbootcamp.com
        </font>
      </span>
    </a>
    <a href="https://therbootcamp.github.io/">
      <font color="#7E7E7E">
       Statistics with R | April 2019
      </font>
    </a>
    </span>
  </div> 

---

```{r, eval = TRUE, echo = FALSE, warning=F,message=F}
# Code to knit slides

```

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
# see: https://github.com/yihui/xaringan
# install.packages("xaringan")
# see: 
# https://github.com/yihui/xaringan/wiki
# https://github.com/gnab/remark/wiki/Markdown
options(width = 110)
options(digits = 4)

# Load packages
require(tidyverse)


# Load data
baselers <- readr::read_csv("1_Data/baselers.csv")

# get color palette functions
source("https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_materials/code/baselrbootcamp_palettes.R")

knitr::opts_chunk$set(dpi = 300, echo = FALSE, warning = FALSE, fig.align = 'center')
```


# Linear Models

.pull-left5[

Linear models are, by far, the most important models in all of statistics.

As we will see today, many statistical tests you may know of are special types of linear models.

Why are linear models so great?

- They are <high>easy to interpret</high>.
- They can <high>approximate non-linear</high> data well.
- They are <high>easy to calculate</high> and impliment (just addition and multiplication!).
- They <high>just work</high>!

]

.pull-right45[


$$\Large income = 885 + age \times 149.3$$

```{r, echo = FALSE, fig.width = 3.5, fig.height = 3}

mod <- lm(income ~ age, data = baselers)
my_coef <- coef(mod)

simple_scatter <- ggplot(baselers %>% filter(age < 80) %>% slice(1:100), aes(age, income)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, col = baselrbootcamp_cols("magenta")) +
  theme_bw()

simple_scatter
```


]

---

.pull-left6[

# What is a linear model?

A linear model is one that can be written in the following form <br>

*Version 1*

$$\huge Y = \beta_{0} + \beta_{1}x_{1} + \beta_{2} x_{2} +  ... + \beta_{n}x_{n} + \epsilon$$


*Version 2*


$$\huge Y = \beta_{0} + \sum_{i=1}^{n}\beta_{i}x_{i}+ \epsilon$$

In the end, a linear model is <high>just addition and multiplication</high>! 

]

.pull-right35[

<br><br><br>

```{r, echo = FALSE}
knitr::include_graphics("https://66.media.tumblr.com/6e401f0cffebf9c7440894d4fa6c48b2/tumblr_o0af44kS071u7w7kwo1_1280.jpg")
```


]

---

.pull-left5[

# Simple Linear Regression


<high>Definition</high>: Simple linear regression is a linear model with one predictor $x$, and where the error term $\epsilon$ is Normally distributed.


$$\huge Y=\beta_{0} + \beta_{1} x + \epsilon$$

|Parameter|Description|In words|
|:------|:-----|:-----|
|$$\beta_{0}$$| Intercept|When $x = 0$, what is the predicted value for $Y$?|
|$$\beta_{1}$$| Coefficient for $x$|For every increase of 1 in $x$, how does $Y$ change?|


]


.pull-right45[

### Variables

Y = Income, X = Age

```{r, echo = FALSE, fig.width = 3.5, fig.height = 3, out.width = "70%"}

simple_scatter
```

### Formula

$$\Large income = 885 + age \times 149.3 + \epsilon$$

### Coefficients

$$\Large \beta_{0} = 885, \beta_{age} = 149.3$$

]

---

.pull-left5[

# Multiple Linear Regression

<high>Definition</high>: Multiple linear regression is a linear model with many predictors $x_{1},  x_{2}, .... x_{n}$, and where the error term $\epsilon$ is Normally distributed.

$$\LARGE Y=\beta_{0} + \beta_{1}x_{1} + \beta_{2} x_{2}+...+ \beta_{n} x_{n} + \epsilon$$
<br>


|Parameter|Description|In words|
|:------|:-----|:-----|
|$$\beta_{0}$$| Intercept|When *all* $x$ values are 0, what is the predicted value for $Y$?|
|$$\beta_{1}, \beta_{2}, \beta_{3} \ldots$$| Coefficient for $x_{1}, x_{2}...$|For every increase of 1 in $x_{1}, x_{2}, ...$, how does $Y$ change?|
]


.pull-right45[

### Variables

$$\large Y = Income, x_{1} = Age, x_{2}=Weight$$

```{r, echo = FALSE, fig.width = 5, fig.height = 2.25, out.width = "100%"}
mod_2 <- glm(income ~ age + height, data = baselers %>% filter(age < 80) %>% slice(1:100))

coef_2 <- coef(mod_2)

p1 <- ggplot(baselers %>% filter(age < 80) %>% slice(1:100), aes(age, income)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, col = baselrbootcamp_cols("magenta")) +
  theme_bw()

p2 <- ggplot(baselers %>% filter(age < 80) %>% slice(1:100), aes(height, income)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, col = baselrbootcamp_cols("magenta")) +
  theme_bw()

ggpubr::ggarrange(p1, p2, ncol = 2, nrow = 1)


```

### Formula

$$\large income = 1628 + 147  \times age - 4.1 \times height + \epsilon$$

### Coefficients

$$\large \beta_{0} = 1628, \beta_{age} = 147, \beta_{weight}=-4.1$$


]



---

.pull-left45[

# Estimating coefficients

How do we estimate the 'right' coefficients in a linear model?

```{r, fig.width = 2.5, fig.height = 2}
data <- baselers %>% 
  filter(age < 80) %>% 
  slice(1:5) %>%
  select(age, income) %>%
  mutate(income = income / 1000) %>%
  arrange(age)

mod <- lm(income ~ age, data = data)
my_coef <- coef(mod)

a_slope <- -.1
a_intercept <- 12

b_slope <-  round(my_coef[2], 3)
b_intercept <- round(my_coef[1], 3)

c_slope <- .11
c_intercept <- 2

d_slope <- 0
d_intercept <- 7


data <- data %>%
  mutate(a = a_intercept + a_slope * age,
         b = b_intercept + b_slope * age,
         c = c_intercept + c_slope * age,
         d = d_intercept + d_slope * age,
         a_se = (a - income) ^ 2,
         b_se = (b - income) ^ 2,
         c_se = (c - income) ^ 2,
         d_se = (d - income) ^ 2)


p <- ggplot(data, aes(age, income)) +
  geom_point(size = 2) +
  theme_bw()

a <- p + geom_abline(slope = a_slope, intercept = a_intercept, col = baselrbootcamp_cols("grey"), size = 1.5, alpha = .8) + labs(title = "A", subtitle = paste0("B_0 = ", a_intercept, ", B_age = ", a_slope))

b <- p + geom_abline(slope = b_slope, intercept = b_intercept, col = baselrbootcamp_cols("grey"), size = 1.5, alpha = .8) + labs(title = "B", subtitle = paste0("B_0 = ", b_intercept, ", B_age = ", b_slope))

c <- p +  geom_abline(slope = c_slope, intercept = c_intercept, col = baselrbootcamp_cols("grey"), size = 1.5, alpha = .8)+ labs(title = "C", subtitle = paste0("B_0 = ", c_intercept, ", B_age = ", c_slope))

d <- p +  geom_abline(slope = d_slope, intercept = d_intercept, col = baselrbootcamp_cols("grey"), size = 1.5, alpha = .8)+ labs(title = "D", subtitle = paste0("B_0 = ", d_intercept, ", B_age = ", d_slope))


```


```{r, fig.width = 3, fig.height = 2.5, out.width = "80%"}
p+ labs(title = "?", subtitle = "B_0 = ?, B_age = ?")
```

$$\Large income = \beta_{0} + \beta_{1} \times age + \epsilon$$
]

.pull-right5[
<br>
<high>Which do you like best?</high>

```{r, fig.width = 5, fig.height = 5, out.width = "95%"}

ggpubr::ggarrange(a, b, c, d, ncol = 2, nrow = 2)

```

]



---

.pull-left45[

# Estimating coefficients

How do we estimate the 'right' coefficients in a linear model?

```{r, fig.width = 2.5, fig.height = 2}
a <- a + annotate(geom = "segment", x = data$age, y = data$income, xend = data$age,  yend = data$a, col = baselrbootcamp_cols("magenta"))

b <- b + annotate(geom = "segment", x = data$age, y = data$income, xend = data$age,  yend = data$b, col = baselrbootcamp_cols("magenta"))

c <- c + annotate(geom = "segment", x = data$age, y = data$income, xend = data$age,  yend = data$c, col = baselrbootcamp_cols("magenta"))

d <- d + annotate(geom = "segment", x = data$age, y = data$income, xend = data$age,  yend = data$d, col = baselrbootcamp_cols("magenta"))

```

```{r, fig.width = 3, fig.height = 2.5, out.width = "80%"}
p+ geom_abline(slope = my_coef[2], intercept = my_coef[1], col = baselrbootcamp_cols("grey"), size = 1.5, alpha = .8) + 
  labs(title = "B", subtitle = paste0("B_0 = ", b_intercept, ", B_age = ", b_slope)) +
  annotate(geom = "segment", x = data$age, y = data$income, xend = data$age,  yend = data$b, col = baselrbootcamp_cols("magenta"))
```

$$\Large income = -0.254 + 0.167 \times age + \epsilon$$
]

.pull-right5[
<br>
<high>Which do you like best?</high>

```{r, fig.width = 5, fig.height = 5, out.width = "95%"}

ggpubr::ggarrange(a, b, c, d, ncol = 2, nrow = 2)

```

]


---

.pull-left45[

# Estimating coefficients

How do we estimate the 'right' coefficients in a linear model?


Find the values that minimise *Mean Squared Error*

$$\LARGE Mean\;Squared\;Error\;(MSE)$$
$$\LARGE = \frac{1}{N}\sum_{i=1}^{n}(Y_{i}-  Prediction_{i})^2$$
<!-- $$\LARGE = \frac{1}{N}\sum_{i=1}^{n}(Y_{i}-  (\beta_{0} + \beta_{1}x_{1}))^2$$ -->


<high>MSE In plain English</high>

> How far, on average are the model fits away from the true values (squared)?



]

.pull-right5[
<br>

$$\Large income = -0.254 + 0.167 \times age + \epsilon$$


```{r, fig.width = 2.5, fig.height = 2.5, out.width = "40%"}
p+ geom_abline(slope = my_coef[2], intercept = my_coef[1], col = baselrbootcamp_cols("grey"), size = 1.5, alpha = .8) + 
  labs(title = "B", subtitle = paste0("B_0 = ", b_intercept, ", B_age = ", b_slope)) +
  annotate(geom = "segment", x = data$age, y = data$income, xend = data$age,  yend = data$b, col = baselrbootcamp_cols("magenta"))
```

```{r, results = "as.is", eval = FALSE}
data %>%
  select(age, income, Prediction = b) %>%
  mutate(SE = (income - Prediction) ^ 2) %>%
  mutate(id = 1:5) %>% select(id, everything()) %>%
  slice(1:5) %>%
  knitr::kable()
```

| id| age| income| Prediction|     SE|
|--:|---:|------:|----------:|------:|
|  1|  24|    4.0|      3.730| 0.0729|
|  2|  27|    4.2|      4.228| 0.0008|
|  3|  31|    5.1|      4.892| 0.0433|
|  4|  44|    6.3|      7.050| 0.5625|
|  5|  65|   10.9|     10.536| 0.1325|

<center><h2>MSE = 0.16<h2></center>

]

---

.pull-left45[

# R-Squared

R-Squared $$R^{2}$$ is the most common method of establishing the overall accuarcy of a linear model.

$$R^{2}$$



]



.pull-right5[

<br><br><br>

```{r}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/r2_wikipedia.png?token=AIFo1PfbpTO7uGR7lXeB3JhUnLNixtP-ks5crP0GwA%3D%3D")
```

]


---

# Testing individual coefficients

- Test statistic, p-value

---

# R


---
## glm()


- formula
- data

---

## evaluating

- rsq::rsq()
- summary()
- coef()
- broom::tidy()

---

## printing results

- fitted()
- resid()

- calculate mad with mean(abs(resid))

---

## Predict new values

predict()

---

class: middle, center

<h1><a href="https://therbootcamp.github.io/SwR_2019Apr/_sessions/LinearModelsI/LinerModelsI_practical.html">Practical</a></h1>

