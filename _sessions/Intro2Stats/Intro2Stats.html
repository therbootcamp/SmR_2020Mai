<!DOCTYPE html>
<html>
  <head>
    <title>Intro to Statistics</title>
    <meta charset="utf-8">
    <meta name="author" content="Statistics with R   Basel R Bootcamp" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="baselrbootcamp.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Intro to Statistics
### Statistics with R<br> <a href='https://therbootcamp.github.io'> Basel R Bootcamp </a> <br> <a href='https://therbootcamp.github.io/SwR_2019Apr/'> <i class='fas fa-clock' style='font-size:.9em;'></i> </a>  <a href='https://therbootcamp.github.io'> <i class='fas fa-home' style='font-size:.9em;' ></i> </a>  <a href='mailto:therbootcamp@gmail.com'> <i class='fas fa-envelope' style='font-size: .9em;'></i> </a>  <a href='https://www.linkedin.com/company/basel-r-bootcamp/'> <i class='fab fa-linkedin' style='font-size: .9em;'></i> </a>
### April 2019

---


layout: true

&lt;div class="my-footer"&gt;
  &lt;span style="text-align:center"&gt;
    &lt;span&gt; 
      &lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/by-sa.png" height=14 style="vertical-align: middle"/&gt;
    &lt;/span&gt;
    &lt;a href="https://therbootcamp.github.io/"&gt;
      &lt;span style="padding-left:82px"&gt; 
        &lt;font color="#7E7E7E"&gt;
          www.therbootcamp.com
        &lt;/font&gt;
      &lt;/span&gt;
    &lt;/a&gt;
    &lt;a href="https://therbootcamp.github.io/"&gt;
      &lt;font color="#7E7E7E"&gt;
       Statistics with R | April 2019
      &lt;/font&gt;
    &lt;/a&gt;
    &lt;/span&gt;
  &lt;/div&gt; 

---







.pull-left6[

# Our goal in the next hour

In this hour, we will try to cover some of the basic principles of statistical inference.

1. Variability
2. Sample statistics
3. Distributions
4. Likelihood
5. Null Hypothesis testing
6. Test statistic
7. P-values

This is a lot to cover, and it may not be clear from the beginning.

But to help us, we'll think about it in terms of beer.

]


.pull-right35[

&lt;br&gt;&lt;br&gt;&lt;br&gt;
&lt;img src="https://cdn.lynda.com/course/495322/495322-636154038826424503-16x9.jpg" style="display: block; margin: auto;" /&gt;

&lt;br&gt;
&lt;img src="https://marketingweek.imgix.net/content/uploads/2018/05/11123103/beer-750.jpg?auto=compress,format,&amp;crop=faces,entropy,edges&amp;fit=crop&amp;q=60&amp;w=750&amp;h=460" width="90%" style="display: block; margin: auto;" /&gt;


]




---

.pull-left6[

# Example: 

Basel has many nice "Buvette's" that serve drinks in warm months.

The Oetlinger Buvette, one of our favorites, offers 33dl beers (or so they say...).

*I am convinced that the Oetlinger Buvette 'underpouring' its beers and they are not truly 33dl.*
&lt;br&gt;&lt;br&gt;
&lt;center&gt;How can I formulate my belief into an &lt;high&gt;formal hypothesis?&lt;/high&gt;&lt;/center&gt;&lt;br&gt;

&lt;center&gt;How can I collect &lt;high&gt;data to test the hypothesis?&lt;/center&gt;&lt;/high&gt;&lt;br&gt;

&lt;center&gt;&lt;font size = 5&gt;What do you think?&lt;/font&gt;&lt;/center&gt;
]


.pull-right35[
&lt;br&gt;&lt;br&gt;
&lt;img src="https://www.basel.com/extension/portal-basel/var/storage/images/media/bibliothek/basel-bilder/rhein-promenade/savoir-vivre-am-basler-rheinufer/92121-1-ger-DE/Savoir-vivre-am-Basler-Rheinufer_front_magnific.jpg" width="100%" style="display: block; margin: auto;" /&gt;



&lt;img src="https://files.newsnetz.ch/story/1/7/7/17765134/14/topelement.jpg" width="100%" style="display: block; margin: auto;" /&gt;


]

---

.pull-left6[




## Beer hypothesis

The mean amount poured in 33dl beers by the Oetlinger Buvette is less than 33dl


`$$\Large H0: \mu &lt; 33$$`

## Beer Data

I ordered 10 beers, and measured the exact amount in each cup, here are the results:




&lt;img src="Intro2Stats_files/figure-html/unnamed-chunk-8-1.png" width="80%" style="display: block; margin: auto;" /&gt;

]


.pull-right35[
&lt;br&gt;&lt;br&gt;
&lt;img src="https://www.basel.com/extension/portal-basel/var/storage/images/media/bibliothek/basel-bilder/rhein-promenade/savoir-vivre-am-basler-rheinufer/92121-1-ger-DE/Savoir-vivre-am-Basler-Rheinufer_front_magnific.jpg" width="100%" style="display: block; margin: auto;" /&gt;



&lt;img src="https://files.newsnetz.ch/story/1/7/7/17765134/14/topelement.jpg" width="100%" style="display: block; margin: auto;" /&gt;


]





---


.pull-left45[

# 1. Variability


All interesting data processes have &lt;high&gt;variability&lt;/high&gt;

- Stock prices change over time, 
- Individual patients respond to drugs differently

Statistical inference is all about &lt;high&gt;accounting for variability&lt;/high&gt;

If there was no variability, there would be no need to do statistics.

*If every single patient responded the exact same way to a drug, you could do a clinical trial with (any) one patient.*


]


.pull-right45[

Statistics is the process of understanding variability

&lt;img src="https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/variability_stock_drug.png?token=AIFo1Ck45ox-HHmcNvSnfBOLXhRRRjpVks5cq1a3wA%3D%3D" width="90%" style="display: block; margin: auto;" /&gt;

]


---


.pull-left45[

# 1. Variability

In general, we distinguish between two types of variability

|Variability Type|Definition|
|:-----|:-----|
|Systematic| Variation that &lt;high&gt;can&lt;/high&gt; be explained by known variables|
|Unsystematic |Variation that &lt;high&gt;cannot&lt;/high&gt; be explained by known variables|

Statistical inference is the practice of discovering ways to separate overall variability into systematic and unsystematic portions.


]


.pull-right45[





&lt;br&gt;&lt;br&gt;

&lt;img src="Intro2Stats_files/figure-html/unnamed-chunk-13-1.png" width="100%" style="display: block; margin: auto;" /&gt;



]


---


.pull-left45[

# 1. Variability

In general, we distinguish between two types of variability

|Variability Type|Definition|
|:-----|:-----|
|Systematic| Variation that &lt;high&gt;can&lt;/high&gt; be explained by known variables|
|Unsystematic |Variation that &lt;high&gt;cannot&lt;/high&gt; be explained by known variables|

Statistical inference is the practice of discovering ways to separate overall variability into systematic and unsystematic portions.


]


.pull-right45[


&lt;br&gt;&lt;br&gt;

&lt;img src="Intro2Stats_files/figure-html/unnamed-chunk-14-1.png" width="100%" style="display: block; margin: auto;" /&gt;



]


---


.pull-left45[

# 2. Sample Statistics

Once we collect (varying) data, we always look for ways to &lt;high&gt;summarise&lt;/high&gt; the data into &lt;high&gt;sample statistics&lt;/high&gt;

Sample statistics usually (but not always) fall into one of two types:

|Type|Examples |
|:----|:------|
|Central Tendency| Mean, mode, median|
|Variability|Standard deviation, variance, range|


Sample statistics give us &lt;high&gt;estimates&lt;/high&gt; of key model parameters (more on this later)

]


.pull-right5[


### Raw data &amp; Sample Statistics

&lt;img src="Intro2Stats_files/figure-html/unnamed-chunk-15-1.png" width="100%" style="display: block; margin: auto;" /&gt;


`$$Mean = \frac{28+31+28+...}{10} = 29.9$$`

`$$Standard \; Deviation = \sqrt{\frac{(28-29.9)^2+(31-29.9)^2+...}{10-1}} = 3.90$$`




]





---

.pull-left45[

# 3. Distributions

Statistics is built on &lt;high&gt;calculating the likelihood of data&lt;/high&gt; given how we assume it varies

To make calculations, we need to define variability according to a &lt;high&gt;probability distribution&lt;/high&gt;.

A probability distribution is a &lt;high&gt;mathematical formula&lt;/high&gt; that precisely defines &lt;high&gt;how likely&lt;/high&gt; every possible value in a dataset is.

Probability distributions are &lt;high&gt;always positive&lt;/high&gt; and their &lt;high&gt;sum adds up to 1.0&lt;/high&gt;

There are many probability distributions, but we often only use a few.

]


.pull-right45[

### 3 key aspects of a distribution

&lt;img src="Intro2Stats_files/figure-html/unnamed-chunk-16-1.png" width="100%" style="display: block; margin: auto;" /&gt;

*1. Probability Density Function (PDF)*

A formula that defines the distribution (you rarely need to know this)

*2. Support*

What values can x take on?

*3. Parameters*

Values that allow you to change the shape of the distribution? (e.g.; mean and variability?)

]


---

.pull-left45[

## Normal (Gaussian) Distribution


A 'bell-shaped curve'

The most important and widely used distribution in all of statistics


&lt;img src="Intro2Stats_files/figure-html/unnamed-chunk-17-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]


.pull-right45[

### Details

*Probability Density Function (PDF)*

`$$\large f(x)= \frac{1}{{\sigma \sqrt {2\pi } }} e^{(x-\mu)^2/2\sigma^2}$$`

*Support*


`$$\large x \; \epsilon (-\infty, \infty)$$`

*Parameters*

|Parameter|Meaning|
|:-----|:-------|
|$$\mu$$ | Center (mean)|
|$$\sigma$$| Variability (standard deviation)|
]


---

.pull-left45[

## Uniform Distribution


A 'Flat distribution'

Used when everything (within a range) is equally likely


&lt;img src="Intro2Stats_files/figure-html/unnamed-chunk-18-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]


.pull-right45[

### Details


*Probability Density Function (PDF)*

`$$\Large f(x)= \frac{1}{b-a}$$`

*Support*


`$$\Large x \; \epsilon (a, b)$$`


*Parameters*

|Parameter|Meaning|
|:-----|:-------|
|$$a$$ | Minimum|
|$$b$$| Maximum|
]


---

.pull-left45[

## Binomial Distribution

A discrete "Counting" distribution

If I flip a coin N times, with p(Head) = p, how many times will I get heads?

&lt;img src="Intro2Stats_files/figure-html/unnamed-chunk-19-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]


.pull-right45[

### Details


*Probability Density Function (PDF)*

`$$\large p(x) = {n \choose x}p^x(1-p)^{n-x}$$`
*Support*


`$$\large x \; \epsilon \{0, 1, ...n\}$$`

*Parameters*

|Parameter|Meaning|
|:-----|:-------|
|$$p$$| Probability of success on each trial|
|$$n$$ | Number of trials (flips)|

]




---

.pull-left45[

# 4: Likelihood

Why do we need distributions? To calculate &lt;high&gt;likelihoods&lt;/high&gt; of data.

- How likely is it that I would get this trial result if the drug is *really* better than a placebo?

Knowing this likelihood allows us to &lt;high&gt;fit parameters&lt;/high&gt; and &lt;high&gt;test&lt;/high&gt; models

- Given that out of 50 trial patients, the average recovery time was 2.3 days, what is the most likely distribution of recovery times for future patients?

]


.pull-right45[


&lt;img src="https://www.xofluza.com/content/dam/gene/xofluza/hcp/images/Mobile/Xofluza-Flu-Efficacy-Primary-Endpoint-TTAS-01-Mobile.jpg" width="100%" style="display: block; margin: auto;" /&gt;



&lt;img src="Intro2Stats_files/figure-html/unnamed-chunk-21-1.png" width="80%" style="display: block; margin: auto;" /&gt;






]

---

.pull-left45[

# 4: Likelihood

Using the binomial distributions on the right, answer the following questions:

- If there is a 50% chance of a drug being successful, then out of 10 drugs, how likely is it that exactly 5 will be successful?

- If there is a 10% chance that a customer will default on his/her loan, then out of 10 customers, how likely is it that none (0) will default?

]


.pull-right45[

&lt;br&gt;&lt;br&gt;

### 2 Binomial Distributions

&lt;img src="Intro2Stats_files/figure-html/unnamed-chunk-23-1.png" width="100%" style="display: block; margin: auto;" /&gt;


]




---

.pull-left65[

# 5: Null hypothesis testing

Null hypothesis testing is a statistical framework where two alternative hypotheses (the Null and the Alternative) are compared

|Hypothesis|Description|Example|
|:-----|:-----|:-------|
|Null (H0)|A proposed effect &lt;high&gt;does not exist&lt;/high&gt; and variation &lt;high&gt;is not systematic&lt;/high&gt;|Drug and placebo have the same effect|
|Alternative (H1)|A proposed effect &lt;high&gt;does exist&lt;/high&gt; and variation &lt;high&gt;is systematic&lt;/high&gt;|Drug and placebo do *not* have the same effect|


In order to compare these hypotheses, we need to calculate the likelihood of the data &lt;high&gt;given&lt;/high&gt; that one of these hypotheses are true.

]


.pull-right3[
&lt;br&gt;&lt;br&gt;&lt;br&gt;
Are these data consistent with H0?

&lt;img src="Intro2Stats_files/figure-html/unnamed-chunk-24-1.png" style="display: block; margin: auto;" /&gt;


]


---


.pull-left5[

# 6: Test statistic

Sample statistics (like means and standard deviations) are converted into &lt;high&gt;test statistics&lt;/high&gt;.

Test statistics are are numbers that help you quantify how likely data is given a null hypothesis.

&lt;!-- `$$Test\;Statistic=\frac{Variance\;explained\;by\;a\;model}{Variance\;NOT\;explained\;by\;a\;model}$$` --&gt;

Different tests give you different test statistics:

|Test|Test statistic|
|:----|:----|
|T-test|T-statistic|
|Correlation test|Correlation coefficient|
|Binomial|Number of successes|

Generally, the more extreme your test statistic is, the more evidence *against* the null hypothesis.


]


.pull-right5[
&lt;br&gt;&lt;br&gt;

&lt;img src="https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/beer_null.png?token=AIFo1GFEeag875nlAbAbeS8rdQAx0mW6ks5cq2_1wA%3D%3D" width="100%" style="display: block; margin: auto;" /&gt;

]

---
.pull-left55[

# 7: P-value

p-values are used to quantify the likelihood of data given the &lt;high&gt;probability distribution&lt;/high&gt; under the &lt;high&gt;null hypothesis&lt;/high&gt;

|p-value Definition|
|:----|
|A p-value is the probability of obtaining a test statistic as extreme as what you got assuming a null hypothesis is true|

&gt; "How likely is it that I would get a test statistic of -2.51 if the mean amount of beer really is 33dl?"

### p &lt; .05 = Reject H0

If a p-value is *small* (i.e.; p &lt; .05), this means that the likelihood of obtaining that data given the null hypothesis is equally *small*, suggesting that the null hypothesis is *wrong*

&lt;high&gt;Warning!&lt;/high&gt; Wait until the New Statistics lecture for alternatives to this approach!

]


.pull-right45[


&lt;img src="https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/pvalue_level.png?token=AIFo1Djs5aBnmWvuT7bFn7IyTzZJ-bQ_ks5cq215wA%3D%3D" width="90%" style="display: block; margin: auto;" /&gt;



]

---


.pull-left4[

# What about Oetlinger?

Our &lt;high&gt;p-value&lt;/high&gt;, indicating the &lt;high&gt;likelihood of obtaining the data if the null hypothesis is true&lt;/high&gt;, was 0.0273

This means, the data are very unlikely if, indeed, the mean amount poured is in fact 33dl.

It is so unlikely, that using the 0.05 threshold, we could conclude...

The Oetlinger buvette is consistently pouring less than 33dl!

We are getting ripped off!


*Note: These data are completely made up :) We're sure the Oetlinger Buvette is not ripping us off*

]


.pull-right55[

&lt;img src="https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/beer_null_p.png?token=AIFo1KnzD3U7_pjvmk0K09hE2HkXISmAks5cq3AGwA%3D%3D" width="100%" style="display: block; margin: auto;" /&gt;


]



---
class: middle, center

# Questions?

&lt;h1&gt;&lt;a href="https://therbootcamp.github.io/SwR_2019Apr/index.html"&gt;Schedule&lt;/a&gt;&lt;/h1&gt;


---
# Backup




---

.pull-left45[

# 4: Likelihood

Using the binomial distributions on the right, answer the following questions:

- You purchase a XX online
- You fear it may be fake

Data:

- You used it 10 times and it fails 5 times (5 out of 10).

Online research shows that 

- Real XX fail 10% of the time
- Fake XX fail 50% of the time.

Question

- How likely is your data if XX is real? What about if it's fake?
- Is it *more likely* that a real XX would give you your data or a fake XX?


]


.pull-right45[

&lt;br&gt;&lt;br&gt;


### 2 Binomial Distributions

&lt;img src="Intro2Stats_files/figure-html/unnamed-chunk-28-1.png" width="100%" style="display: block; margin: auto;" /&gt;



]
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
