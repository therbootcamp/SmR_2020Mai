<!DOCTYPE html>
<html>
  <head>
    <title>Intro to Statistics</title>
    <meta charset="utf-8">
    <meta name="author" content="Statistics with R   Basel R Bootcamp" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="baselrbootcamp.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Intro to Statistics
### Statistics with R<br> <a href='https://therbootcamp.github.io'> Basel R Bootcamp </a> <br> <a href='https://therbootcamp.github.io/SwR_2019Apr/'> <i class='fas fa-clock' style='font-size:.9em;'></i> </a>  <a href='https://therbootcamp.github.io'> <i class='fas fa-home' style='font-size:.9em;' ></i> </a>  <a href='mailto:therbootcamp@gmail.com'> <i class='fas fa-envelope' style='font-size: .9em;'></i> </a>  <a href='https://www.linkedin.com/company/basel-r-bootcamp/'> <i class='fab fa-linkedin' style='font-size: .9em;'></i> </a>
### April 2019

---


layout: true

&lt;div class="my-footer"&gt;
  &lt;span style="text-align:center"&gt;
    &lt;span&gt; 
      &lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/by-sa.png" height=14 style="vertical-align: middle"/&gt;
    &lt;/span&gt;
    &lt;a href="https://therbootcamp.github.io/"&gt;
      &lt;span style="padding-left:82px"&gt; 
        &lt;font color="#7E7E7E"&gt;
          www.therbootcamp.com
        &lt;/font&gt;
      &lt;/span&gt;
    &lt;/a&gt;
    &lt;a href="https://therbootcamp.github.io/"&gt;
      &lt;font color="#7E7E7E"&gt;
       Statistics with R | April 2019
      &lt;/font&gt;
    &lt;/a&gt;
    &lt;/span&gt;
  &lt;/div&gt; 

---







.pull-left6[

# Our goal in the next hour

In this hour, we will try to cover some of the basic principles of statistical inference.

1. Variability
2. Sample statistics
3. Sampling (samples vs. populations)
4. Distributions
5. Likelihood
6. Null Hypothesis testing
7. Test statistic
8. P-values

This is a lot to cover, and it may not be clear from the beginning.

But to help us, we'll think about it in terms of beer.

]


.pull-right35[

&lt;br&gt;&lt;br&gt;&lt;br&gt;
&lt;img src="https://cdn.lynda.com/course/495322/495322-636154038826424503-16x9.jpg" style="display: block; margin: auto;" /&gt;

&lt;br&gt;
&lt;img src="https://marketingweek.imgix.net/content/uploads/2018/05/11123103/beer-750.jpg?auto=compress,format,&amp;crop=faces,entropy,edges&amp;fit=crop&amp;q=60&amp;w=750&amp;h=460" width="90%" style="display: block; margin: auto;" /&gt;


]




---

.pull-left6[

# Example: 

Basel has many nice "Buvette's" that serve drinks in warm months.

The Oetlinger Buvette, one of our favorites, offers 33dl beers (or so they say...).

*I am convinced that the Oetlinger Buvette 'underpouring' its beers and they are not truly 33dl.*
&lt;br&gt;&lt;br&gt;
&lt;center&gt;How can I formulate my belief into an &lt;high&gt;formal hypothesis?&lt;/high&gt;&lt;/center&gt;&lt;br&gt;

&lt;center&gt;How can I collect &lt;high&gt;data to test the hypothesis?&lt;/center&gt;&lt;/high&gt;&lt;br&gt;

&lt;center&gt;&lt;font size = 5&gt;What do you think?&lt;/font&gt;&lt;/center&gt;
]


.pull-right35[
&lt;br&gt;&lt;br&gt;
&lt;img src="https://www.basel.com/extension/portal-basel/var/storage/images/media/bibliothek/basel-bilder/rhein-promenade/savoir-vivre-am-basler-rheinufer/92121-1-ger-DE/Savoir-vivre-am-Basler-Rheinufer_front_magnific.jpg" width="100%" style="display: block; margin: auto;" /&gt;



&lt;img src="https://files.newsnetz.ch/story/1/7/7/17765134/14/topelement.jpg" width="100%" style="display: block; margin: auto;" /&gt;


]

---

.pull-left6[




## Beer hypothesis

The mean amount poured in 33dl beers by the Oetlinger Buvette is less than 33dl


`$$\Large H0: \mu &lt; 33$$`

## Beer Data

I ordered 10 beers, and measured the exact amount in each cup, here are the results:




&lt;img src="Intro2Stats_files/figure-html/unnamed-chunk-8-1.png" width="80%" style="display: block; margin: auto;" /&gt;

]


.pull-right35[
&lt;br&gt;&lt;br&gt;
&lt;img src="https://www.basel.com/extension/portal-basel/var/storage/images/media/bibliothek/basel-bilder/rhein-promenade/savoir-vivre-am-basler-rheinufer/92121-1-ger-DE/Savoir-vivre-am-Basler-Rheinufer_front_magnific.jpg" width="100%" style="display: block; margin: auto;" /&gt;



&lt;img src="https://files.newsnetz.ch/story/1/7/7/17765134/14/topelement.jpg" width="100%" style="display: block; margin: auto;" /&gt;


]





---


.pull-left45[

# 1. Variability

All interesting data processes have &lt;high&gt;variability&lt;/high&gt;

- Stock prices change over time, 
- Individual patients respond to drugs differently

Statistical inference is all about &lt;high&gt;accounting for variability&lt;/high&gt;

If there was no variability, there would be no need to do statistics.

*If every single patient responded the exact same way to a drug, you could do a clinical trial with (any) one patient.*


]


.pull-right45[

Statistics is the process of understanding variability

&lt;img src="https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/variability_stock_drug.png?token=AIFo1Ck45ox-HHmcNvSnfBOLXhRRRjpVks5cq1a3wA%3D%3D" width="90%" style="display: block; margin: auto;" /&gt;

]


---


.pull-left45[

# 1. Variability

In general, we distinguish between two types of variability

|Variability Type|Definition|
|:-----|:-----|
|Systematic| Variation that &lt;high&gt;can&lt;/high&gt; be explained by known variables|
|Unsystematic |Variation that &lt;high&gt;cannot&lt;/high&gt; be explained by known variables|

Statistical inference is the practice of discovering ways to separate overall variability into systematic and unsystematic portions.


]


.pull-right45[





&lt;br&gt;&lt;br&gt;

&lt;img src="Intro2Stats_files/figure-html/unnamed-chunk-13-1.png" width="100%" style="display: block; margin: auto;" /&gt;



]


---


.pull-left45[

# 1. Variability

In general, we distinguish between two types of variability

|Variability Type|Definition|
|:-----|:-----|
|Systematic| Variation that &lt;high&gt;can&lt;/high&gt; be explained by known variables|
|Unsystematic |Variation that &lt;high&gt;cannot&lt;/high&gt; be explained by known variables|

Statistical inference is the practice of discovering ways to separate overall variability into systematic and unsystematic portions.


]


.pull-right45[


&lt;br&gt;&lt;br&gt;

&lt;img src="Intro2Stats_files/figure-html/unnamed-chunk-14-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]

---

.pull-left45[

# 2. Sample Statistics

Once we collect (varying) data, we always look for ways to &lt;high&gt;summarise&lt;/high&gt; the data into &lt;high&gt;sample statistics&lt;/high&gt;

Sample statistics usually (but not always) fall into one of two types:

|Type|Examples |
|:----|:------|
|Central Tendency| Mean, mode, median|
|Variability|Standard deviation, variance, range|


Sample statistics give us &lt;high&gt;estimates&lt;/high&gt; of key model parameters (more on this later)

]


.pull-right5[


### Raw data &amp; Sample Statistics

&lt;img src="Intro2Stats_files/figure-html/unnamed-chunk-15-1.png" width="100%" style="display: block; margin: auto;" /&gt;


`$$Mean = \frac{28+31+28+...}{10} = 29.9$$`

`$$Standard \; Deviation = \sqrt{\frac{(28-29.9)^2+(31-29.9)^2+...}{10-1}} = 3.90$$`


]



---

.pull-left45[
# 3. Sampling Procedures

In statistics, we always distinguish between a &lt;high&gt;sample&lt;/high&gt; and a &lt;/high&gt;population&lt;/high&gt;

Populations are what we are really interested in
- How will *all cancer patients* be affected by Drug X?
- How will *future stocks* change?
- What is the relationship between income and happiness in *all people*?

We rarely have access to entire populations (when we do, we don't need to do statistics!)

We must use &lt;high&gt;sampling procedures&lt;/high&gt; to obtain a smaller &lt;high&gt;sample&lt;/high&gt; of cases.

Based on that sample, we try to make &lt;high&gt;inferences&lt;/high&gt; to populations.


]


.pull-right5[

&lt;br&gt;&lt;br&gt;
&lt;img src="https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/sampling_procedure.png?token=AIFo1AUl-Ddz-zYZgoGbzwJoW8SO3LhUks5criQVwA%3D%3D" style="display: block; margin: auto;" /&gt;



]



---

.pull-left45[

# 4. Distributions

Statistics is built on &lt;high&gt;calculating the likelihood of data&lt;/high&gt; given how we assume it varies

To make calculations, we need to define variability according to a &lt;high&gt;probability distribution&lt;/high&gt;.

A probability distribution is a &lt;high&gt;mathematical formula&lt;/high&gt; that precisely defines &lt;high&gt;how likely&lt;/high&gt; every possible value in a dataset is.

Probability distributions are &lt;high&gt;always positive&lt;/high&gt; and their &lt;high&gt;sum adds up to 1.0&lt;/high&gt;

[Wikipedia: Distribution List](https://en.wikipedia.org/wiki/List_of_probability_distributions)

]


.pull-right5[

### 3 key aspects of a distribution

&lt;img src="Intro2Stats_files/figure-html/unnamed-chunk-17-1.png" width="90%" style="display: block; margin: auto;" /&gt;

*1. Probability Density Function (PDF)*

A formula that defines the distribution (you rarely need to know this)

*2. Support*

What values can x take on?

*3. Parameters*

Values that allow you to change the shape of the distribution? (e.g.; mean and variability?)

]


---

.pull-left45[

## Normal (Gaussian) Distribution


A 'bell-shaped curve'

The most important and widely used distribution in all of statistics


&lt;img src="Intro2Stats_files/figure-html/unnamed-chunk-18-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]


.pull-right45[

### Details

*Probability Density Function (PDF)*

`$$\large f(x)= \frac{1}{\sigma \sqrt {2\pi }} e^{(x-\mu)^2/2\sigma^2}$$`

*Support*


`$$\large x \; \epsilon (-\infty, \infty)$$`

*Parameters*

|Parameter|Meaning|
|:-----|:-------|
|$$\mu$$ | Center (mean)|
|$$\sigma$$| Variability (standard deviation)|
]


---

.pull-left45[

## Uniform Distribution


A 'Flat distribution'

Used when everything (within a range) is equally likely


&lt;img src="Intro2Stats_files/figure-html/unnamed-chunk-19-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]


.pull-right5[

### Details


*Probability Density Function (PDF)*

`$$\Large f(x)= \frac{1}{b-a}$$`

*Support*


`$$\Large x \; \epsilon (a, b)$$`


*Parameters*

|Parameter|Meaning|
|:-----|:-------|
|$$a$$ | Minimum|
|$$b$$| Maximum|
]


---

.pull-left5[

## Binomial Distribution

A discrete "Counting" distribution

If I flip a coin N times, with p(Head) = p, how many times will I get heads?

&lt;img src="Intro2Stats_files/figure-html/unnamed-chunk-20-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]


.pull-right45[

### Details


*Probability Density Function (PDF)*

`$$\large p(x) = {n \choose x}p^x(1-p)^{n-x}$$`
*Support*


`$$\large x \; \epsilon \{0, 1, ...n\}$$`

*Parameters*

|Parameter|Meaning|
|:-----|:-------|
|$$p$$| Probability of success on each trial|
|$$n$$ | Number of trials (flips)|

]




---

.pull-left45[

# 5: Likelihood

Why do we need distributions? To calculate &lt;high&gt;likelihoods&lt;/high&gt; of data.

&gt; How likely is it that I would get this trial result if the drug is *really* better than a placebo?

Knowing this likelihood allows us to &lt;high&gt;fit parameters&lt;/high&gt;, &lt;high&gt;test&lt;/high&gt; models, and make &lt;high&gt;predictions&lt;/high&gt; about future data


&gt; Given that out of 50 trial patients, the average recovery time was 2.3 days, what is the most likely distribution of recovery times for future patients?

]


.pull-right5[


&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://www.xofluza.com/content/dam/gene/xofluza/hcp/images/Mobile/Xofluza-Flu-Efficacy-Primary-Endpoint-TTAS-01-Mobile.jpg" alt="An advertisement for xofluza" width="80%" /&gt;
&lt;p class="caption"&gt;An advertisement for xofluza&lt;/p&gt;
&lt;/div&gt;


&lt;img src="Intro2Stats_files/figure-html/unnamed-chunk-22-1.png" width="75%" style="display: block; margin: auto;" /&gt;






]

---

.pull-left45[

# 5: Likelihood

Using the binomial distributions on the right, answer the following questions:

### Q1

If there is a 50% chance of a clinical trial being successful, then out of 10 trials, how likely is it that exactly 5 will be successful?

### Q2

If there is a 10% chance that a customer will default on his/her loan, then out of 10 customers, how likely is it that none (0) will default?

]


.pull-right5[

&lt;br&gt;&lt;br&gt;

### 2 Binomial Distributions

&lt;img src="Intro2Stats_files/figure-html/unnamed-chunk-24-1.png" width="100%" style="display: block; margin: auto;" /&gt;


]




---

.pull-left65[

# 6: Null hypothesis testing

Null hypothesis testing is a statistical framework where two alternative hypotheses (the Null and the Alternative) are compared

|Hypothesis|Description|Example|
|:-----|:-----|:-------|
|Null (H0)|A proposed effect &lt;high&gt;does not exist&lt;/high&gt; and variation &lt;high&gt;is not systematic&lt;/high&gt;|Drug and placebo have the same effect|
|Alternative (H1)|A proposed effect &lt;high&gt;does exist&lt;/high&gt; and variation &lt;high&gt;is systematic&lt;/high&gt;|Drug and placebo do *not* have the same effect|

In order to compare these hypotheses, we calculate the likelihood of obtaining data &lt;high&gt;assuming&lt;/high&gt; that the null hypothesis is true.

[Wikipedia: Null Hypothesis Testing (NHT)](https://en.wikipedia.org/wiki/Null_hypothesis)


]


.pull-right3[
&lt;br&gt;&lt;br&gt;&lt;br&gt;
Are these data consistent with H0?

&lt;img src="Intro2Stats_files/figure-html/fig.cap-1.png" style="display: block; margin: auto;" /&gt;


]


---


.pull-left5[

# 7: Test statistics

Sample statistics (like means and standard deviations) are converted into &lt;high&gt;test statistics&lt;/high&gt;.

Test statistics are unit-free numbers that help you quantify how likely data is given a null hypothesis.

&lt;!-- `$$Test\;Statistic=\frac{Variance\;explained\;by\;a\;model}{Variance\;NOT\;explained\;by\;a\;model}$$` --&gt;

Different tests give you different test statistics:

|Test|Test statistic|
|:----|:----|
|T-test|T-statistic|
|Correlation test|Correlation coefficient|
|ANOVA|F-statistic|

Generally, the &lt;high&gt;more extreme&lt;/high&gt; (i.e.; highly positive or highly negative) your test statistic is, the &lt;high&gt;more evidence against&lt;/high&gt; the null hypothesis.


]


.pull-right5[
&lt;br&gt;&lt;br&gt;

&lt;img src="https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/beer_null.png?token=AIFo1GFEeag875nlAbAbeS8rdQAx0mW6ks5cq2_1wA%3D%3D" width="100%" style="display: block; margin: auto;" /&gt;

]

---
.pull-left55[

# 8: P-value

p-values are used to quantify the likelihood of data given the &lt;high&gt;probability distribution&lt;/high&gt; under the &lt;high&gt;null hypothesis&lt;/high&gt;

|p-value Definition|
|:----|
|A p-value is the probability of obtaining test statistics as extreme as what you got assuming a null hypothesis is true|

&gt; "How likely is it that I would get a test statistic of -2.51 if the mean amount of beer really is 33dl?"

### p &lt; .05 = Reject H0

If a p-value is *small* (i.e.; p &lt; .05), this means that the likelihood of obtaining that data given the null hypothesis is equally *small*, suggesting that the null hypothesis is *wrong*

&lt;high&gt;Warning!&lt;/high&gt; Wait until the New Statistics lecture for alternatives to this approach!

]


.pull-right45[


&lt;img src="https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/pvalue_level.png?token=AIFo1Djs5aBnmWvuT7bFn7IyTzZJ-bQ_ks5cq215wA%3D%3D" width="90%" style="display: block; margin: auto;" /&gt;



]

---


.pull-left5[

# What about Oetlinger?

|Step|Result|
|:----|:-----|
|Null Hypothesis (H0)|The mean amount of beer poured by Oetlinger is (exactly) equal to 33ml.|
|Sample Statistics|Mean = 33, Standard Deviation = 3.90.|
|Test statistic and p-value|We convert our stample statistics to a test statistic of &lt;high&gt;t = -2.51&lt;/high&gt; and a &lt;high&gt;p-value&lt;/high&gt; of &lt;high&gt;p = 0.0273|

&lt;high&gt;Conclusion&lt;/high&gt;: Using a p &lt; 0.05 threshold, we conclude that, because likelihood of obtaining the data if the null hypothesis is true is only .0273 (&lt; 0.05), then the null hypothesis is likely wrong, and *the Oetlinger buvette is consistently pouring less than 33dl!*

*Note: These data are completely made up :) We're sure the Oetlinger Buvette is not ripping us off*

]


.pull-right45[
&lt;br&gt;&lt;br&gt;
&lt;img src="https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/beer_null_p.png?token=AIFo1KnzD3U7_pjvmk0K09hE2HkXISmAks5cq3AGwA%3D%3D" width="100%" style="display: block; margin: auto;" /&gt;


]


---


.pull-left5[

# What about Oetlinger?

|Step|Result|
|:----|:-----|
|Null Hypothesis (H0)|The mean amount of beer poured by Oetlinger is (exactly) equal to 33ml.|
|Sample Statistics|Mean = 33, Standard Deviation = 3.90.|
|Test statistic and p-value|We convert our stample statistics to a test statistic of &lt;high&gt;t = -2.51&lt;/high&gt; and a &lt;high&gt;p-value&lt;/high&gt; of &lt;high&gt;p = 0.0273|

&lt;high&gt;Conclusion&lt;/high&gt;: Using a p &lt; 0.05 threshold, we conclude that, because likelihood of obtaining the data if the null hypothesis is true is only .0273 (&lt; 0.05), then the null hypothesis is likely wrong, and *the Oetlinger buvette is consistently pouring less than 33dl!*

*Note: These data are completely made up :) We're sure the Oetlinger Buvette is not ripping us off*

]


.pull-right45[
&lt;br&gt;
### Doing our test in R!


```r
# Define beer sample
beer &lt;- c(28, 31, 28, 37, 30, 
          33, 25, 33, 24, 30)

# Conduct one-sample t-test
t.test(x = beer, 
       mu = 33)
```

```
## 
## 	One Sample t-test
## 
## data:  beer
## t = -2.5, df = 9, p-value = 0.03
## alternative hypothesis: true mean is not equal to 33
## 95 percent confidence interval:
##  27.11 32.69
## sample estimates:
## mean of x 
##      29.9
```


]



---
class: middle, center

# Questions?

&lt;h1&gt;&lt;a href="https://therbootcamp.github.io/SwR_2019Apr/index.html"&gt;Schedule&lt;/a&gt;&lt;/h1&gt;
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
