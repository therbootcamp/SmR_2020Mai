---
title: "Intro to Statistics"
author: "Statistics with R<br>
  <a href='https://therbootcamp.github.io'>
    Basel R Bootcamp
  </a>
  <br>
  <a href='https://therbootcamp.github.io/SwR_2019Apr/'>
    <i class='fas fa-clock' style='font-size:.9em;'></i>
  </a>&#8239; 
  <a href='https://therbootcamp.github.io'>
    <i class='fas fa-home' style='font-size:.9em;' ></i>
  </a>&#8239;
  <a href='mailto:therbootcamp@gmail.com'>
    <i class='fas fa-envelope' style='font-size: .9em;'></i>
  </a>&#8239;
  <a href='https://www.linkedin.com/company/basel-r-bootcamp/'>
    <i class='fab fa-linkedin' style='font-size: .9em;'></i>
  </a>"
date: "April 2019"
output:
  xaringan::moon_reader:
    css: ["default", "baselrbootcamp.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
---

layout: true

<div class="my-footer">
  <span style="text-align:center">
    <span> 
      <img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/by-sa.png" height=14 style="vertical-align: middle"/>
    </span>
    <a href="https://therbootcamp.github.io/">
      <span style="padding-left:82px"> 
        <font color="#7E7E7E">
          www.therbootcamp.com
        </font>
      </span>
    </a>
    <a href="https://therbootcamp.github.io/">
      <font color="#7E7E7E">
       Statistics with R | April 2019
      </font>
    </a>
    </span>
  </div> 

---

```{r, eval = TRUE, echo = FALSE, warning=F,message=F}
# Code to knit slides

```

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
# see: https://github.com/yihui/xaringan
# install.packages("xaringan")
# see: 
# https://github.com/yihui/xaringan/wiki
# https://github.com/gnab/remark/wiki/Markdown
options(width = 110)
options(digits = 4)

# Load packages
require(tidyverse)
library(cowplot)
library(ggpubr)

knitr::opts_chunk$set(fig.align = "center", warning = FALSE, message = FALSE, dpi = 300)


# Load data
baselers <- readr::read_csv("1_Data/baselers.csv")

# get color palette functions
source("https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_materials/code/baselrbootcamp_palettes.R")

```



.pull-left6[

# Our goal in the next hour

In this hour, we will try to cover some of the basic principles of statistical inference.

1. Variability
2. Sample statistics
3. Sampling (samples vs. populations)
4. Distributions
5. Likelihood
6. Null Hypothesis testing
7. Test statistic
8. P-values

This is a lot to cover, and it may not be clear from the beginning.

But to help us, we'll think about it in terms of beer.

]


.pull-right35[

<br><br><br>
```{r, echo = FALSE}
knitr::include_graphics("https://cdn.lynda.com/course/495322/495322-636154038826424503-16x9.jpg")
```

<br>
```{r, echo = FALSE, out.width = "90%"}
knitr::include_graphics("https://marketingweek.imgix.net/content/uploads/2018/05/11123103/beer-750.jpg?auto=compress,format,&crop=faces,entropy,edges&fit=crop&q=60&w=750&h=460")
```


]




---

.pull-left6[

# Example: 

Basel has many nice "Buvette's" that serve drinks in warm months.

The Oetlinger Buvette, one of our favorites, offers 33dl beers (or so they say...).

*I am convinced that the Oetlinger Buvette 'underpouring' its beers and they are not truly 33dl.*
<br><br>
<center>How can I formulate my belief into an <high>formal hypothesis?</high></center><br>

<center>How can I collect <high>data to test the hypothesis?</center></high><br>

<center><font size = 5>What do you think?</font></center>
]


.pull-right35[
<br><br>
```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("https://www.basel.com/extension/portal-basel/var/storage/images/media/bibliothek/basel-bilder/rhein-promenade/savoir-vivre-am-basler-rheinufer/92121-1-ger-DE/Savoir-vivre-am-Basler-Rheinufer_front_magnific.jpg")
```



```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("https://files.newsnetz.ch/story/1/7/7/17765134/14/topelement.jpg")
```


]

---

.pull-left6[

```{r, echo= FALSE}
beer <- c(28, 31, 28, 37, 30, 33, 25, 33, 24, 30)
beer_data <- tibble(beer, id = 1:length(beer))
```


## Beer hypothesis

The mean amount poured in 33dl beers by the Oetlinger Buvette is less than 33dl


$$\Large H0: \mu < 33$$

## Beer Data

I ordered 10 beers, and measured the exact amount in each cup, here are the results:

```{r, echo= FALSE}
beer <- c(28, 31, 28, 37, 30, 33, 25, 33, 24, 30)
beer_data <- tibble(beer, id = 1:length(beer))
```


```{r, echo = FALSE, out.width = "80%", fig.align = 'center', fig.width = 5, fig.height = 3}
beer_gg <- ggplot(beer_data, aes(x = id, y = beer)) +
  geom_hline(yintercept = 33, col = baselrbootcamp_cols("green")) +
  geom_text(data = beer_data %>% filter(beer > 30), aes(label = beer), nudge_y = 2) +
    geom_text(data = beer_data %>% filter(beer <= 30), aes(label = beer), nudge_y = -2) +
  scale_x_continuous(breaks = 1:10) +
  labs(y = "Amount in dl", title = "10 beers from Oetlinger Buvette") +
  ylim(c(20, 40)) +
  geom_point(size = 2.5)

beer_gg
```

]


.pull-right35[
<br><br>
```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("https://www.basel.com/extension/portal-basel/var/storage/images/media/bibliothek/basel-bilder/rhein-promenade/savoir-vivre-am-basler-rheinufer/92121-1-ger-DE/Savoir-vivre-am-Basler-Rheinufer_front_magnific.jpg")
```



```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("https://files.newsnetz.ch/story/1/7/7/17765134/14/topelement.jpg")
```


]





---


.pull-left45[

# 1. Variability

All interesting data processes have <high>variability</high>

- Stock prices change over time, 
- Individual patients respond to drugs differently

Statistical inference is all about <high>accounting for variability</high>

If there was no variability, there would be no need to do statistics.

*If every single patient responded the exact same way to a drug, you could do a clinical trial with (any) one patient.*


]


.pull-right45[

Statistics is the process of understanding variability

```{r, echo = FALSE, out.width = "90%"}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/variability_stock_drug.png?token=AIFo1Ck45ox-HHmcNvSnfBOLXhRRRjpVks5cq1a3wA%3D%3D")
```

]


---


.pull-left45[

# 1. Variability

In general, we distinguish between two types of variability

|Variability Type|Definition|
|:-----|:-----|
|Systematic| Variation that <high>can</high> be explained by known variables|
|Unsystematic |Variation that <high>cannot</high> be explained by known variables|

Statistical inference is the practice of discovering ways to separate overall variability into systematic and unsystematic portions.


]


.pull-right45[


```{r, echo = FALSE}

set.seed(103)
x <- rnorm(50, mean = 50, sd = 10)


id <- 1:50
income <- rnorm(n = 50, mean = 7000, sd = 1000)
exercise <- rnorm(n = 50, mean = 30, sd = 10)
beer <- sample(c(0, 1), size = 50, replace = TRUE)
sex <- sample(c(0, 1), size = 50, replace = TRUE)
happiness <- rnorm(50)
happiness[beer == 1] <- .2 * exercise + 2 + rnorm(sum(beer == 1), mean = 0, sd = 1) - 2
happiness[beer == 0] <- .2 * exercise - 2 + rnorm(sum(beer == 0), mean = 0, sd = 1) - 2
happiness2 <- .2 * exercise + rnorm(50, mean = 0, sd = 1) - 2


dat <- tibble(id, income, exercise, happiness, happiness2, group = rep("Baselers", 50))

overall <- ggplot(dat, aes(x = id, y = happiness)) +
  geom_point() +
  labs(y = "Happiness",
       x = "id",
       title = "Total Variability in Happiness")


low_fact <- ggplot(dat, aes(factor(sex), happiness)) +
  geom_jitter(width = .05) +
  labs(y = "Happiness",
       x = "Sex",
       title = "Low Variance Explained") +
  scale_y_continuous(breaks = seq(0, 10, 1), limits = c(1, 10))

high_fact <- ggplot(dat, aes(factor(beer), happiness)) +
  geom_jitter(width = .05) +
  labs(y = "Happiness",
       x = "Beer",
       title = "High Variance Explained") +
  scale_y_continuous(breaks = seq(0, 10, 1), limits = c(1, 10))


low_cont <- ggplot(dat, aes(income, happiness2)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(y = "Happiness",
       x = "Income",
       title = "Low Variance Explained") +
  scale_y_continuous(breaks = seq(0, 10, 1), limits = c(1, 10))

high_cont <- ggplot(dat, aes(exercise, happiness2)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(y = "Happiness",
       x = "Minutes of Exercise per day",
       title = "High Variance Explained")+
  scale_y_continuous(breaks = seq(0, 10, 1), limits = c(1, 10))

```


<br><br>

```{r, echo = FALSE, fig.width = 4, fig.height = 4, dpi= 200, out.width = "100%"}
overall + scale_x_continuous(seq(1, 50, 1))
```



]


---


.pull-left45[

# 1. Variability

In general, we distinguish between two types of variability

|Variability Type|Definition|
|:-----|:-----|
|Systematic| Variation that <high>can</high> be explained by known variables|
|Unsystematic |Variation that <high>cannot</high> be explained by known variables|

Statistical inference is the practice of discovering ways to separate overall variability into systematic and unsystematic portions.


]


.pull-right45[


<br><br>

```{r, echo = FALSE, fig.width = 6, fig.height = 6, dpi= 200, out.width = "100%"}
ggpubr::ggarrange(low_fact, high_fact, low_cont, high_cont, ncol = 2, nrow = 2)
```

]

---

.pull-left45[

# 2. Sample Statistics

Once we collect (varying) data, we always look for ways to <high>summarise</high> the data into <high>sample statistics</high>

Sample statistics usually (but not always) fall into one of two types:

|Type|Examples |
|:----|:------|
|Central Tendency| Mean, mode, median|
|Variability|Standard deviation, variance, range|


Sample statistics give us <high>estimates</high> of key model parameters (more on this later)

]


.pull-right5[


### Raw data & Sample Statistics

```{r, echo = FALSE, fig.width = 4, fig.height = 3, dpi = 200, out.width = "100%"}
ggplot(beer_data, aes(x = id, y = beer)) +
  geom_text(data = beer_data %>% filter(beer > 30), aes(label = beer), nudge_y = 2) +
    geom_text(data = beer_data %>% filter(beer <= 30), aes(label = beer), nudge_y = -2) +
  scale_x_continuous(breaks = 1:10) +
  labs(y = "Amount in dl", title = "10 beers from Oetlinger Buvette") +
  ylim(c(20, 40)) +
  geom_hline(yintercept = mean(beer_data$beer), col = baselrbootcamp_cols("green"), size = 1.5) +
  geom_segment(aes(x = id, y = beer, xend = id, yend = 29.9), col = baselrbootcamp_cols("yellow"), lty = 1, size= 1.5) +
  annotate(geom = "segment", x = 2, xend = 2, y = 22 - 3.9 / 2, yend = 22 + 3.9 / 2, size = 1.5, col = baselrbootcamp_cols("yellow")) +
  annotate(geom = "label", x = 4, y = 22.5, label = "Std Deviation = 3.90", cex = 3) +
  annotate(geom = "label", x = 7, y = 30, label = "Mean = 29.9", cex = 3) +
  geom_point(size = 2.5)
```


$$Mean = \frac{28+31+28+...}{10} = 29.9$$

$$Standard \; Deviation = \sqrt{\frac{(28-29.9)^2+(31-29.9)^2+...}{10-1}} = 3.90$$


]



---

.pull-left45[
# 3. Sampling Procedures

In statistics, we always distinguish between a <high>sample</high> and a </high>population</high>

Populations are what we are really interested in
- How will *all cancer patients* be affected by Drug X?
- How will *future stocks* change?
- What is the relationship between income and happiness in *all people*?

We rarely have access to entire populations (when we do, we don't need to do statistics!)

We must use <high>sampling procedures</high> to obtain a smaller <high>sample</high> of cases.

Based on that sample, we try to make <high>inferences</high> to populations.


]


.pull-right5[

<br><br>
```{r, echo = FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/sampling_procedure.png?token=AIFo1AUl-Ddz-zYZgoGbzwJoW8SO3LhUks5criQVwA%3D%3D")
```



]



---

.pull-left45[

# 4. Distributions

Statistics is built on <high>calculating the likelihood of data</high> given how we assume it varies

To make calculations, we need to define variability according to a <high>probability distribution</high>.

A probability distribution is a <high>mathematical formula</high> that precisely defines <high>how likely</high> every possible value in a dataset is.

Probability distributions are <high>always positive</high> and their <high>sum adds up to 1.0</high>

[Wikipedia: Distribution List](https://en.wikipedia.org/wiki/List_of_probability_distributions)

]


.pull-right5[

### 3 key aspects of a distribution

```{r, echo = FALSE, fig.width = 4, fig.height = 2, out.width = "90%", dpi=200}

p1 <- ggplot(data = data.frame(x = c(60, 140)), aes(x)) +
  stat_function(fun = dnorm, n = 1001, args = list(mean = 110, sd = 10), col = baselrbootcamp_cols("green"),size = 2) + 
    stat_function(fun = dnorm, n = 1001, args = list(mean = 90, sd = 5), col = baselrbootcamp_cols("yellow"),size = 2) + 
  ylab("Likelihood") +
  scale_y_continuous(breaks = NULL) +
  xlab("x") +
  annotate("text", x = 90, .09, label =parse(text = paste0('mu == 90 ~ sigma == 5')), size = 5) +
annotate("text", x = 120, .05, label =parse(text = paste0('mu == 110 ~ sigma == 10')), size = 5) +
  ylim(c(0, .1))

  
p1
```

*1. Probability Density Function (PDF)*

A formula that defines the distribution (you rarely need to know this)

*2. Support*

What values can x take on?

*3. Parameters*

Values that allow you to change the shape of the distribution? (e.g.; mean and variability?)

]


---

.pull-left45[

## Normal (Gaussian) Distribution


A 'bell-shaped curve'

The most important and widely used distribution in all of statistics


```{r, echo = FALSE, fig.width = 4, fig.height = 3, out.width = "100%", dpi=200}

p1 <- ggplot(data = data.frame(x = c(60, 140)), aes(x)) +
  stat_function(fun = dnorm, n = 1001, args = list(mean = 110, sd = 10), col = baselrbootcamp_cols("green"),size = 2) + 
    stat_function(fun = dnorm, n = 1001, args = list(mean = 90, sd = 5), col = baselrbootcamp_cols("yellow"),size = 2) + 
  ylab("Likelihood") +
  scale_y_continuous(breaks = NULL) +
  xlab("x") +
  annotate("text", x = 90, .09, label =parse(text = paste0('mu == 90 ~ sigma == 5')), size = 5) +
annotate("text", x = 120, .05, label =parse(text = paste0('mu == 110 ~ sigma == 10')), size = 5) +
  ylim(c(0, .1))

  
p1
```

]


.pull-right45[

### Details

*Probability Density Function (PDF)*

$$\large f(x)= \frac{1}{\sigma \sqrt {2\pi }} e^{(x-\mu)^2/2\sigma^2}$$

*Support*


$$\large x \; \epsilon (-\infty, \infty)$$

*Parameters*

|Parameter|Meaning|
|:-----|:-------|
|$$\mu$$ | Center (mean)|
|$$\sigma$$| Variability (standard deviation)|
]


---

.pull-left45[

## Uniform Distribution


A 'Flat distribution'

Used when everything (within a range) is equally likely


```{r, echo = FALSE, fig.width = 4, fig.height = 3, out.width = "100%", dpi=200}

p1 <- ggplot(data = data.frame(x = c(60, 130)), aes(x)) +
  stat_function(fun = dunif, n = 1001, args = list(min = 80, max = 120), col = baselrbootcamp_cols("green"),size = 2) + 
    stat_function(fun = dunif, n = 1001, args = list(min = 70, max = 85), col = baselrbootcamp_cols("yellow"),size = 2) + 
  ylab("Likelihood") +
  scale_y_continuous(breaks = NULL) +
  xlab("x") +
  annotate("text", x = 77.5, .08, label =parse(text = paste0('a == 70 ~ b == 85')), size = 5) +
annotate("text", x = 105, .035, label =parse(text = paste0('a == 80 ~ b == 120')), size = 5) +
  ylim(c(0, .1))

  
p1
```

]


.pull-right5[

### Details


*Probability Density Function (PDF)*

$$\Large f(x)= \frac{1}{b-a}$$

*Support*


$$\Large x \; \epsilon (a, b)$$


*Parameters*

|Parameter|Meaning|
|:-----|:-------|
|$$a$$ | Minimum|
|$$b$$| Maximum|
]


---

.pull-left5[

## Binomial Distribution

A discrete "Counting" distribution

If I flip a coin N times, with p(Head) = p, how many times will I get heads?

```{r, echo = FALSE, fig.width = 4, fig.height = 3, out.width = "100%", dpi=200}

d1 <-  data.frame(x = seq(0, 10, 1),
                               y = dbinom(seq(0, 10, 1), size = 10, prob = .5))


d2 <-  data.frame(x = seq(0, 10, 1),
                               y = dbinom(seq(0, 10, 1), size = 10, prob = .1))


p1 <- ggplot(data =d2, 
             aes(x = x, y = y)) +
  geom_point(data = d1, col = baselrbootcamp_cols("green"), size = 2) +
  geom_line(data = d1, size = .5, col = "black", lty = 3) + 
  geom_point(data = d2, col = baselrbootcamp_cols("yellow"), size = 2) +
  geom_line(data = d2, size = .5, col = "black", lty = 3) + 
  ylab("Likelihood") +
  ylim(0, .5) + 
  annotate("text", x = 2, y = .45, label =parse(text = paste0('p == .1 ~ n== 10')), size = 5) +
  annotate("text", x = 5, y = .3, label =parse(text = paste0('p == .5 ~ n== 10')), size = 5) +
  scale_x_continuous(breaks = 0:10)

p1

```

]


.pull-right45[

### Details


*Probability Density Function (PDF)*

$$\large p(x) = {n \choose x}p^x(1-p)^{n-x}$$
*Support*


$$\large x \; \epsilon \{0, 1, ...n\}$$

*Parameters*

|Parameter|Meaning|
|:-----|:-------|
|$$p$$| Probability of success on each trial|
|$$n$$ | Number of trials (flips)|

]




---

.pull-left45[

# 5: Likelihood

Why do we need distributions? To calculate <high>likelihoods</high> of data.

> How likely is it that I would get this trial result if the drug is *really* better than a placebo?

Knowing this likelihood allows us to <high>fit parameters</high>, <high>test</high> models, and make <high>predictions</high> about future data


> Given that out of 50 trial patients, the average recovery time was 2.3 days, what is the most likely distribution of recovery times for future patients?

]


.pull-right5[


```{r, echo = FALSE, out.width = "80%", fig.cap = "An advertisement for xofluza"}
knitr::include_graphics("https://www.xofluza.com/content/dam/gene/xofluza/hcp/images/Mobile/Xofluza-Flu-Efficacy-Primary-Endpoint-TTAS-01-Mobile.jpg")
```


```{r, echo = FALSE, fig.width = 3.5, fig.height = 2.5, out.width = "75%"}

set.seed(105)
x <- rnorm(50, mean = 50, sd = 10)

id <- 1:50
drug <- sample(c("Drug", "Placebo"), size = 50, replace = TRUE)
effect <- rnorm(50)
effect[drug == "Drug"] <- 2.3 + rnorm(sum(drug == "Drug"), mean = 0, sd = 1.5) - .41
effect[drug == "Placebo"] <- 3.3 + rnorm(sum(drug == "Placebo"), mean = 0, sd = 1.5) - .12

dat <- tibble(id, drug, effect, group = rep("Baselers", 50))
avg <- dat %>%
  group_by(drug) %>%
  summarise(days_mean = mean(effect))

drug_placebo_gg <- ggplot(dat, aes(factor(drug), effect)) +
  geom_jitter(width = .05) +
  labs(y = "Days",
       x = "Condition",
       title = "Fictional data comparing Drug to Placebo arm") +
  scale_y_continuous(breaks = seq(0, 10, 1), limits = c(0, 7)) +
  geom_label(data = avg, aes(x = drug, y = days_mean, label = round(days_mean, 2))) +
  theme(plot.title = element_text(size = 10))

drug_placebo_gg
```



```{r, eval = FALSE, echo = FALSE, fig.width = 4, fig.height = 3, out.width = "80%", dpi=200}

p1 <- ggplot(data = data.frame(x = c(0, 10)), aes(x)) +
  stat_function(fun = dnorm, n = 1001, args = list(mean = 3, sd = 2), col = baselrbootcamp_cols("green"),size = 2) + 
    stat_function(fun = dnorm, n = 1001, args = list(mean = 1, sd = 3), col = baselrbootcamp_cols("yellow"),size = 2) + 
  ylab("Likelihood") +
  scale_y_continuous(breaks = NULL) +
  xlab("x") +
  annotate("text", x = 3, .23, label =parse(text = paste0('mu == 3 ~ sigma == 2')), size = 5) +
annotate("text", x = 2, .15, label =parse(text = paste0('mu == 1 ~ sigma == 3')), size = 5) +
  ylim(c(0, .25)) +
  scale_x_continuous(breaks = 0:10)

  
p1
```


]

---

.pull-left45[

# 5: Likelihood

Using the binomial distributions on the right, answer the following questions:

### Q1

If there is a 50% chance of a clinical trial being successful, then out of 10 trials, how likely is it that exactly 5 will be successful?

### Q2

If there is a 10% chance that a customer will default on his/her loan, then out of 10 customers, how likely is it that none (0) will default?

]


.pull-right5[

<br><br>

### 2 Binomial Distributions

```{r, echo = FALSE, fig.width = 4, fig.height = 3, out.width = "100%", dpi=200}

d1 <-  data.frame(x = seq(0, 10, 1),
                               y = dbinom(seq(0, 10, 1), size = 10, prob = .5))

d2 <-  data.frame(x = seq(0, 10, 1),
                               y = dbinom(seq(0, 10, 1), size = 10, prob = .1))


p1 <- ggplot(data =d2, 
             aes(x = x, y = y)) +
  geom_hline(yintercept =  seq(0, .5, .1), col = "gray", size = .3, alpha = .5) +
  geom_vline(xintercept =  seq(0, 10), col = "gray", size = .3, alpha = .5) +
  geom_point(data = d1, col = baselrbootcamp_cols("green"), size = 2) +
  geom_line(data = d1, size = .5, col = "black", lty = 3) + 
  geom_point(data = d2, col = baselrbootcamp_cols("yellow"), size = 2) +
  geom_line(data = d2, size = .5, col = "black", lty = 3) + 
  ylab("Likelihood") +
  ylim(0, .5) + 
  annotate("text", x = 2, y = .45, label =parse(text = paste0('p == .1 ~ n== 10')), size = 5) +
  annotate("text", x = 5, y = .3, label =parse(text = paste0('p == .5 ~ n== 10')), size = 5) +
  scale_x_continuous(breaks = 0:10)

p1

```


]




---

.pull-left65[

# 6: Null hypothesis testing

Null hypothesis testing is a statistical framework where two alternative hypotheses (the Null and the Alternative) are compared

|Hypothesis|Description|Example|
|:-----|:-----|:-------|
|Null (H0)|A proposed effect <high>does not exist</high> and variation <high>is not systematic</high>|Drug and placebo have the same effect|
|Alternative (H1)|A proposed effect <high>does exist</high> and variation <high>is systematic</high>|Drug and placebo do *not* have the same effect|

In order to compare these hypotheses, we calculate the likelihood of obtaining data <high>assuming</high> that the null hypothesis is true.

[Wikipedia: Null Hypothesis Testing (NHT)](https://en.wikipedia.org/wiki/Null_hypothesis)


]


.pull-right3[
<br><br><br>
Are these data consistent with H0?

```{r, echo = FALSE, fig.width = 3, fig.height = 3, fig.cap }
drug_placebo_gg
```


]


---


.pull-left5[

# 7: Test statistics

Sample statistics (like means and standard deviations) are converted into <high>test statistics</high>.

Test statistics are unit-free numbers that help you quantify how likely data is given a null hypothesis.

<!-- $$Test\;Statistic=\frac{Variance\;explained\;by\;a\;model}{Variance\;NOT\;explained\;by\;a\;model}$$ -->

Different tests give you different test statistics:

|Test|Test statistic|
|:----|:----|
|T-test|T-statistic|
|Correlation test|Correlation coefficient|
|ANOVA|F-statistic|

Generally, the <high>more extreme</high> (i.e.; highly positive or highly negative) your test statistic is, the <high>more evidence against</high> the null hypothesis.


]


.pull-right5[
<br><br>

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/beer_null.png?token=AIFo1GFEeag875nlAbAbeS8rdQAx0mW6ks5cq2_1wA%3D%3D")
```

]

---
.pull-left55[

# 8: P-value

p-values are used to quantify the likelihood of data given the <high>probability distribution</high> under the <high>null hypothesis</high>

|p-value Definition|
|:----|
|A p-value is the probability of obtaining test statistics as extreme as what you got assuming a null hypothesis is true|

> "How likely is it that I would get a test statistic of -2.51 if the mean amount of beer really is 33dl?"

### p < .05 = Reject H0

If a p-value is *small* (i.e.; p < .05), this means that the likelihood of obtaining that data given the null hypothesis is equally *small*, suggesting that the null hypothesis is *wrong*

<high>Warning!</high> Wait until the New Statistics lecture for alternatives to this approach!

]


.pull-right45[


```{r, echo = FALSE, out.width = "90%"}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/pvalue_level.png?token=AIFo1Djs5aBnmWvuT7bFn7IyTzZJ-bQ_ks5cq215wA%3D%3D")
```



]

---


.pull-left5[

# What about Oetlinger?

|Step|Result|
|:----|:-----|
|Null Hypothesis (H0)|The mean amount of beer poured by Oetlinger is (exactly) equal to 33ml.|
|Sample Statistics|Mean = 33, Standard Deviation = 3.90.|
|Test statistic and p-value|We convert our stample statistics to a test statistic of <high>t = -2.51</high> and a <high>p-value</high> of <high>p = 0.0273|

<high>Conclusion</high>: Using a p < 0.05 threshold, we conclude that, because likelihood of obtaining the data if the null hypothesis is true is only .0273 (< 0.05), then the null hypothesis is likely wrong, and *the Oetlinger buvette is consistently pouring less than 33dl!*

*Note: These data are completely made up :) We're sure the Oetlinger Buvette is not ripping us off*

]


.pull-right45[
<br><br>
```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/beer_null_p.png?token=AIFo1KnzD3U7_pjvmk0K09hE2HkXISmAks5cq3AGwA%3D%3D")
```


]


---


.pull-left5[

# What about Oetlinger?

|Step|Result|
|:----|:-----|
|Null Hypothesis (H0)|The mean amount of beer poured by Oetlinger is (exactly) equal to 33ml.|
|Sample Statistics|Mean = 33, Standard Deviation = 3.90.|
|Test statistic and p-value|We convert our stample statistics to a test statistic of <high>t = -2.51</high> and a <high>p-value</high> of <high>p = 0.0273|

<high>Conclusion</high>: Using a p < 0.05 threshold, we conclude that, because likelihood of obtaining the data if the null hypothesis is true is only .0273 (< 0.05), then the null hypothesis is likely wrong, and *the Oetlinger buvette is consistently pouring less than 33dl!*

*Note: These data are completely made up :) We're sure the Oetlinger Buvette is not ripping us off*

]


.pull-right45[
<br>
### Doing our test in R!

```{r}
# Define beer sample
beer <- c(28, 31, 28, 37, 30, 
          33, 25, 33, 24, 30)

# Conduct one-sample t-test
t.test(x = beer, 
       mu = 33)
```


]



---
class: middle, center

# Questions?

<h1><a href="https://therbootcamp.github.io/SwR_2019Apr/index.html">Schedule</a></h1>


