---
title: "Intro to Statistics"
author: "Statistics with R<br>
  <a href='https://therbootcamp.github.io'>
    Basel R Bootcamp
  </a>
  <br>
  <a href='https://therbootcamp.github.io/SwR_2019Apr/'>
    <i class='fas fa-clock' style='font-size:.9em;'></i>
  </a>&#8239; 
  <a href='https://therbootcamp.github.io'>
    <i class='fas fa-home' style='font-size:.9em;' ></i>
  </a>&#8239;
  <a href='mailto:therbootcamp@gmail.com'>
    <i class='fas fa-envelope' style='font-size: .9em;'></i>
  </a>&#8239;
  <a href='https://www.linkedin.com/company/basel-r-bootcamp/'>
    <i class='fab fa-linkedin' style='font-size: .9em;'></i>
  </a>"
date: "February 2019"
output:
  xaringan::moon_reader:
    css: ["default", "baselrbootcamp.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
---

layout: true

<div class="my-footer">
  <span style="text-align:center">
    <span> 
      <img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/by-sa.png" height=14 style="vertical-align: middle"/>
    </span>
    <a href="https://therbootcamp.github.io/">
      <span style="padding-left:82px"> 
        <font color="#7E7E7E">
          www.therbootcamp.com
        </font>
      </span>
    </a>
    <a href="https://therbootcamp.github.io/">
      <font color="#7E7E7E">
       Statistics with R | April 2019
      </font>
    </a>
    </span>
  </div> 

---

```{r, eval = TRUE, echo = FALSE, warning=F,message=F}
# Code to knit slides

```

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
# see: https://github.com/yihui/xaringan
# install.packages("xaringan")
# see: 
# https://github.com/yihui/xaringan/wiki
# https://github.com/gnab/remark/wiki/Markdown
options(width = 110)
options(digits = 4)

# Load packages
require(tidyverse)
library(cowplot)

knitr::opts_chunk$set(fig.align = "center", warning = FALSE, message = FALSE)


# Load data
baselers <- readr::read_csv("1_Data/baselers.csv")

# get color palette functions
source("https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_materials/code/baselrbootcamp_palettes.R")

```


# Statistics

.pull-left45[


]

.pull-right45[

<p align = "center">
<img src="image/XX" height=440px><br>
<font style="font-size:10px">from <a href="https://xkcd.com/1781/">xkcd.com</a></font>
</p>

]

---

.pull-left45[

# Topics

- Motivating example
- Samples versus populations
- Variables
    - Something of interest that varies
- Distributions (random variables)
    - Definition
    - Examples
        - Normal distribution
        - Uniform
    - Parameters
- Estimating parameters
    - Standard deviation
    - Mean
- Likelihood
   - How likely is an observation given an assumption.
- Null hypothesis testing
    - Null versus alternative hypothesis
    - test-statistic
    - p-value (How likely was I to get a result as extreme or more extreme than the one I got assuming the null is true)
- Decision making
    - p < .05 convention
- Correlation versus causation
    - Classic fails:
        - Ice cream and forest fires.
        - Chocolate and IQ
    - Inferring causation requires an experiment and/or causal modelling

]

.pull-right45[
# Examples

- Classic tests (ESP, 'fields')
- Effect of a drug on life expectancy
- World happiness / economic indicators 

]


---

.pull-left45[

7 key concepts

1. Variability
    - Sample versus population
2. Distributions
    - Different shapes
    - Min, max
    - Mean
    - Standard deviation
3. Models
    - An Equation that describes how one variable varies as a function of other(s)
4. Parameters
    - Numbers in a model that can change
5. Likelihood
    - Error distribution
6. Test statistic
    - Unit-free method of helping us calculate the (un)likelihood of data given a model and certain parameters.
    - Variance explaiend by the model (effect) / Variance not explained by the model (error)
7. P-value
   - Probability 
8. Additional
   - Random sampling, iid.
   - Correlation vs. causation.
   

]


.pull-right45[



]

---

.pull-left6[

# Example: 

Basel has many nice "Buvette's" that serve drinks in warm months.

The Oetlinger BuvettE, one of our favorites, offers 33dl beers (or so they say...).

Is the Oetlinger Buvette 'underpouring' its beers?

### Data

Order 10 beers, and measure the exact amount in each cup.

```{r, echo = FALSE, out.width = "80%", fig.align = 'center'}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/10_beers.jpg?token=AIFo1GFPQoYDXYW5W7FOsftG_-XbNsp6ks5cqeGfwA%3D%3D")
```

]


.pull-right35[
<br><br>
```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("https://www.basel.com/extension/portal-basel/var/storage/images/media/bibliothek/basel-bilder/rhein-promenade/savoir-vivre-am-basler-rheinufer/92121-1-ger-DE/Savoir-vivre-am-Basler-Rheinufer_front_magnific.jpg")
```



```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("https://files.newsnetz.ch/story/1/7/7/17765134/14/topelement.jpg")
```


]


---


.pull-left45[

# 1. Variability


All interesting data processes have <high>variability</high>

- Stock prices, patient response to drugs, product manufacturing

Statistical inference is all about <high>accounting for variability</high>

If there was no variability, there would be no need to do statistics.

- If every single patient responded the exact same way to a drug, you could do a clinical trial with (any) one patient.


]


.pull-right45[


```{r, echo = FALSE, out.width = "90%"}
knitr::include_graphics("https://static.seekingalpha.com/uploads/2017/1/6/saupload_Coach-Stock-Price.png")
```


```{r, echo = FALSE, out.width = "90%"}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/cinicaltrial_wide.png?token=AIFo1HsHC0LwU8xjFOB4tKAdFGwNML1Iks5cqNM8wA%3D%3D")
```


```{r, echo = FALSE, out.width = "90%"}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/oetlinger_beer_single_wide.png?token=AIFo1FeCgh_xObYc6YrdafPUsK401Bd1ks5cqeeGwA%3D%3D")
```

]



---


.pull-left45[

# 1. Variability

In general, we distinguish between two types of variability

|Variability Type|Definition|
|:-----|:-----|
|Systematic| Variation that <high>can</high> be explained by known variables|
|Unsystematic |Variation that <high>cannot</high> be explained by known variables|

Statistical inference is the practice of separating overall variability into systematic and unsystematic portions.


]


.pull-right45[


```{r, echo = FALSE, out.width = "90%"}
knitr::include_graphics("https://static.seekingalpha.com/uploads/2017/1/6/saupload_Coach-Stock-Price.png")
```


```{r, echo = FALSE, out.width = "90%"}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/cinicaltrial_wide.png?token=AIFo1HsHC0LwU8xjFOB4tKAdFGwNML1Iks5cqNM8wA%3D%3D")
```


```{r, echo = FALSE, out.width = "90%"}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/oetlinger_beer_single_wide.png?token=AIFo1FeCgh_xObYc6YrdafPUsK401Bd1ks5cqeeGwA%3D%3D")
```

]



---

.pull-left45[

# 2. Distributions

Statistics is built on <high>calculating the likelihood of data</high> given how we assume it varies

To make calculations, we need to define variability according to a <high>probability distribution</high>.

A probability distribution is a <high>mathematical formula</high> that precisely defines <high>how likely</high> every possible value in a dataset is.

Probability distributions are <high>always positive</high> and their <high>sum adds up to 1.0</high>

There are many probability distributions, but we often only use a few.

]


.pull-right45[

### 3 key aspects of a distribution

```{r, echo = FALSE, fig.width = 4, fig.height = 2, out.width = "100%", dpi=200}

p1 <- ggplot(data = data.frame(x = c(60, 140)), aes(x)) +
  stat_function(fun = dnorm, n = 1001, args = list(mean = 100, sd = 10), col = baselrbootcamp_cols("green"),size = 2) + 
  ylab("Likelihood") +
  scale_y_continuous(breaks = NULL) +
  xlab("x")
  
p1
```

*1. Probability Density Function (PDF)*

A formula that defines the distribution (you rarely need to know this)

*2. Support*

What values can x take on?

*3. Parameters*

Values that allow you to change the shape of the distribution? (e.g.; mean and variability?)

]


---

.pull-left45[

## Normal (Gaussian) Distribution


A 'bell-shaped curve'

The most important and widely used distribution in all of statistics


```{r, echo = FALSE, fig.width = 4, fig.height = 3, out.width = "100%", dpi=200}

p1 <- ggplot(data = data.frame(x = c(60, 140)), aes(x)) +
  stat_function(fun = dnorm, n = 1001, args = list(mean = 110, sd = 10), col = baselrbootcamp_cols("green"),size = 2) + 
    stat_function(fun = dnorm, n = 1001, args = list(mean = 90, sd = 5), col = baselrbootcamp_cols("yellow"),size = 2) + 
  ylab("Likelihood") +
  scale_y_continuous(breaks = NULL) +
  xlab("x") +
  annotate("text", x = 90, .09, label =parse(text = paste0('mu == 90 ~ sigma == 5')), size = 5) +
annotate("text", x = 120, .05, label =parse(text = paste0('mu == 110 ~ sigma == 10')), size = 5) +
  ylim(c(0, .1))

  
p1
```

]


.pull-right45[

### Details

*Probability Density Function (PDF)*

$$\large f(x)= \frac{1}{{\sigma \sqrt {2\pi } }} e^{(x-\mu)^2/2\sigma^2}$$

*Support*


$$\large x \; \epsilon (-\infty, \infty)$$

*Parameters*

|Parameter|Meaning|
|:-----|:-------|
|$$\mu$$ | Center (mean)|
|$$\sigma$$| Variability (standard deviation)|
]


---

.pull-left45[

## Uniform Distribution


A 'Flat distribution'

Used when everything (within a range) is equally likely


```{r, echo = FALSE, fig.width = 4, fig.height = 3, out.width = "100%", dpi=200}

p1 <- ggplot(data = data.frame(x = c(60, 130)), aes(x)) +
  stat_function(fun = dunif, n = 1001, args = list(min = 80, max = 120), col = baselrbootcamp_cols("green"),size = 2) + 
    stat_function(fun = dunif, n = 1001, args = list(min = 70, max = 85), col = baselrbootcamp_cols("yellow"),size = 2) + 
  ylab("Likelihood") +
  scale_y_continuous(breaks = NULL) +
  xlab("x") +
  annotate("text", x = 77.5, .08, label =parse(text = paste0('a == 90 ~ b == 5')), size = 5) +
annotate("text", x = 105, .035, label =parse(text = paste0('a == 80 ~ b == 120')), size = 5) +
  ylim(c(0, .1))

  
p1
```

]


.pull-right45[

### Details


*Probability Density Function (PDF)*

$$\Large f(x)= \frac{1}{b-a}$$

*Support*


$$\Large x \; \epsilon (a, b)$$


*Parameters*

|Parameter|Meaning|
|:-----|:-------|
|$$a$$ | Minimum|
|$$b$$| Maximum|
]


---

.pull-left45[

## Binomial Distribution

A discrete "Counting" distribution

If I flip a coin N times, with p(Head) = p, how many times will I get heads?

```{r, echo = FALSE, fig.width = 4, fig.height = 3, out.width = "100%", dpi=200}

d1 <-  data.frame(x = seq(0, 10, 1),
                               y = dbinom(seq(0, 10, 1), size = 10, prob = .5))


d2 <-  data.frame(x = seq(0, 10, 1),
                               y = dbinom(seq(0, 10, 1), size = 10, prob = .1))


p1 <- ggplot(data =d2, 
             aes(x = x, y = y)) +
  geom_point(data = d1, col = baselrbootcamp_cols("green"), size = 2) +
  geom_line(data = d1, size = .5, col = "black", lty = 3) + 
  geom_point(data = d2, col = baselrbootcamp_cols("yellow"), size = 2) +
  geom_line(data = d2, size = .5, col = "black", lty = 3) + 
  ylab("Likelihood") +
  ylim(0, .5) + 
  annotate("text", x = 2, y = .45, label =parse(text = paste0('p == .1 ~ n== 10')), size = 5) +
  annotate("text", x = 5, y = .3, label =parse(text = paste0('p == .5 ~ n== 10')), size = 5) +
  scale_x_continuous(breaks = 0:10)

p1

```

]


.pull-right45[

### Details


*Probability Density Function (PDF)*

$$\large p(x) = {n \choose x}p^x(1-p)^{n-x}$$
*Support*


$$\large x \; \epsilon \{0, 1, ...n\}$$

*Parameters*

|Parameter|Meaning|
|:-----|:-------|
|$$p$$| Probability of success on each trial|
|$$n$$ | Number of trials (flips)|

]




---

.pull-left45[

# 4: Likelihood

Why do we need distributions? To calculate <high>likelihoods</high> of data.

- How likely is it that I would get this trial result if the drug is *really* better than a placebo?

Knowing this likelihood allows us to <high>fit parameters</high> and <high>test</high> models

- Given that out of 10 patients, the average recovery time was 2.3 days, what is the most likely distribution of recovery times for future patients?

]


.pull-right45[


```{r, echo = FALSE, out.width = "85%"}
knitr::include_graphics("https://www.xofluza.com/content/dam/gene/xofluza/hcp/images/Mobile/Xofluza-Flu-Efficacy-Primary-Endpoint-TTAS-01-Mobile.jpg")
```



```{r, echo = FALSE, fig.width = 4, fig.height = 3, out.width = "80%", dpi=200}

p1 <- ggplot(data = data.frame(x = c(0, 10)), aes(x)) +
  stat_function(fun = dnorm, n = 1001, args = list(mean = 3, sd = 2), col = baselrbootcamp_cols("green"),size = 2) + 
    stat_function(fun = dnorm, n = 1001, args = list(mean = 1, sd = 3), col = baselrbootcamp_cols("yellow"),size = 2) + 
  ylab("Likelihood") +
  scale_y_continuous(breaks = NULL) +
  xlab("x") +
  annotate("text", x = 3, .23, label =parse(text = paste0('mu == 3 ~ sigma == 2')), size = 5) +
annotate("text", x = 2, .15, label =parse(text = paste0('mu == 1 ~ sigma == 3')), size = 5) +
  ylim(c(0, .25)) +
  scale_x_continuous(breaks = 0:10)

  
p1
```


]

---

.pull-left45[

# 4: Likelihood

Using the binomial distributions on the right, answer the following questions:

- If there is a 50% chance of a drug being successful, then out of 10 drugs, how likely is it that exactly 5 will be successful?

- If there is a 10% chance that a customer will default on his/her loan, then out of 10 customers, how likely is it that none (0) will default?

]


.pull-right45[

<br><br>

### 2 Binomial Distributions

```{r, echo = FALSE, fig.width = 4, fig.height = 3, out.width = "100%", dpi=200}

d1 <-  data.frame(x = seq(0, 10, 1),
                               y = dbinom(seq(0, 10, 1), size = 10, prob = .5))

d2 <-  data.frame(x = seq(0, 10, 1),
                               y = dbinom(seq(0, 10, 1), size = 10, prob = .1))


p1 <- ggplot(data =d2, 
             aes(x = x, y = y)) +
  geom_hline(yintercept =  seq(0, .5, .1), col = "gray", size = .3, alpha = .5) +
  geom_vline(xintercept =  seq(0, 10), col = "gray", size = .3, alpha = .5) +
  geom_point(data = d1, col = baselrbootcamp_cols("green"), size = 2) +
  geom_line(data = d1, size = .5, col = "black", lty = 3) + 
  geom_point(data = d2, col = baselrbootcamp_cols("yellow"), size = 2) +
  geom_line(data = d2, size = .5, col = "black", lty = 3) + 
  ylab("Likelihood") +
  ylim(0, .5) + 
  annotate("text", x = 2, y = .45, label =parse(text = paste0('p == .1 ~ n== 10')), size = 5) +
  annotate("text", x = 5, y = .3, label =parse(text = paste0('p == .5 ~ n== 10')), size = 5) +
  scale_x_continuous(breaks = 0:10)

p1

```


]

---


.pull-left55[

# 6: Test statistic

Test statistics are <high>distribution specific</high> numbers that help you quantify evidence for a statistical model

<!-- $$Test\;Statistic=\frac{Variance\;explained\;by\;a\;model}{Variance\;NOT\;explained\;by\;a\;model}$$ -->

Different tests give you different test statistics:

|Test|Test statistic|
|:----|:----|
|T-test|T-statistic|
|Correlation test|Correlation coefficient|
|Binomial|Number of successes|

Generally, the more extreme your test statistic is, the more evidence that "Something is going on"


]


.pull-right35[






]

---
.pull-left55[

# 7: P-value

p-values are used to quantify the likelihood of data given a specific distribution.


|Definition|
|:----|
|A p-value is the probability of obtaining data as extreme or more extreme than what you got assuming a specific model is true.|


Null hypothesis testing

If a p-value is *small* (i.e.; p < .05), this means that the likelihood of obtaining that data given the model is small.



]


.pull-right45[



]

---

# 8: Additional considerations

.pull-left45[



]


.pull-right45[



]



---

# Looking ahead




---

class: middle, center

<h1><a href="https://therbootcamp.github.io/SwR_2019Apr/_sessions/RegressionI/RegressionI_practical.html">Practical</a></h1>


---
# Backup




---

.pull-left45[

# 4: Likelihood

Using the binomial distributions on the right, answer the following questions:

- You purchase a XX online
- You fear it may be fake

Data:

- You used it 10 times and it fails 5 times (5 out of 10).

Online research shows that 

- Real XX fail 10% of the time
- Fake XX fail 50% of the time.

Question

- How likely is your data if XX is real? What about if it's fake?
- Is it *more likely* that a real XX would give you your data or a fake XX?


]


.pull-right45[

<br><br>


### 2 Binomial Distributions

```{r, echo = FALSE, fig.width = 4, fig.height = 3, out.width = "100%", dpi=200}

d1 <-  data.frame(x = seq(0, 10, 1),
                               y = dbinom(seq(0, 10, 1), size = 10, prob = .5))


p1 <- ggplot(data =d2, 
             aes(x = x, y = y)) +
  geom_hline(yintercept =  seq(0, .5, .1), col = "gray", size = .3, alpha = .5) +
  geom_vline(xintercept =  seq(0, 10), col = "gray", size = .3, alpha = .5) +
  geom_point(data = d1, col = baselrbootcamp_cols("green"), size = 2) +
  geom_line(data = d1, size = .5, col = "black", lty = 3) + 
  geom_point(data = d2, col = baselrbootcamp_cols("yellow"), size = 2) +
  geom_line(data = d2, size = .5, col = "black", lty = 3) + 
  ylab("Likelihood") +
  ylim(0, .5) + 
  annotate("text", x = 2, y = .45, label =parse(text = paste0('p == .1 ~ n== 10')), size = 5) +
  annotate("text", x = 5, y = .3, label =parse(text = paste0('p == .5 ~ n== 10')), size = 5) +
  scale_x_continuous(breaks = 0:10)

p1

```



]

