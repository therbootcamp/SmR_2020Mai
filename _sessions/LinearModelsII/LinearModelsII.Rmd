---
title: "Linear Models II"
author: "Statistics with R<br>
  <a href='https://therbootcamp.github.io'>
    Basel R Bootcamp
  </a>
  <br>
  <a href='https://therbootcamp.github.io/SwR_2019Apr/'>
    <i class='fas fa-clock' style='font-size:.9em;'></i>
  </a>&#8239; 
  <a href='https://therbootcamp.github.io'>
    <i class='fas fa-home' style='font-size:.9em;' ></i>
  </a>&#8239;
  <a href='mailto:therbootcamp@gmail.com'>
    <i class='fas fa-envelope' style='font-size: .9em;'></i>
  </a>&#8239;
  <a href='https://www.linkedin.com/company/basel-r-bootcamp/'>
    <i class='fab fa-linkedin' style='font-size: .9em;'></i>
  </a>"
date: "April 2019"
output:
  xaringan::moon_reader:
    css: ["default", "baselrbootcamp.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
---

layout: true

<div class="my-footer">
  <span style="text-align:center">
    <span> 
      <img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/by-sa.png" height=14 style="vertical-align: middle"/>
    </span>
    <a href="https://therbootcamp.github.io/">
      <span style="padding-left:82px"> 
        <font color="#7E7E7E">
          www.therbootcamp.com
        </font>
      </span>
    </a>
    <a href="https://therbootcamp.github.io/">
      <font color="#7E7E7E">
       Statistics with R | April 2019
      </font>
    </a>
    </span>
  </div> 

---

```{r, eval = TRUE, echo = FALSE, warning=F,message=F}
# Code to knit slides

```

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
# see: https://github.com/yihui/xaringan
# install.packages("xaringan")
# see: 
# https://github.com/yihui/xaringan/wiki
# https://github.com/gnab/remark/wiki/Markdown
options(width = 110)
options(digits = 4)

# Load packages
require(tidyverse)

knitr::opts_chunk$set(dpi = 300, echo = TRUE, warning = FALSE, fig.align = 'center', warning = FALSE)

source("https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_materials/code/baselrbootcamp_palettes.R")

library(broom)

# Load data
baselers <- readr::read_csv("1_Data/baselers.csv")

baselers <- baselers %>%
  mutate(education = case_when(education == "apprenticeship" ~ "High School",
                               education == "obligatory_school" ~ "BA",
                               education == "SEK_II" ~ "MS",
                               education == "SEK_III" ~ "PhD"
                               ))

```


```{r, echo = FALSE}

set.seed(105)
x <- rnorm(50, mean = 50, sd = 10)

id <- 1:50
drug <- sample(c("Drug", "Placebo"), size = 50, replace = TRUE)
effect <- rnorm(50)
effect[drug == "Drug"] <- 2.3 + rnorm(sum(drug == "Drug"), mean = 0, sd = 1.5) - .41
effect[drug == "Placebo"] <- 3.3 + rnorm(sum(drug == "Placebo"), mean = 0, sd = 1.5) - .12

dat <- tibble(id, drug, effect, group = rep("Baselers", 50))
avg <- dat %>%
  group_by(drug) %>%
  summarise(days_mean = mean(effect))

drug_placebo_gg <- ggplot(dat, aes(factor(drug), effect)) +
  geom_jitter(width = .05) +
  labs(y = "Days",
       x = "Condition",
       title = "Fictional data") +
  scale_y_continuous(breaks = seq(0, 10, 1), limits = c(0, 7)) +
  geom_label(data = avg, aes(x = drug, y = days_mean, label = round(days_mean, 2)))

```


# Linear Model Applications

.pull-left5[

<u>Linear Model Equation</u>

$$\Large y = \beta_{0} + \beta_{1}x_{1} + \beta_{2} x_{2} +  ... + \beta_{n}x_{n} + \epsilon$$

<br>

<u>Hypothesis tests are linear models!</u>

In fact, many of your favorite hypothesis tests, including <high>t-tests</high>, <high>correlation tests</high>, and <high>ANOVAs</high> can all be expressed as linear models! 

This means that you can use the `lm()` or `glm()` function to do all of these tests!

However, R also has special <high>hypothesis test functions</high> with more user-friendly outputs.


]

.pull-right4[

<p align = "center">
  <img src="image/linear_model_vendiagram.png" height="430px">
</p>

]


---


.pull-left55[

# Linear Model Applications

All of these tests assume your dependent variable is <high>Normally distributed</high>.

```{r, echo = FALSE, fig.width = 6, fig.height = 2, out.width = "60%"}

dat <- tibble(x = rnorm(1000, mean = 100, sd = 10),
              y = rep(.001, 1000) + rnorm(1000, mean = 0, sd = .0005))

p1 <- ggplot(data = data.frame(x = c(60, 140)), aes(x)) +
  stat_function(fun = dnorm, n = 1001, args = list(mean = 100, sd = 10), col = baselrbootcamp_cols("green"),size = 2) + 
  ylab("") +
  annotate("point", x = dat$x, y = dat$y, alpha = .1) +
  scale_y_continuous(breaks = NULL) +
  xlab("x")

  
p1
```

What differentiates these tests is the scale of your independent variable

### Independent Variable Scales

|Scale|Description|Examples|
|:-----|:------|
|Nominal|A discrete category without order|Sex, College, Favorite Color|
|Ratio|A continuous number|Income, Height, |


]

.pull-right4[

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/linear_model_vendiagram.png?token=AIFo1EwgCqkIpF_GPncSc5Y7WbO0S7Maks5crjjrwA%3D%3D")
```

]

---

.pull-left65[

# Null hypothesis testing

Null hypothesis testing is a statistical framework where two alternative hypotheses (the Null and the Alternative) are compared

|Hypothesis|Description|Example|
|:-----|:-----|:-------|
|Null (H0)|A proposed effect <high>does not exist</high>|Drug and placebo have the same effect|
|Alternative (H1)|A proposed effect <high>does exist</high>|Drug and placebo do *not* have the same effect|

[Wikipedia: Null Hypothesis](https://en.wikipedia.org/wiki/Null_hypothesis)


]


.pull-right3[
<br><br><br>
Are these data consistent with H0?

```{r, echo = FALSE, fig.width = 3, fig.height = 3, fig.cap }
drug_placebo_gg
```


]


---

.pull-left45[

# Correlation test

### *Does Y tend to change when X changes?*

Conduct a <high>correlation test</high> when you have 2 continuous, Normally distributed independent variables X and Y.

### Formula

$$\LARGE Y=\beta_{0}+\beta_{1}x$$

$$\LARGE \beta_{1}=\rho\frac{\sigma_{y}}{\sigma_{x}}$$

<high>p</high> is the <high>population correlation</high> between $x$ and $Y$

<high>r</high> is the <high>sample correlation</high> between $x$ and $Y$


]

.pull-right45[
<br>

```{r, echo = FALSE, fig.width = 4, fig.height = 4, message = FALSE, warning = FALSE}
set.seed(104)
x <- rnorm(20, mean = 10, sd = 2)
pos_low <- x * 1 + rnorm(20, sd = 5)

pos_low_r <- cor(x, pos_low)


dat <- tibble(x, pos_low)

pos_low_gg <- ggplot(dat, aes(x = x, y = pos_low)) +
  geom_point() +
  labs(x = "x (Normally Distributed)", y = "y (Normally Distributed)", title = "Ready for a Correlation test!")

pos_low_gg

```


]


---

.pull-left4[

# Correlation test

### Correlation Coefficient

```{r, echo = FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/correlation_coefficien_diagram.png?token=AIFo1BALlc9Z4fF-bO-VIsFcMfJn3zoQks5cr6w9wA%3D%3D")
```

### Hypotheses


Null: $H_{0}:\rho  = 0$, "There <high>is no</high> correlation in the population"

Alternative: $H_{A}:\rho  \neq 0$, "There <high>is a </high> (non-zero) correlation in the population!"

]

.pull-right45[

<br><br>
```{r, echo = FALSE, fig.width = 5.25, fig.height = 5.25}
set.seed(104)
x <- rnorm(20, mean = 10, sd = 2)
pos_low <- x * 1 + rnorm(20, sd = 5)
pos_high <- x * 1 + rnorm(20, sd = .5)
neg_low <- x * -1 + rnorm(20, sd = 5)
neg_high <- x * -1 + rnorm(20, sd = .5)

pos_low_r <- cor(x, pos_low)
pos_high_r <- cor(x, pos_high)
neg_low_r <- cor(x, neg_low)
neg_high_r <- cor(x, neg_high)


dat <- tibble(x, pos_low, pos_high, neg_low, neg_high)

pos_low_gg <- ggplot(dat, aes(x = x, y = pos_low)) +
  geom_point() +
  geom_smooth(se = FALSE, method = "lm", col = baselrbootcamp_cols("magenta")) +
  labs(title = "Low, positive correlation",
       subtitle = paste0("r = ", round(pos_low_r, 2)), x = "x", y = "y")

pos_high_gg <- ggplot(dat, aes(x = x, y = pos_high)) +
  geom_point() +
  geom_smooth(se = FALSE, method = "lm", col = baselrbootcamp_cols("magenta")) +
  labs(title = "High, negative correlation",
       subtitle = paste0("r = ", round(pos_high_r, 2)), x = "x", y = "y")

neg_low_gg <- ggplot(dat, aes(x = x, y = neg_low)) +
  geom_point() +
  geom_smooth(se = FALSE, method = "lm", col = baselrbootcamp_cols("magenta")) +
  labs(title = "Low, negative correlation",
       subtitle = paste0("r = ", round(neg_low_r, 2)), x = "x", y = "y")

neg_high_gg <- ggplot(dat, aes(x = x, y = neg_high)) +
  geom_point() +
  geom_smooth(se = FALSE, method = "lm", col = baselrbootcamp_cols("magenta")) +
  labs(title = "High, negative correlation",
       subtitle = paste0("r = ", round(neg_high_r, 2)), x = "x", y = "y")


ggpubr::ggarrange(pos_low_gg, neg_low_gg, pos_high_gg, neg_high_gg, ncol = 2, nrow = 2)
```


]


---

.pull-left4[

# Correlation test

### Correlation Coefficient

```{r, echo = FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/correlation_coefficien_diagram.png?token=AIFo1BALlc9Z4fF-bO-VIsFcMfJn3zoQks5cr6w9wA%3D%3D")
```

### Hypotheses


Null: $H_{0}:\rho  = 0$, "There <high>is no</high> correlation in the population"

Alternative: $H_{A}:\rho  \neq 0$, "There <high>is a </high> (non-zero) correlation in the population!"

]

.pull-right55[
<br>
*Do older baselers have higher incomes than younger baselers?*

```{r, echo  = FALSE, warning = FALSE, fig.width = 4, fig.height = 4, dpi = 300, out.width = "90%"}
ggplot(baselers, aes(x = age, y = income)) +
  geom_point(alpha = .03, col = baselrbootcamp_cols("green")) +
  labs(title = "Age and Income of Baselers",
       caption = "Fake data :)")
```

]

---

.pull-left4[

# Correlation test

### Correlation Coefficient

```{r, echo = FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/correlation_coefficien_diagram.png?token=AIFo1BALlc9Z4fF-bO-VIsFcMfJn3zoQks5cr6w9wA%3D%3D")
```

### Hypotheses


Null: $H_{0}:\rho  = 0$, "There <high>is no</high> correlation in the population"

Alternative: $H_{A}:\rho  \neq 0$, "There <high>is a </high> (non-zero) correlation in the population!"

]

.pull-right55[
<br>
### Correlation test with `cor.test()`

```{r}
# Is there a relationship between age and income?

income_htest <- cor.test(formula = ~ age + income,
                         data = baselers)

# Print result
income_htest
```


]

---

.pull-left4[

# Correlation test

### Correlation Coefficient

```{r, echo = FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/src/img/correlation_coefficien_diagram.png?token=AIFo1BALlc9Z4fF-bO-VIsFcMfJn3zoQks5cr6w9wA%3D%3D")
```

### Hypotheses


Null: $H_{0}:\rho  = 0$, "There <high>is no</high> correlation in the population"

Alternative: $H_{A}:\rho  \neq 0$, "There <high>is a </high> (non-zero) correlation in the population!"

]

.pull-right55[
<br>
### Correlation test with `cor.test()`

```{r}
# Show all named elements
names(income_htest)

# Show estimated correlation coefficient
income_htest$estimate

# Show p-value
income_htest$p.value
```

]

---
.pull-left45[

# T-test

### *Does the mean of group A differ from group B?*

Conduct a <high>t-test</high> when you have one nominal independent variable with <high>2 levels</high> A and B

### Formula

$$\LARGE Y=\beta_{0}+\beta_{1}x$$

$$\beta_{0} = Mean\;of\;group\;A, \beta_{1} = Mean\;of\;group\;B$$

|Group|x|
|:----|:----|
|Group = A|x = 0|
|Group = B|x = 1|

]


.pull-right5[

<br>
```{r, echo = FALSE, fig.width = 4, fig.height = 4}
set.seed(102)

global_mean <- 12

dat <- tibble(Group = rep(c("A", "B"), each = 50),
                             y = c(rnorm(50, mean = global_mean, sd = 3) - 1,
                                   rnorm(50, mean = global_mean, sd = 3) + 3))

t <- t.test(y ~ Group, data = dat)$statistic %>% round(2)
p <- t.test(y ~ Group, data = dat)$p.value %>% round(3)

mean_c <- dat %>% group_by(Group) %>% summarise(y = mean(y)) %>% pull(y) %>% round(2)
sd_avg <- dat %>% group_by(Group) %>% summarise(y = sd(y)) %>% pull(y) %>% mean() %>% round(2)

a_gg <- ggplot(data = dat) +
               aes(x = Group, y = y) +
      stat_summary(geom = "bar", fun.y = "mean", col = "black", fill = "white") +
                 geom_jitter(width = .1) +
  annotate("segment", x = c(1, 2), xend = c(1, 2), y = mean_c - sd_avg, yend = mean_c + sd_avg, size = 2, col = baselrbootcamp_cols("magenta")) +
  annotate("label", x = c(1, 2), y = mean_c, label = mean_c)  +
   labs(title = "Ready for a t-test!", y = "Y (Normally Distributed)") +
  ylim(0, 25) +
    coord_cartesian(clip = 'off') +
  guides(fill = FALSE)

a_gg
```


]



---
.pull-left45[

# T-test

### *Does the mean of group A differ from group B?*

Conduct a <high>t-test</high> when you have one nominal independent variable with <high>2 levels</high> A and B

### Test Statistic

$$\Large t = \frac{\bar{x}_{A}-\bar{x}_{B}}{\sqrt{s^{2}(\frac{1}{n_{A}}+\frac{1}{n_{B}})}}$$
$$\Large s^{2} = Pooled\;variance$$

*Pooled variance means "Average Variance"* 

See [formula on Wikipedia](https://en.wikipedia.org/wiki/Pooled_variance)

]

.pull-right5[
<br>

```{r, echo = FALSE, fig.width = 7, fig.height = 7}
set.seed(102)

global_mean <- 12

dat <- tibble(Group = rep(c("A", "B"), each = 50),
                             y = c(rnorm(50, mean = global_mean, sd = 3) - .5,
                                   rnorm(50, mean = global_mean, sd = 3)))

t <- t.test(y ~ Group, data = dat)$statistic %>% round(2)
p <- t.test(y ~ Group, data = dat)$p.value %>% round(3)

mean_c <- dat %>% group_by(Group) %>% summarise(y = mean(y)) %>% pull(y) %>% round(2)
sd_avg <- dat %>% group_by(Group) %>% summarise(y = sd(y)) %>% pull(y) %>% mean() %>% round(2)

a_gg <- ggplot(data = dat) +
               aes(x = Group, y = y) +
      stat_summary(geom = "bar", fun.y = "mean", col = "black", fill = "white") +
                 geom_jitter(width = .1) +
  annotate("segment", x = c(1, 2), xend = c(1, 2), y = mean_c - sd_avg, yend = mean_c + sd_avg, size = 2, col = baselrbootcamp_cols("magenta")) +
  annotate("label", x = c(1, 2), y = mean_c, label = mean_c)  +
   labs(title = paste0("t = ", t, ", p = ", p),
       subtitle = paste0("Mean difference = ", diff(mean_c) %>% round(2),  ", s2 = ", sd_avg %>% round(2))) +
  ylim(0, 25) +
    coord_cartesian(clip = 'off')


dat <- tibble(Group = rep(c("A", "B"), each = 50),
                             y = c(rnorm(50, mean = global_mean - 2, sd = 7) + .2,
                                   rnorm(50, mean = global_mean + 2, sd = 7) - .2))

t <- t.test(y ~ Group, data = dat)$statistic %>% round(2)
p <- t.test(y ~ Group, data = dat)$p.value %>% round(3)

mean_c <- dat %>% group_by(Group) %>% summarise(y = mean(y)) %>% pull(y) %>% round(2)
sd_avg <- dat %>% group_by(Group) %>% summarise(y = sd(y)) %>% pull(y) %>% mean() %>% round(2)

b_gg <- ggplot(data = dat) +
               aes(x = Group, y = y) +
      stat_summary(geom = "bar", fun.y = "mean", col = "black", fill = "white", data = dat, aes(Group, y)) +
                 geom_jitter(width = .1) +
  annotate("segment", x = c(1, 2), xend = c(1, 2), y = mean_c - sd_avg, yend = mean_c + sd_avg, size = 2, col = baselrbootcamp_cols("magenta")) +
  annotate("label", x = c(1, 2), y = mean_c, label = mean_c)  +
   labs(title = paste0("t = ", t, ", p = ", p),
       subtitle = paste0("Mean difference = ", diff(mean_c) %>% round(2),  ", s2 = ", sd_avg %>% round(2))) +
  ylim(0, 25) +
    coord_cartesian(clip = 'off')


dat <- tibble(Group = rep(c("A", "B"), each = 50),
                             y = c(rnorm(50, mean = global_mean, sd = 2) + .58,
                                   rnorm(50, mean = global_mean, sd = 2)))

t <- t.test(y ~ Group, data = dat)$statistic %>% round(2)
p <- t.test(y ~ Group, data = dat)$p.value %>% round(3)

mean_c <- dat %>% group_by(Group) %>% summarise(y = mean(y)) %>% pull(y) %>% round(2)
sd_avg <- dat %>% group_by(Group) %>% summarise(y = sd(y)) %>% pull(y) %>% mean() %>% round(2)

c_gg <- ggplot(data = dat) +
               aes(x = Group, y = y) +
    stat_summary(geom = "bar", fun.y = "mean", col = "black", fill = "white") +
                 geom_jitter(width = .1) +
  annotate("segment", x = c(1, 2), xend = c(1, 2), y = mean_c - sd_avg, yend = mean_c + sd_avg, size = 2, col = baselrbootcamp_cols("magenta")) +
  annotate("label", x = c(1, 2), y = mean_c, label = mean_c)  +
   labs(title = paste0("t = ", t, ", p = ", p),
       subtitle = paste0("Mean difference = ", diff(mean_c) %>% round(2),  ", s2 = ", sd_avg %>% round(2))) +
  ylim(0, 25) +
    coord_cartesian(clip = 'off')



dat <- tibble(Group = rep(c("A", "B"), each = 50),
                             y = c(rnorm(50, mean = global_mean - 2, sd = 2) - .25,
                                   rnorm(50, mean = global_mean + 2, sd = 2) + .25))

t <- t.test(y ~ Group, data = dat)$statistic %>% round(2)
p <- t.test(y ~ Group, data = dat)$p.value %>% round(3)

mean_c <- dat %>% group_by(Group) %>% summarise(y = mean(y)) %>% pull(y) %>% round(2)
sd_avg <- dat %>% group_by(Group) %>% summarise(y = sd(y)) %>% pull(y) %>% mean() %>% round(2)

d_gg <- ggplot(data = dat) +
               aes(x = Group, y = y) +
      stat_summary(geom = "bar", fun.y = "mean", col = "black", fill = "white") +

                 geom_jitter(width = .1) +
  annotate("segment", x = c(1, 2), xend = c(1, 2), y = mean_c - sd_avg, yend = mean_c + sd_avg, size = 2, col = baselrbootcamp_cols("magenta")) +
  annotate("label", x = c(1, 2), y = mean_c, label = mean_c)  +
   labs(title = paste0("t = ", t, ", p = ", p),
       subtitle = paste0("Mean difference = ", diff(mean_c) %>% round(2),  ", s2 = ", sd_avg %>% round(2))) +
  ylim(0, 25) +
    coord_cartesian(clip = 'off')


t_test_grid_gg <- ggpubr::ggarrange(a_gg, b_gg, c_gg, d_gg, nrow = 2, ncol = 2)

t_test_grid_gg
```

]



---
.pull-left45[

# T-test

### *Does the mean of group A differ from group B?*

Conduct a <high>t-test</high> when you have one nominal independent variable with <high>2 levels</high> A and B

### Test Statistic

$$\Large t = \frac{\bar{x}_{A}-\bar{x}_{B}}{\sqrt{s^{2}(\frac{1}{n_{A}}+\frac{1}{n_{B}})}}$$
$$\Large s^{2} = Pooled\;variance$$

*Pooled variance means "Average Variance"* 

See [formula on Wikipedia](https://en.wikipedia.org/wiki/Pooled_variance)

]

.pull-right5[

<br><br><br><br><br>
*Do male and female baselers have different mean incomes?*

```{r, echo  = FALSE, warning = FALSE, fig.width = 5, fig.height = 3.5, dpi = 300, out.width = "90%"}
ggplot(baselers, aes(x = sex, y = income, col = sex)) +
    geom_jitter(alpha = .01, width = .1, col = "black") +
   geom_boxplot() +
  labs(title = "Income of female and male baselers",
       caption = "Fake data :)",
       x = "Sex",
       y = "Income (per month)") +
  guides(col = FALSE)
```

]



---
.pull-left45[

# T-test

### *Does the mean of group A differ from group B?*

Conduct a <high>t-test</high> when you have one nominal independent variable with <high>2 levels</high> A and B

### Test Statistic

$$\Large t = \frac{\bar{x}_{A}-\bar{x}_{B}}{\sqrt{s^{2}(\frac{1}{n_{A}}+\frac{1}{n_{B}})}}$$
$$\Large s^{2} = Pooled\;variance$$

*Pooled variance means "Average Variance"* 

See [formula on Wikipedia](https://en.wikipedia.org/wiki/Pooled_variance)

]

.pull-right5[

<br>
### t-test with `t.test()`

```{r}
# 2-sample t-test
income_htest <- t.test(formula = income ~ sex,
                       data = baselers)

# Print
income_htest
```

]



---
.pull-left45[

# T-test

### *Does the mean of group A differ from group B?*

Conduct a <high>t-test</high> when you have one nominal independent variable with <high>2 levels</high> A and B

### Test Statistic

$$\Large t = \frac{\bar{x}_{A}-\bar{x}_{B}}{\sqrt{s^{2}(\frac{1}{n_{A}}+\frac{1}{n_{B}})}}$$
$$\Large s^{2} = Pooled\;variance$$

*Pooled variance means "Average Variance"* 

See [formula on Wikipedia](https://en.wikipedia.org/wiki/Pooled_variance)

]

.pull-right5[

<br>
### t-test with `t.test()`

```{r}
# Show all named elements
names(income_htest)

# Print the test statistic
income_htest$statistic

# Print the p.value
income_htest$p.value
```

]




---
.pull-left45[

# ANOVA

### *Do the means of my (many) groups differ?*

Conduct an <high>ANOVA</high> when you have one nominal independent variable with <high>more than 2 levels</high> A, B, C, ...

### Test Statistic

$$\large F = \frac{Variance\;Between\;Groups}{Variance\;Within\;Groups}$$

>Is the variability between my groups large relative to the variability within each group?

See [ANOVA formula on Wikipedia](https://en.wikipedia.org/wiki/Analysis_of_variance)
]

.pull-right5[

<br>

```{r, fig.width = 4.5, fig.height = 4, echo = FALSE}
a_y <- rnorm(100, mean = 10, sd = 2)
b_y <- rnorm(100, mean = 12, sd = 2)
c_y <- rnorm(100, mean = 10, sd = 2)

dat <- tibble(Group = rep(c("A", "B", "C"), each = 100),
              Y = c(a_y, b_y, c_y))

mean_c <- dat %>% group_by(Group) %>% summarise(x = mean(Y)) %>% pull(x) %>% round(2)
sd_avg <- dat %>% group_by(Group) %>% summarise(x = sd(Y)) %>% pull(x) %>% mean()


ggplot(data = dat, aes(x = Group, y = Y)) +
  stat_summary(geom = "bar", fun.y = "mean", col = "black", fill = "white") +
                 geom_jitter(width = .1) +
  annotate("segment", x = c(1, 2, 3), xend = c(1, 2, 3), y = mean_c - sd_avg, yend = mean_c + sd_avg, size = 2, col = baselrbootcamp_cols("magenta")) +
  annotate("label", x = c(1, 2, 3), y = mean_c, label = mean_c) +
  labs(title = "Ready for an ANOVA!")
```

]


---
.pull-left45[

# ANOVA

### *Do the means of my (many) groups differ?*

Conduct an <high>ANOVA</high> when you have one nominal independent variable with <high>more than 2 levels</high> A, B, C, ...

### Test Statistic

$$\large F = \frac{Variance\;Between\;Groups}{Variance\;Within\;Groups}$$

>Is the variability between my groups large relative to the variability within each group?

See [ANOVA formula on Wikipedia](https://en.wikipedia.org/wiki/Analysis_of_variance)

]

.pull-right5[


```{r, fig.width = 4, fig.height = 6, echo = FALSE, out.width = "80%"}
set.seed(100)

a_y <- rnorm(100, mean = 10, sd = 4) - .75
b_y <- rnorm(100, mean = 10, sd = 4) + .25
c_y <- rnorm(100, mean = 10, sd = 4) - .75

dat <- tibble(Group = rep(c("A", "B", "C"), each = 100),
              Y = c(a_y, b_y, c_y))

mean_c <- dat %>% group_by(Group) %>% summarise(x = mean(Y)) %>% pull(x) %>% round(2)
sd_avg <- dat %>% group_by(Group) %>% summarise(x = sd(Y)) %>% pull(x) %>% mean(2)

fstat <- aov(data = dat, formula = Y ~ Group) %>% tidy() %>% slice(1) %>% pull(statistic) %>% round(2)
p <- aov(data = dat, formula = Y ~ Group) %>% tidy() %>% slice(1) %>% pull(p.value) %>% round(2)


a_gg <- ggplot(data = dat, aes(x = Group, y = Y)) +
  stat_summary(geom = "bar", fun.y = "mean", col = "black", fill = "white") +
                 geom_jitter(width = .1) +
  annotate("segment", x = c(1, 2, 3), xend = c(1, 2, 3), y = mean_c - sd_avg, yend = mean_c + sd_avg, size = 2, col = baselrbootcamp_cols("magenta")) +
  annotate("label", x = c(1, 2, 3), y = mean_c, label = mean_c) +
  labs(title = paste0("F = ", fstat, ", p = ", p)) +
  ylim(c(0, 20))




a_y <- rnorm(100, mean = 10, sd = 2) - .75
b_y <- rnorm(100, mean = 10, sd = 2) + .25
c_y <- rnorm(100, mean = 10, sd = 2) - .75

dat <- tibble(Group = rep(c("A", "B", "C"), each = 100),
              Y = c(a_y, b_y, c_y))

mean_c <- dat %>% group_by(Group) %>% summarise(x = mean(Y)) %>% pull(x) %>% round(2)
sd_avg <- dat %>% group_by(Group) %>% summarise(x = sd(Y)) %>% pull(x) %>% mean(2)

fstat <- aov(data = dat, formula = Y ~ Group) %>% tidy() %>% slice(1) %>% pull(statistic) %>% round(2)
p <- aov(data = dat, formula = Y ~ Group) %>% tidy() %>% slice(1) %>% pull(p.value) %>% round(2)


b_gg <- ggplot(data = dat, aes(x = Group, y = Y)) +
  stat_summary(geom = "bar", fun.y = "mean", col = "black", fill = "white") +
                 geom_jitter(width = .1) +
  annotate("segment", x = c(1, 2, 3), xend = c(1, 2, 3), y = mean_c - sd_avg, yend = mean_c + sd_avg, size = 2, col = baselrbootcamp_cols("magenta")) +
  annotate("label", x = c(1, 2, 3), y = mean_c, label = mean_c) +
  labs(title = paste0("F = ", fstat, ", p = ", p)) +
  ylim(c(0, 20))


ggpubr::ggarrange(a_gg, b_gg, ncol = 1, nrow = 2)


```

]



---
.pull-left45[

# ANOVA

### *Do the means of my (many) groups differ?*

Conduct an <high>ANOVA</high> when you have one nominal independent variable with <high>more than 2 levels</high> A, B, C, ...

### Test Statistic

$$\large F = \frac{Variance\;Between\;Groups}{Variance\;Within\;Groups}$$

>Is the variability between my groups large relative to the variability within each group?

See [ANOVA formula on Wikipedia](https://en.wikipedia.org/wiki/Analysis_of_variance)

]

.pull-right5[

### ANOVA with `aov()`

<br>
*Is there a difference in height based on education?*

```{r, echo  = FALSE, warning = FALSE, fig.width = 4, fig.height = 3, dpi = 300, out.width = "100%"}
ggplot(baselers, aes(x = education, y = height, col = education)) +
  geom_jitter(alpha = .05, width = .1, col = "black") +
     geom_boxplot() +
  labs(title = "Baseler height based on education",
       caption = "Fake data :)") +
  guides(col = FALSE)
```

]



---
.pull-left45[

# ANOVA

### *Do the means of my (many) groups differ?*

Conduct an <high>ANOVA</high> when you have one nominal independent variable with <high>more than 2 levels</high> A, B, C, ...

### Test Statistic

$$\large F = \frac{Variance\;Between\;Groups}{Variance\;Within\;Groups}$$

>Is the variability between my groups large relative to the variability within each group?

See [ANOVA formula on Wikipedia](https://en.wikipedia.org/wiki/Analysis_of_variance)

]

.pull-right5[

### ANOVA with `aov()`

<br>
*Is there a difference in height based on education?*

```{r}
# Is there a relationship btwn education and height?

ht_htest <- aov(formula = height ~ education,
                data = baselers)

# Print result
ht_htest
```

]




---
.pull-left45[

# ANOVA

### *Do the means of my (many) groups differ?*

Conduct an <high>ANOVA</high> when you have one nominal independent variable with <high>more than 2 levels</high> A, B, C, ...

### Test Statistic

$$\large F = \frac{Variance\;Between\;Groups}{Variance\;Within\;Groups}$$

>Is the variability between my groups large relative to the variability within each group?

See [ANOVA formula on Wikipedia](https://en.wikipedia.org/wiki/Analysis_of_variance)

]

.pull-right5[

### ANOVA with `aov()`

<br>
*Is there a difference in height based on education?*

```{r}
# Show summary results
summary(ht_htest)
```

]


---
.pull-left35[

# ANOVA - Post-hoc tests

### *Which groups differ?*

After conducting an <high>ANOVA</high>, conduct a <high>post-hoc test</high> to see which specific pairs of groups differ
.
### Test Statistic

$$\large t = \frac{\bar{x}_{A}-\bar{x}_{B}}{Total\;Variability}$$

See [Tukey's Test article on Wikipedia](https://en.wikipedia.org/wiki/Tukey%27s_range_test)

]

.pull-right6[

### Post-hoc tests with `TukeyHSD()`

<br>
*Is there a difference in height based on education?*

```{r}
# Conduct post-hoc tests
#   Which pairs of groups differ?

TukeyHSD(ht_htest)
```

]






---

.pull-left4[

# `tidy()`

The `tidy()` function from the `broom` package <high>converts</high> the most important results of many statistical objects to a <high>data frame</high>.


```{r, out.width = "45%", echo = FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/Erfurt_2018June/master/_sessions/_image/broom_hex.png")
```

```{r, eval = FALSE, echo = TRUE}
# install and load broom
install.packages('broom')
library(broom)
```


Try `tidy()` on your favorite statistical object and see what you get!

]

.pull-right55[

<br><br>

```{r, eval = TRUE, echo = TRUE}
# Load broom package
library(broom)   # For tidy()

# Conduct correlation test
income_htest <- cor.test(formula = ~ height + income,
                         data = baselers)

# Standard printout
income_htest
```

]


---

.pull-left4[

# `tidy()`

The `tidy()` function from the `broom` package <high>converts</high> the most important results of many statistical objects to a <high>data frame</high>.


```{r, out.width = "45%", echo = FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/Erfurt_2018June/master/_sessions/_image/broom_hex.png")
```

```{r, eval = FALSE, echo = TRUE}
# install and load broom
install.packages('broom')
library(broom)
```


Try `tidy()` on your favorite statistical object and see what you get!

]

.pull-right55[

<br><br>

```{r, eval = TRUE, echo = TRUE}
# Load broom package
library(broom)   # For tidy()

# Conduct correlation test
income_htest <- cor.test(formula = ~ height + income,
                         data = baselers)

# tidy results
tidy(income_htest)
```

]

---

.pull-left4[

# `tidy()`

The `tidy()` function from the `broom` package <high>converts</high> the most important results of many statistical objects to a <high>data frame</high>.


```{r, out.width = "45%", echo = FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/Erfurt_2018June/master/_sessions/_image/broom_hex.png")
```

```{r, eval = FALSE, echo = TRUE}
# install and load broom
install.packages('broom')
library(broom)
```


Try `tidy()` on your favorite statistical object and see what you get!

]

.pull-right55[

<br><br>

```{r, eval = TRUE, echo = TRUE}
# Load broom package
library(broom)   # For tidy()

# Conduct t.test
height_sex_ttest <- t.test(formula = height ~ sex,
                           data = baselers)

# standard print out
height_sex_ttest
```

]


---

.pull-left4[

# `tidy()`

The `tidy()` function from the `broom` package <high>converts</high> the most important results of many statistical objects to a <high>data frame</high>.


```{r, out.width = "45%", echo = FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/Erfurt_2018June/master/_sessions/_image/broom_hex.png")
```

```{r, eval = FALSE, echo = TRUE}
# install and load broom
install.packages('broom')
library(broom)
```


Try `tidy()` on your favorite statistical object and see what you get!

]

.pull-right55[

<br><br>

```{r, eval = TRUE, echo = TRUE}
# Load broom package
library(broom)   # For tidy()

# Conduct t.test
height_sex_ttest <- t.test(formula = height ~ sex,
                           data = baselers)

# tidy results
tidy(height_sex_ttest)
```

]


---

.pull-left35[
# Simulation functions

R has a several functions that allow you to draw <high>random samples</high> data from specified distributions:

|Type|Function|
|:----|:------|
|Normal|`rnorm(n, mean, sd)`|
|Uniform|`runif(n, min, max)`|
|Binomial|`rbinom(n, size, prob)`|
|Sample|`sample(x)`|

These allow you to run your own <high>simulations</high>!

]

.pull-right6[

*Drawing Random Samples*

```{r, echo = TRUE}
# 5 samples from N(mean = 100, sd = 10)
noisy <- rnorm(n = 5, mean = 100, sd = 10)

noisy

mean(noisy)

sd(noisy)

# Simulate 10 coin flilps
coin_flips <- rbinom(n = 10, size = 1, prob = .5)

coin_flips

```


]

---

class: middle, center

<h1><a href="https://therbootcamp.github.io/SwR_2019Apr/_sessions/LinearModelsII/LinearModelsII_practical.html">Practical</a></h1>



