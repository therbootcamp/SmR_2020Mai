---
title: "Robust Statistics"
author: "<table style='table-layout:fixed;width:100%;border:0;padding:0;margin:0'><col width='10%'><col width='10%'>
  <tr style='border:none'>
    <td style='display:block;width:100%;text-align:left;vertical-align:bottom;padding:0;margin:0;border:none' nowrap>
      <font style='font-style:normal'>Statistics with R</font><br>
      <a href='https://therbootcamp.github.io/SWR_2019Apr/'>
        <i class='fas fa-clock' style='font-size:.9em;' ></i>
      </a>
      <a href='https://therbootcamp.github.io'>
        <i class='fas fa-home' style='font-size:.9em;'></i>
      </a>
      <a href='mailto:therbootcamp@gmail.com'>
        <i class='fas fa-envelope' style='font-size: .9em;'></i>
      </a>
      <a href='https://www.linkedin.com/company/basel-r-bootcamp/'>
        <i class='fab fa-linkedin' style='font-size: .9em;'></i>
      </a>
      <a href='https://therbootcamp.github.io'>
        <font style='font-style:normal'>Basel R Bootcamp</font>
      </a>
    </td>
    <td style='width:100%;vertical-align:bottom;text-align:right;padding:0;margin:0;border:none'>
      <img src='https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/by-sa.png' style='height:15px;width:80px'/>
    </td>
  </tr></table>"
output:
  html_document:
    css: practical.css
    self_contained: no
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(comment = NA, 
                      fig.width = 6, 
                      fig.height = 6,
                      fig.align = 'center',
                      echo = FALSE, 
                      eval = FALSE, 
                      warning = FALSE)

options(digits = 3)

# Load packages
library(tidyverse)

# Load packages
#read_csv("1_Data/...")

```

<p align="center" width="100%">
  <img src="image/avocado_small.png" alt="Trulli" style="width:100%y">
  <br>
  <font style="font-size:10px">from <a href="https://www.self.com/story/how-to-pick-the-best-avocado-every-time">self.com</a></font>
</p>


# {.tabset}

## Overview

In this practical you'll explore issues surrounding the topic of robust statistics using a variety of packages (see *Functions*) tab. You'll will explore sales of avocados across various locations in the US from 2015 to 2018. 

By the end of this practical you will know how to:

1. Evaluate regression assumptions.
2. Run off-the-shelve non-parametric tests.
3. Run bootstrap analyses.

## Tasks

### A - Setup

1. Open your `BaselRBootcamp` R project. It should already have the folders `1_Data` and `2_Code`. Make sure that the data files listed in the `Datasets` section above are in your `1_Data` folder

```{r}
# Done!
```

2. Open a new R script. At the top of the script, using comments, write your name and the date. Save it as a new file called `robuststats_practical.R` in the `2_Code` folder.  

```{r}
# Done!
```

3. Using `library()` load the `tidyverse` package (if you don't have it, you'll need to install it with `install.packages()`)!

```{r, echo = TRUE, eval = FALSE}
# Load packages necessary for this script
library(tidyverse)
library(lubdridate)
library(lme4)
library(rsq)
library(moments)
library(boot)
library(quantreg)
library(Rfit)
```

4. Using the following template, load the `avocado.csv`, `avocado_cali.csv`, and `avocado_ny.csv` data sets into R and store them as a new objects called `avocado`, `avocado_cali`, and `avocado_ny` (Hint: Don't type the path directly! Use the "tab" completion!).

```{r, echo = TRUE, eval = FALSE}
# Load XXX.csv from the 1_Data folder

XX <- read_csv(file = "XX/XX")
```

```{r, echo = TRUE}
# Load data sets from the 1_Data folder

avocado <- read_csv(file = "1_Data/avocado.csv")
avocado_cali <- read_csv(file = "1_Data/avocado_cali.csv")
```

5. Take a look at the first few rows of the data sets by printing them to the console.

```{r, echo = TRUE, eval = FALSE}
# Print XXX object
XXX
```

```{r}
# Print XXX object
avocado
avocado_cali
avocado_ny
```

7. Use the the `summary()` function to print more details on the columns of the data sets.

```{r, echo = TRUE, eval = FALSE}
# Show summary of XXX object
summary(XXX)
```

```{r}
# Show summary of XXX object
summary(avocado)
summary(avocado_cali)
summary(avocado_ny)
```

8. Use the `View()` function to view the entire data sets in new windows

```{r, echo = TRUE, eval = FALSE}
# View data frame XXX 
View(XXX)
```

```{r, eval = F}
# View data frame XXX 
View(avocado)
View(avocado_cali)
View(avocado_ny)
```

### B - Assumptions

1. In this practical, you will be working with a data set containing information on the sales volume of avocados across various areas in the US and over the the last three years. Your initial goal is to predict, whether the sales `volume` is a function of the `price_per_avocado`. Begin by running a simple regression on the `avocado_cali` data set using `lm` and store the model as `m1`. See the template below.   

Note: In this practical, we will be using the basic `lm` function, rather than the more general `lm` function.

```{r, echo = TRUE, eval = FALSE}
# regress volume on average price
m1 <- lm(YY ~ XX, data = ZZ)
```

```{r, echo = TRUE}
# regress volume on average price
m1 <- lm(volume ~ price_per_avocado, 
          data = avocado_cali)
```

2. Evaluate the model using `summary()`. Is the average price a good predictor of volume?

```{r, echo = TRUE, eval = FALSE}
# model summary
summary(XX)
```

```{r}
# model summary
summary(m1)
```

3. According to test yes, the average price strongly predicts volume. However, this does not imply that the relationship between average price and volume is described well by the regression line. Create a simple plot of the average price (x-axis) and volume (y-axis) using the template below. 

```{r, echo = TRUE, eval = FALSE}
# plot model
plot(avocado_cali$XX, avocado_cali$YY)
abline(m1)
```

```{r}
# plot model
plot(avocado_cali$price_per_avocado, avocado_cali$volume)
abline(m1)
```

4. The plot revealed that the regression is actually not really a good summary of the data. Confirm this by plotting the residuals against the predicted values.    

```{r, echo = TRUE, eval = FALSE}
# Residual plot
plot(predicted(XX), residuals(XX))
```

```{r}
# Residual plot
plot(predict(m1), residuals(m1))
```

5. The model fits the data badly including several serious violations of regression assumptions: Linearity, normality, and heteroscedascity do not seem to hold. One cause for these violations are missing variables. Add one other variable to the regression that contains information on the kind of avocado (see *Data* tab) and evaluate the model again.        

```{r, echo = TRUE, eval = FALSE}
# regress volume on average price and XX
m2 <- lm(YY ~ XX1 + XX2, data = ZZ)
plot(predict(m2), residuals(m2))
```

```{r}
# regress volume on average price and XX
m2 <- lm(volume ~ price_per_avocado + type,
          data = avocado_cali)
plot(predict(m2), residuals(m2))
```

6. Accounting for the different types of avocados has helped a great deal. The residual plot still does not look ideal. But at least the correlation between predicted values and residuals seems to have substantially decreased. Verify this by comparing the correlations. 

```{r, echo = TRUE, eval = FALSE}
# compare correlation of predicted values & residals
cor(predict(XX), residuals(XX))
cor(predict(YY), residuals(YY))
```

```{r}
# compare correlation of predicted values & residals
cor(predict(m1), residuals(m1))
cor(predict(m2), residuals(m2))
```

7. The correlation actually has not come down, because the correlation for `m1` was despite the visual appearance already zero. Nonetheless, we are moving in the right direction. One other problem for the relationship is the very skewed distribution of `volume`. Inspect this by plotting a simple histogram of `volume`.  

```{r, echo = TRUE, eval = FALSE}
# histogram of volume
hist(avocado_cali$XX)
```

```{r}
# histogram of volume
hist(avocado_cali$volume)
```

8. To deal with this problem one could engage in variable transformations, better would be to model the data using an appropriate probability distribution, which counts typically is a Poisson distribution. Fortunately, we don't have to do either. There is another variable called `volume_index` that is much better behaved. Verify this by also creating a histogram of this variable. 

```{r, echo = TRUE, eval = FALSE}
# histogram of volume_index
hist(avocado_cali$XX)
```

```{r}
# histogram of volume_index
hist(avocado_cali$volume_index)
```

9. The distribution of `volume_index` is also not ideal, but at least it exhibits much less skewness. Verify this by computing the skewness of both variables using the `skewness` function from the `moments` package. 

```{r, echo = TRUE, eval = FALSE}
# skewness of volume and volume index
skewness(avocado_cali$XX)
skewness(avocado_cali$YY)
```

```{r}
# skewness of volume and volume index
skewness(avocado_cali$volume)
skewness(avocado_cali$volume_index)
```

10. Now that you confirmed that `volume_index` is probably the better variable to model, run a regression of `volume_indes` on `price_per_avocado` and `type` and call the model `m3`.  

```{r, echo = TRUE, eval = FALSE}
# regress volume_index on average price and type
m3 <- lm(YY ~ XX1 + XX2, data = ZZ)
```

```{r}
# regress volume_index on average price and type
m3 <- lm(volume_index ~ price_per_avocado + type,
          data = avocado_cali)
```

12. Now plot the residuals of `m3` against the predicted values. What do you think have the residuals become more behaved?

```{r}
# evaluate m3
plot(predict(m3), residuals(m3))
```

13. The residual behavior is still not ideal. There seems to still some skewness differences between low and high predicted values, but now one would not expect anymore that those would substantially alter the results. So, go on and evaluate the model using `summary()`.      

```{r}
# evaluate m3
summmary(m3)
```

### C - Variance inflation

1. In the previous section, you have evaluated the effect of price on volume controlling for the type of avocado. This has shown you that price indeed seems to exert strong influence on volume. Some of this impact, however, could be due to other third variables. For instance, it might be possible that the price simply went up through general inflation, while at the same time avocados became more popular. Control for this and other factors by including in the regression of `volume_index` the following variables in addition to `price_per_avocado` and `type`: `date`, `season`, `temperature`, `humidity`, `precipitation`, `type`. Call the model `m4`. 

```{r}
# new comprehensive model 4
m4 <- lm(volume_index ~ price_per_avocado + type + date  + season + temperature + humidity + precipitation + type, data = avocado_cali)
```

2. Evaluate the model using `summary()` and compare it to the summary of `m3`. First, is the the effect of price still significant. Second, how have have the estimate changed and the standard error changed?

```{r}
# evaluate m3 and m4
summary(m3)
summary(m4)
```

3. You will have observed that both the estimate and the standard error went slightly up. The former is a result of the fact that the other variables took account of some variance in the price that is not related to volume. The latter is, as you can verify in the formula on the variance inflation factor, a result of two adversarial factors: (1) a better fit of the model and (2) an increased variance inflation factor. Let's inspect these two. First, evaluate the residual variance of `m3` and `m4` using the template below.

```{r, echo = TRUE, eval = FALSE}
# resdiaul variance of m3 and m4
var_res1 <- var(residuals(XX))
var_res2 <- var(residuals(YY))
var_res1
var_res2
```

```{r}
# resdiaul variance of m3 and m4
var_res1 <- var(residuals(m3))
var_res2 <- var(residuals(m4))
var_res1
var_res2
```

4. You'll see that that the residual variance decreased to about half. According to this the variance of the $beta$ weight (i.e., the square of the standard error should have also halved). Given that it has not, suggests that the variance inflation factor has increased by at least the same amount. Find out! Run two regression predicting `price_per_avocado`: First, by all other predictors in `m3`, i.e., `type`, and call the model `m_price1`. Second, by all other predictors in `m4` and call the model `m_price2`. 

```{r, echo = TRUE, eval = FALSE}
# predicting price_per_avocado
m_price1 <- lm(price_per_avocado ~ XX, data = avocado_cali)
m_price2 <- lm(price_per_avocado ~ XX + XX  + ..., data = avocado_cali)
```

```{r}
# resdiual variance of m3 and m4
m_price1 <- lm(price_per_avocado ~ type, data = avocado_cali)
m_price2 <- lm(price_per_avocado ~ type + date  + season + temperature + humidity + precipitation, data = avocado_cali)
```

5. Next, calculate the $R^2$ value for both models and the variance inflation factor using the template below.  

```{r, echo = TRUE, eval = FALSE}
# proportion explained variance
rsq1 <- rsq(XX)
rsq2 <- rsq(YY)
rsq1
rsq2

my_rsq=function(m,x) var(predict(m)) / var(x)

my_rsq(m_price1, avocado_cali$price_per_avocado)

# variance inflation factors
1/(1-rsq(XX))
1/(1-rsq(YY))
```

```{r}
# proportion explained variance
rsq1 <- rsq(m_price1)
rsq2 <- rsq(m_price2)
rsq1
rsq2

# variance inflation factors
vif1 <- 1/(1-rsq1)
vif2 <- 1/(1-rsq2)
vif1
vif2
```

6. So, while residual variance decreased from `.042` to `.022`, the variance inflation factor increased from `2.24` to `4.86`. Using these values we can actually compute the standard errors of `price_per_avocado` in model `m3` and `m4` following the formula on the respective slide. See code below. Confirm that the calculated values correspond to the standard errors in the summary outputs for `m1` and `m2`.

Note: R determines the standard errors using a correction for degrees of freedom in the calculation of the residual variance. One can account for this by recalculating the residual variance using $\hat\sigma_e^2=\frac{\sum e^2}{n-df-1}$, with $df=2$ for `m3` and $df=9$ for `m4`.    

```{r, echo = TRUE}
# needed formula elements
var_x <- var(avocado_cali$price_per_avocado)
n = nrow(avocado_cali)

# without correction -----

# variance and standard error of price in m3
var_beta1 <- (var_res1 / (var_x * (n-1))) * vif1 
sqrt(var_beta1)

# variance and standard error of price in m4
var_beta2 =  (var_res2 / (var_x * (n-1))) * vif2 
sqrt(var_beta2)

# with correction -----

# variance and standard error of price in m3 with correction
var_c_res1 <- sum(residuals(m4)**2) / (n - 2 - 1)
var_beta1 <- (var_c_res1 / (var_x * (n-1))) * vif1 
sqrt(var_beta1)

# variance and standard error of price in m4 with correction
var_c_res2 <- sum(residuals(m4)**2) / (n - 9 - 1)
var_beta1 <- (var_c_res2 / (var_x * (n-1))) * vif1 
sqrt(var_beta1)

```

7. The above illustrates that including variables regressors in the regression is a double-edged sword. They improve the overall prediction and may uncover more pure relationships, but they will also increase our uncertainty over the true values of the regression weights by increasing the respective standard errors. If you like, see if you can find other combination of predictors that increase or decrease the standard error of the weight for price.  

### C - Polynomial effects

1. So far we have assumed that all relationships are completely linear. However, it seems plausible the effect of price could, for instance, be a quadratic one or even of a higher polynomial. Create a regression predicting `volume_index` by `type`, `price_per_avocado`, and the quadratic effect of `price_per_avocado`. The latter can be included using the function `I()`. See template below. 

```{r, echo = TRUE, eval = FALSE}
# linear and quadratic effect of price
m5 <- lm(YY ~ YY + XX + I(XX^2), 
          data = avocado_cali)
```   

```{r}
# linear and quadratic effect of price
m5 <- lm(volume_index ~ type + price_per_avocado + I(price_per_avocado^2), 
          data = avocado_cali)
``` 

2. Eva lute the model (`m5`) using `summary()`. Is the quadratic effect significant?

```{r}
# linear and quadratic effect of price
summary(m5)
``` 

3. Test whether the inclusion of the quadratic factor is warranted by testing `m5` against `m3` (the model without the quadratic effect) using `anova()`. Note: `anova()` performs a deviance test between two models, not an analysis of variance in the classical sense. 

```{r, echo = TRUE, eval = FALSE}
# evaluate merit of quadratic effect
anova(m3,m5)
``` 

```{r}
# evaluate merit of quadratic effect
summary(m5)
``` 

4. Yes, the quadratic effect improves the prediction of volume significantly. However, this came again with a substantial downside. You will see that the standard error of the linear effect of price went up dramatically. This is because `price_per_avocado` and `price_per_avocado^2` are highly correlated. To prevent this, the quadratic terms are typically centered, as implemented in the template below. Run the regression and compare the results to `m5`.

```{r, echo = TRUE, eval = FALSE}
# linear and quadratic effect of price
m6 <- lm(YY ~ YY + XX + I((XX - mean(XX)) ^2), 
          data = avocado_cali)
```   

```{r}
# linear and quadratic effect of price
m6 <- lm(volume_index ~ type + price_per_avocado + I((price_per_avocado - mean(price_per_avocado))^2), 
          data = avocado_cali)
``` 

5. You'll see the standard error of the linear effect went back to about where it was in `m3` or `m4`. Try out if you can further improve the prediction of volume by including a cubic effect, i.e., `XX^3`. 

### D - Non-parametric tests

1. Let's now look at the development of avocado consumption over time. Use the following code to create data sets containing the Cali' avocado consumption in 2016 and 2017.

```{r, echo = TRUE, eval = FALSE}
# Cali avocado volume 2016 & 2017
avocado_cali_2016 = avocado_cali %>% 
  filter(year == 2016)
avocado_cali_2017 = avocado_cali %>% 
  filter(year == 2017, 
         week(date) %in% week(avocado_cali_2016$date))
``` 

2. Now test, whether the consumption has risen from 2016 to 2017 using a paired t-test, i.e., `t.test(X, Y, paired = TRUE)`. Try this out for both `volume` and `volume_index`.

```{r, echo = TRUE, eval = FALSE}
# compare Cali and NY avocado volume
t.test(avocado_cali_2016$XX,
       avocado_cali_2017$XX, paired = TRUE)

# compare Cali and NY avocado volume_index
t.test(avocado_cali_2016$YY,
       avocado_cali_2017$YY, paired = TRUE)
``` 

```{r}
# compare Cali and NY avocado volume
t.test(avocado_cali_2016$volume,
       avocado_cali_2017$volume, paired = TRUE)

# compare Cali and NY avocado volume_index
t.test(avocado_cali_2016$volume_index,
       avocado_cali_2017$volume_index, paired = TRUE)
``` 

3. For `volume_index` the result turned out significant, 2017 indeed saw higher avocado consumption than 2016. However, for `volume` the result did not quite reach significance. This likely has to do with the fact that `volume` is extremely skewed. To confirm the result, run the same tests using a pared Wilcoxon test, i.e., `wilcox.test(XX, YY, paired = TRUE)`, which should not be affected by the skewness. 

```{r, echo = TRUE, eval = FALSE}
# compare Cali and NY avocado volume
wilcox.test(avocado_cali_2016$XX,
            avocado_cali_2017$XX, paired = TRUE)

# compare Cali and NY avocado volume_index
wilcox.test(avocado_cali_2016$YY,
            avocado_cali_2017$YY, paired = TRUE)
``` 

```{r}
# compare Cali and NY avocado volume
wilcox.test(avocado_cali_2016$volume,
            avocado_cali_2017$volume, paired = TRUE)

# compare Cali and NY avocado volume_index
wilcox.test(avocado_cali_2016$volume_index,
            avocado_cali_2017$volume_index, paired = TRUE)
``` 

4. In fact, the Wilcoxon test shows significant results for both measures, suggesting that there indeed is a robust difference. Another, even more robust way of testing this is the sign test. Test this using the template below

```{r, echo = TRUE, eval = FALSE}
# sign test preparations
signs = avocado_cali_2017$XX > avocado_cali_2016$XX
n_plus = sum(signs)
n = length(signs)

# sign test: p of receiving a larger n_plus than observed
pbinom(n_plus, size = n, prob = .5)
``` 

```{r}
# sign test preparations
signs = avocado_cali_2017$volume > avocado_cali_2016$volume
n_plus = sum(signs)
n = length(signs)

# sign test: p of receiving a larger n_plus than observed
pbinom(n_plus, size = n, prob = .5)

# sign test preparations
signs = avocado_cali_2017$volume_index > avocado_cali_2016$volume_index
n_plus = sum(signs)
n = length(signs)

# sign test: p of receiving a larger n_plus than observed
pbinom(n_plus, size = n, prob = .5)

``` 

5. The sign test confirms the results. Now, go explore and see whether you can find additional differences between the avocado years of 2016 and 2017 in California.


### E - Robust regression

1. Remember that the residual distribution of our regression of `volume_index` on `price_per_avocado` and `type` was less than ideal? Take a look again by running the code below. 

```{r}
# regress volume_index on average price and type
m3 <- lm(volume_index ~ price_per_avocado + type,
          data = avocado_cali)
plot(predict(m3), residuals(m3))

```

2. The somewhat different residual distributions for high and low predictions suggest that we should take a look at robust regression analyses. Run similar regression models using the `rq` function from the `quantreg` package and the `rfit` function from the `Rfit` package using the template below. 

```{r, echo = TRUE, eval = FALSE}
# quantile regression
m3_q <- rq(YY ~ XX + XX, data = ZZ)

# rank-based regression
m3_rb <- rfit(YY ~ XX + XX, data = ZZ)
```

```{r}
# quantile regression
m3_q <- rq(volume_index ~ price_per_avocado + type, data = avocado_cali)

# rank-based regression
m3_rb <- rfit(volume_index ~ price_per_avocado + type, data = avocado_cali)
```

3. Compare the results of the original `m3` model to the two robust regression models using `summary()` on each model. Are the coefficients much different from each another?

```{r}
# regular regression
summary(m3)

# quantile regression
summary(m3_q)

# rank-based regression
summary(m3_rb)
```

4. The solutions of the three model classes were quite similar, right? Now, try the same predicting `volume` instead of `volume_index`. Fit regressions using regular regression and the two robust approaches and compare the parameters. 

```{r}
# regular regression
m3_2 <- lm(volume ~ price_per_avocado + type, data = avocado_cali)
summary(m3_2)

# quantile regression
m3_q_2 <- rq(volume ~ price_per_avocado + type, data = avocado_cali)
summary(m3_q_2)

# rank-based regression
m3_rb_2 <- rfit(volume ~ price_per_avocado + type, data = avocado_cali)
summary(m3_rb_2)
```

5. The numbers aren't so easy to compare, but you will be able to confirm that the regular regression coefficients are far of the ones of the other two methods, especially for `price_per_avocado`. As in most circumstances, the difference is largest between the `rq` and `lm` results, with `rfit` being somewhere in the middle, but closer to `rq`. Now, as usual, go explore.

### X - Advanced: Bootstrap

Bootstrapping is an amazingly powerful tool to make inferences about the sampling distribution of any statistic you can come up with. Let's try to use it to come up with standard errors for our favorite regression model: `volume_index ~ price_per_avocado + type`.  

1. To use bootstrap, one needs to define a function that calculates the statistics for each of the different bootstrap samples provided to it. See the code below. Notice that the `data` argument has not been assigned our data set `avocado_cali` but rather carries the generic `data`, which will be provided to the function by the `boot` function. Run the function. 

```{r, echo = TRUE}
# bootstrap function for linear model
boot_lm <- function(data, indices){
  data <- data[indices,] # select obs. in bootstrap sample
  m <- lm(volume_index ~ price_per_avocado + type, data = data)
  coefficients(m)
  }
```

2. The next step is to run the bootstrap mechanism using the `boot` function from the `boot` package to create the bootstrap results. Here we are now providing `avocado_cali` along with the function defined above and the number of repetitions `R`. Run the bootstrap.

```{r, echo = TRUE}
# bootstrap sampling results 
B <- boot(avocado_cali, statistic = boot_lm, R = 100)
```

3. Now you can evaluate the results. Print the `B` object. The output will be slightly different than usual. The column `original` contains the results of the regression model for the full data (i.e., `m3`). The column `bias` contains the differences between the average simulated parameters and the original parameters. If these are large, then this may, among other things, mean that you need to run a larger number of bootstrap samples.  Finally, the column `std. error` contains the standard error across the bootstrap sampling results. 

```{r, echo = TRUE}
# print bootstrap results
B
```

4. Finally, compare the standard errors of the `m3` or the other, robust regression models, to those of the bootstrap. 

```{r, echo = TRUE}
# print bootstrap results
summary(m3)
```

5. Go explore. Can you figure out how to bootstrap the difference between the volumes of `avocado_cali_2016` and `avocado_cali_2017`?

### Y - Advanced: Assumptions pt. 2

In this section you can full demonstrate your statistics Jedi-knight skills by analyzing the full `avocado` data set. Given that the `avocado` data set contains data from different regions the full data needs to modeled as a mixed model. Your goal is to specify a model that produces a relatively well-behaved looking residual plot, by including any necessary variables from the data set as fixed or random effects. See template below.

```{r, echo = TRUE, eval = FALSE}
# model
mm <- lmer(volume_index ~ XX + (1 | region), data = avocado)

# residual plot
plot(predict(mm), residuals(m,))
```




## Examples

```{r, eval = FALSE, echo = TRUE}

library(tidyverse)

# descriptive stats ----------------

library(moments)

var(hwy)      # variance
skewness(hwy) # skewness


# simple regression ----------------

# model
m <- lm(hwy ~ displ, data = mpg)

# evaluate model
summary(m)     # summary results
predict(m)     # predicted/fitted values
residuals(m)   # residual errors
rsq(m)         # r-square 
1 / (1-rsq(m)) # variance inflation factor 

# residual plot
plot(predict(m), residuals(m))


# model comparison ----------------

# model
m1 <- lm(hwy ~ displ, data = mpg)

# model + quadratic effect
m2 <- lm(hwy ~ I(displ^2), data = mpg)

# anova
anova(m1, m2)

# non-parametric tests ----------------

#  split data (not strictly necessary)
mpg_suv     <- mpg %>% select(class == 'suv')
mpg_compact <- mpg %>% select(class == 'compact')

# wilcoxon
wilcoxon.test(mpg_suv$hwy, mpg_compact$hwy)

# robust regression ----------------

library(quantreg)
library(Rfit)

# quantile regression
m_q  <- rq(hwy ~ displ, data = mpg)

# rank-bsed regression
m_rb <- rfit(hwy ~ displ, data = mpg)

```


## Datasets

|File | Rows | Columns |
|:----|:-----|:------|
|[avocado.csv](https://therbootcamp.github.io/SwR_2019Apr/_sessions/1_Data/avocado.csv) | 17573 | 16 |
|[avocado_cali.csv](https://therbootcamp.github.io/SwR_2019Apr/_sessions/1_Data/avocado_cali.csv) | 338 | 16 |

This data was downloaded from the [Hass](https://en.wikipedia.org/wiki/Hass_avocado) Avocado Board website for the duration of April 2015 and May 2018. The data contain weekly (reported on Sundays) retail scan data for National retail volume (units) and price. Retail scan data comes directly from retailers’ cash registers based on actual retail sales of Hass avocados. The Average Price (of avocados) in the table reflects a per unit (per avocado) cost. Other varieties of avocados (e.g. greenskins) are not included in this table.

The data set `avocado.csv` contains the data for all 52 regions. The smaller data set `avocado_cali.csv` contains the data for only California.

#### Variable description

| Name | Description |
|:-------------|:-------------------------------------|
| `volume` | The total number of sold avocados that week. |
| `volume_index` | A sales index scaling the sales number to the range of 0 to 10.  |
| `price_per_avocado` | The average price of a single avocado. |
| `type` | The type of avocado: "conventional" or "organic". |
| `temperature` | The regional temperature on the day of the data recording. |
| `temperature_usa` | The country-wide temperature on the day of the data recording. |
| `humidity` | The regional humidity on the day of the data recording. |
| `humidity_usa` | The country-wide humidity on the day of the data recording. |
| `precipitation` | The regional precipitation on the day of the data recording. |
| `precipitation_usa` | The country-wide precipitation on the day of the data recording. |
| `year` | The year of the data recording. |
| `season` | The season of the data recording. |
| `date` | The date of the data recording. |
| `region` | The region for which sales were recorded. |
| `longitude` | The longitude of the region center. |
| `latitude` | The latitude of the region center. |

## Functions

### Packages

|Package| Installation|
|:------|:------|
|`tidyverse`|`install.packages("tidyverse")`|
|`lubridate`|`install.packages("lubridate")`|
|`moments`|`install.packages("moments")`|
|`quantreg`|`install.packages("quantreg")`|
|`boot`|`install.packages("boot")`|
|`Rfit`|`install.packages("Rfit")`|
|`lme4`|`install.packages("lme4")`|
|`rsq`|`install.packages("rsq")`|


### Functions

| Function| Package | Description |
|:---|:------|:---------------------------------------------|
| `lm`|`stats`| More basic version of `glm()`  for regular regression models. | 
| `summary`, `rsq` |`base`| Print model summary / model R-squared | 
| `predict`, `residuals` |`stats`| Extract fitted values / residuals from model object. | 
| `rq` |`quantreg`| Conduct quantile regression. | 
| `rfit` |`Rfit`| Conduct rank-based regression. | 
| `wilcox.test` |`stats`| Wilcoxon test. |
| `pbinom` |`stats`| Cumulative binomial distribution. Useful for sign-test. |

## Resources

### vignettes

Links to decent introductory / tutorial papers: 

[Regression assumptions](https://therbootcamp.github.io/SwR_2019Apr/_sessions/RobustStats/literature/Berry1993.pdf), 
[Robust regression](https://therbootcamp.github.io/SwR_2019Apr/_sessions/RobustStats/literature/FoxWeisberg2013.pdf), 
[Rank-based regression with Rfit](https://therbootcamp.github.io/SwR_2019Apr/_sessions/RobustStats/literature/KlokeMcKean2012.pdf)
[Bootstrap](https://therbootcamp.github.io/SwR_2019Apr/_sessions/RobustStats/literature/FoxWeisberg2013.pdf)
[Variance inflation](https://therbootcamp.github.io/SwR_2019Apr/_sessions/RobustStats/literature/Stine1995.pdf)

