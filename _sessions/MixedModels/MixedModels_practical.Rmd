---
title: "Mixed Models"
subtitle: "Linear models with hierarchical data"
author: "<table style='table-layout:fixed;width:100%;border:0;padding:0;margin:0'><col width='10%'><col width='10%'>
  <tr style='border:none'>
    <td style='display:block;width:100%;text-align:left;vertical-align:bottom;padding:0;margin:0;border:none' nowrap>
      <font style='font-style:normal'>Statistics with R</font><br>
      <a href='https://therbootcamp.github.io/SwR_2019Apr/'>
        <i class='fas fa-clock' style='font-size:.9em;' ></i>
      </a>
      <a href='https://therbootcamp.github.io'>
        <i class='fas fa-home' style='font-size:.9em;'></i>
      </a>
      <a href='mailto:therbootcamp@gmail.com'>
        <i class='fas fa-envelope' style='font-size: .9em;'></i>
      </a>
      <a href='https://www.linkedin.com/company/basel-r-bootcamp/'>
        <i class='fab fa-linkedin' style='font-size: .9em;'></i>
      </a>
      <a href='https://therbootcamp.github.io'>
        <font style='font-style:normal'>Basel R Bootcamp</font>
      </a>
    </td>
    <td style='width:100%;vertical-align:bottom;text-align:right;padding:0;margin:0;border:none'>
      <img src='https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/by-sa.png' style='height:15px;width:80px'/>
    </td>
  </tr></table>"
output:
  html_document:
    css: practical.css
    self_contained: no
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(comment = NA, 
                      fig.width = 6, 
                      fig.height = 6,
                      fig.align = 'center',
                      echo = FALSE, 
                      eval = FALSE, 
                      warning = FALSE)

options(digits = 3)

# Load packages
library(tidyverse)
library(lme4)
library(sjstats)
library(pbkrtest)

# Load data
tom <- read_csv("1_Data/tomatometer.csv")
schools <- read_csv("1_Data/schools.csv")
cr <- read_csv("1_Data/cancer_remission.csv")
```

<p align="center" width="100%">
  <img src="image/rottentomatoes.png" alt="Trulli" style="width:100%;height:280px">
  <br>
  <font style="font-size:10px">from <a href="https://www.rottentomatoes.com/">rottentomatoes.com</a></font>
</p>


# {.tabset}

## Overview

In this practical you'll practice "mixed effects modeling" with the `lme4`, `sjstats`, `stats`, and `pbkrtest` packages.

By the end of this practical you will know how to:

1. Run mixed effects models in R.
2. Extract p-values for fixed effects.
3. Select the appropriate random effects structure.
4. Specify crossed vs. nested random effects.
5. Extract variance components and compute the explained variance and intra-class correlation
6. Visualize your linear mixed effects model.
7. Run a generalized mixed effects model.

## Tasks

### A - Setup

1. Open your `BaselRBootcamp` R project. It should already have the folders `1_Data` and `2_Code`. Make sure that the data files listed in the `Datasets` section above are in your `1_Data` folder

```{r}
# Done!
```

2. Open a new R script. At the top of the script, using comments, write your name and the date. Save it as a new file called `MixedModels_practical.R` in the `2_Code` folder.  

```{r}
# Done!
```

3. Using `library()` load the `tidyverse`, `lme4`, and `sjstats` packages (if you don't have them, you'll need to install them with `install.packages()`)!

```{r, echo = TRUE, eval = FALSE}
# Load packages necessary for this script
library(tidyverse)
library(lme4)
library(sjstats)
```

4. Using the following template, load the `tomatometer.csv` and `schools.csv` data into R and store it as a new object called `tom` and `schools`, respectively (Hint: Don't type the path directly! Use the "tab" completion!).

```{r, echo = TRUE, eval = FALSE}
# Load tomatometer.csv from the 1_Data folder
XX <- read_csv(file = "XX/XX")

# Load school.csv from the 1_Data folder
XX<- read_csv("XX/XX")
```


```{r}
# Load tomatometer.csv from the 1_Data folder
tom <- read_csv(file = "1_Data/tomatometer.csv")

# Load school.csv from the 1_Data folder
schools <- read_csv("1_Data/schools.csv")
```

5. Take a look at the first few rows of the datasets by printing them to the console.

```{r, echo = TRUE, eval = FALSE}
# Print the object(s)
XXX
XXX
```

```{r}
# Print the tom object
tom
schools
```

7. Use the the `summary()` function to print more details on the columns of the datasets.

```{r, echo=TRUE, eval = FALSE}
summary(XXX)
summary(XXX)
```

```{r}
summary(tom)
summary(schools)
```

8. Use the `View()` function to view the entire dataframe(s) in a new window.

```{r, echo = TRUE, eval = FALSE}
View(XXX)
View(XXX)
```

```{r}
View(tom)
View(schools)
```

### B - Running a Linear Mixed Effects Model

In the first part of this practical we will work with the `tom` data example from the slides and test the effect of a person's `State` ("Sober" vs. "Drunk") on his or her `Tomatometer` rating.

1. Run fixed effects only model predicting `Tomatometer` with `State` and save the result as `FE_mod`. Then inspect the results.

```{r, echo = TRUE, eval = FALSE}
# Use lm, as lmer only works if at least one random effect is specified
FE_mod <- glm(XXX ~ XXX, 
              data = XXX)

# Inspect the results
summary(XXX)
```

```{r}
# Use lm, as lmer only works if at least one random effect is specified
FE_mod <- glm(Tomatometer ~ State, 
              data = tom)

# look at the resuts
summary(FE_mod)
```

2. Currently "Drunk" is taken as base state, so the intercept shows the mean at state "Drunk" and the slope shows how much lower the ratings in `State == "Sober"` are. To obtain a more intuitive model with "Sober" as the base level, *coerce* the `State` variable into a factor and set the levels. Do this by running the following code.

```{r, echo = TRUE}
# Coerce the State variable into a factor
tom <- tom %>%
  mutate(State = factor(State, levels = c("Sober", "Drunk")))
```

3. Now rerun your model from task B1.

```{r}
# Use lm, as lmer only works if at least one random effect is specified
FE_mod <- glm(Tomatometer ~ State, 
              data = tom)

# look at the resuts
summary(FE_mod)
```

4. Compare the model output of B3 with the one you obtained in B1. How did the coefficients change? Why?

5. Now run a model with by-subjects random intercepts (subjects identifiers are stored in the `ID` variable). *Hint*: Random effects are specified in parenthesis in the formula in the following way `(`[design_matrix](https://en.wikipedia.org/wiki/Design_matrix)`|grouping_variable)`. **Note**: The `REML = FALSE` in the model specification tells R to fit the model using maximum likelihood (ML), rather than restricted maximal likelihood (REML; for more information on this in a technical approach see [here](https://projecteuclid.org/euclid.cbms/1462106081) and [here](http://www.stats.net.au/Maths_REML_manual.pdf), for a less technical approach see [here](https://en.wikipedia.org/wiki/Restricted_maximum_likelihood)). This will later be important for certain model comparisons that only work if the model was fitted using ML.


```{r echo = TRUE, eval = FALSE}
# Mixed effects model with by-subject random intercepts
subj_RI_mod <- lmer(XXX ~ XXX +           # These are the fixed effects
                    (1|XXX),              # These are the random effects
                    data = XXX,           # Specify the data used
                    REML = FALSE)
```

```{r}
# Mixed effects model with by-subject random intercepts
subj_RI_mod <- lmer(Tomatometer ~ State + # These are the fixed effects
                    (1|ID),               # These are the random effects
                    data = tom,        # Specify the data used
                    REML = FALSE)
```

6. Using `summary()`, inspect the results of the mixed effects model.

```{r}
summary(subj_RI_mod)
summary(FE_mod)
```

7. Did the effect of `State` change, now that you incorporated the random effects? Can you find out what did change from the model outputs? 

```{r}
# While the estimates of the fixed effects did not change, the t-values did change
# substantially (the t-value for StateDrunk in FE_mod is lower than the one in
# subj_RI_mod, and the one for the intercept dropped by 50%).
```

8. Sometimes it is useful to extract the estimates from the model output (e.g., so we don't have to manually type them, which would be error prone). Extract the fixed effects from `subj_RI_mod` using the `fixef()` function.

```{r echo = TRUE, eval = FALSE}
fixef(XXX)
```

```{r}
fixef(subj_RI_mod)
```

9. Now extract the random effects using the `ranef()` function.

```{r}
ranef(subj_RI_mod)
```

10. Expand your mixed effects model from task B4 by adding by-subjects slopes. *Hint*: add the `State` variable to the left side of the bar `|` in the random effects part of the formula.

```{r echo = TRUE, eval = FALSE}
# Mixed effects model with by-subject random intercepts and slopes
subj_RI_RS_mod <- lmer(XXX ~ XXX +          # These are the fixed effects
                      (XXX|XXX),            # These are the random effects
                      data = XXX,           # Specify the data used
                      REML = FALSE)
```

```{r}
# Mixed effects model with by-subject random intercepts and slopes
subj_RI_RS_mod <- lmer(Tomatometer ~ State + # These are the fixed effects
                      (State|ID),            # These are the random effects
                      data = tom,         # Specify the data used
                      REML = FALSE)
```

11. Compare the outputs of the fixed effects only model, of the by-subjects random intercepts model, and of the by-subjects random intercepts and slopes model. Did the coefficients change? Why not? What did change?

```{r}
summary(subj_RI_RS_mod)
summary(subj_RI_mod)
summary(FE_mod)

# For FE_mod and subj_RI_mod the answer is the same as in task B5. The coefficients
# again didn't change, as the group means obviously also didn't change. But again
# the t-values changed considerably (e.g. the StateDrunk t-value in subj_RI_mod
# is 98.31, the on in subj_RI_RS_mod is 51.81).
```

12. Each movie was rated in both states, so there is also a repetition in the items, which again leads to a violation of the independence assumption. To account for this, expand the model from task B9 by adding by-movie random intercepts and slopes. *Note*: The additional argument specifies a different optimizer such that the model converges.

```{r echo = TRUE, eval = FALSE}
# Mixed effects model with by-subject and by-movie random intercepts and slopes
max_mod <- lmer(XXX ~ XXX +           # These are the fixed effects
               (XXX|XXX) + (XXX|XXX), # These are the random effects
               data = XXX,            # Specify the data used
               REML = FALSE,
               control = lmerControl(optimizer = "bobyqa")) # use a different optimizer
                                                            # to avoid non convergence
```

```{r}
# Mixed effects model with by-subject and by-movie random intercepts and slopes
max_mod <- lmer(Tomatometer ~ State +       # These are the fixed effects
               (State|ID) + (State|Movie),  # These are the random effects
               data = tom,               # Specify the data used
               REML = FALSE,
               control = lmerControl(optimizer = "bobyqa")) # use a different optimizer
                                                            # to avoid non convergence
```

13. The `converge_ok()` function from the `sjstats` package let's you check whether the model converged or not. If the functions returns `TRUE` you're good. Test whether `max_mod` converged.

```{r}
converge_ok(max_mod)
```

14. `max_mod` is now the *maximal model justified by the design*. What does this mean? Why is it a good thing to specify this model?

```{r}
# It means you specified all the possible random effects, including their correlations.
# Barr and colleagues found that Type I error rates are inflated in case of failure to
# specify the maximal structure. The results of the next task indeed confirm this. However,
# there is a tradeoff, as we will learn later on.
```

15. Compare the results with those of the earlier models by looking at their summary outputs using `summary()`.

```{r}
summary(max_mod)
summary(subj_RI_RS_mod)
summary(subj_RI_mod)
summary(FE_mod)
```

16. What changed with the increasingly complex random effects structure?

```{r}
# The t-value for the State effect changed dramatically. Now it is "only" 17.28.
# While still substantial, this illustrates the point Barr et al. 2013 make in their
# paper where they argue that for confirmatory hypothesis testing one should always
# specify the maximal random effects structure justified by the design, as otherwise
# the Type I error rate is inflated (i.e., the fixed effects are too often classified
# as being significant, even though they are in truth not related to the outcome
# variable).
```

### C - $R^2$

When fitting statistical models, we are often interested in how much systematic variation they can capture. In linear (mixed effects) models this is the $R^2$ value (the coefficient of determination); for generalized (mixed effects) models, we can compute pseudo $R^2$ values. 

1. We will use the `r2()` function from the `sjstats` package to compute $R^2$ values. Look at the help menu of the function to get an overview of what models you can use it for.

```{r}
?r2
```

2. Compute the $R^2$ of the maximal model.

```{r}
r2(max_mod)
```

3. What does the output of the last task mean? What is the marginal $R^2$ and what is the conditional $R^2$?

```{r}
# The marginal r-squared only considers the variance of the fixed effects. In this
# case this means that the fixed in our maximal model can account for 55% of the variation
# in the data.
# The conditional r-squared takes both the fixed and random effects into account. Our maximal
# model can thus account for 83% of the variance in the data.

```

4. Compare the $R^2$ values of `max_mod` with those of less complex models. Are the changes large? Which $R^2$ values changed?

```{r}
r2(max_mod)
r2(subj_RI_RS_mod)
r2(subj_RI_mod)
```

### D - Visualize Your Model

It is often useful to visualize your model's output. Run the code below to extract coefficients from `max_mod` and plot the data along with a few sample lines to visualize the variability in the slopes and intercepts. As plotting is not the topic of this bootcamp we don't have time to go into details here. If you are interested in learning more about plotting, there are many books and tutorials about it, and we also cover it in the *R for Data Science* bootcamp.

1. Run the following code to visualize your `max_mod` data.

```{r echo = TRUE}
# extract fixed effects
m_line <- fixef(max_mod)

# extract random effects
ranefs <- ranef(max_mod)
predicted <- tibble(
  intercept = ranefs$ID[,1] + fixef(max_mod)[1],
  slope = ranefs$ID[,2] + fixef(max_mod)[2])

# randomly draw 15 subjects to plot the fitted lines
rand15 <- sample(1:nrow(predicted), 15)

LMM_plot <- ggplot(tom, aes(State, Tomatometer)) +
  geom_point(colour= "#606061", alpha = .15, size = 2.5)+
  geom_segment(aes(x = 1, y = intercept, xend = 2, yend = intercept + slope),
               data = predicted %>% slice(rand15), colour = "#EA4B68", size = 1.5,
               alpha = .8) +
  geom_segment(aes(x = 1, y = m_line[1], xend = 2, yend = sum(m_line)),
               colour = "black", size = 2, alpha = 1) +
  theme(axis.title.x = element_text(vjust = -1),
        axis.title.y = element_text(vjust = 1)) +
  theme_bw() +
  ylim(0, 100) +
  theme(
    strip.text = element_text(size = 12, face = "bold"),
    axis.text = element_text(size = 16),
    axis.title = element_text(size = 18,face = "bold")
  )

LMM_plot
```


### E - Computing p-Values for Fixed Effects

As you probably noticed, the `lmer()` summary output does not include p-values. This is not because the authors of `lme4` were lazy, but because how to best compute p-values for mixed effects models is a still ongoing discussion. However, several possibilities exist of how to test whether a variable is a significant predictor (i.e., a significant fixed effect). We will look at a couple of them.

#### Likelihood Ratio Test

One possibility to obtain p-values is by conducting a likelihood ratio test ([LRT](https://en.wikipedia.org/wiki/Likelihood-ratio_test)). In an LRT, the model is fitted once with and once without the fixed effect of interest, all else being equal. These two models are then compared in using the LRT. This test only works if the models were fit using maximum likelihood (ML) estimation, rather than restricted maximum likelihood (REML) estimation, which is why we used `REML = FALSE` when fitting the mixed effects models.

1. First, fit an intercept only model (`IO_mod`) where you predict `Tomatometer` with only the grand mean, and the by-subjects and by-movies random intercepts.

```{r echo = TRUE, eval = FALSE}
# Intercept only mixed effects model with by-subject and by-movie random intercepts
IO_mod <- lmer(XXX ~ 1 +         # There are no fixed effects so add 1 to fit the intercept
              (1|XXX) + (1|XXX), # These are the random effects
              data = XXX,        # Specify the data used
              REML = FALSE)
```

```{r}
# Intercept only mixed effects model with by-subject and by-movie random intercepts
IO_mod <- lmer(Tomatometer ~ 1 +    # There are no fixed effects so add 1 to fit the intercept
               (1|ID) + (1|Movie),  # These are the random effects
               data = tom,       # Specify the data used
               REML = FALSE)
```


2. Look at the model output using `summary()`.

```{r}
summary(IO_mod)
```

3. Now fit the same model as in task E1, but add the `State` variables. Save the output as `RI_mod`. Note that we again need to use another optimizer to avoid convergence problems (You don't need to worry about this now, you can use the code template below). 

```{r echo = TRUE, eval = FALSE}
# Intercept only mixed effects model with by-subject and by-movie random intercepts
RI_mod <- lmer(XXX ~ XXX +         # The fixed effects
              (1|XXX) + (1|XXX),   # These are the random effects
              data = XXX,          # Specify the data used
              REML = FALSE,
              control = lmerControl(optimizer = "bobyqa")) # use a different optimizer
                                                            # to avoid non convergence
```

```{r}
# Intercept only mixed effects model with by-subject and by-movie random intercepts
RI_mod <- lmer(Tomatometer ~ State +    # The fixed effects
               (1|ID) + (1|Movie),  # These are the random effects
               data = tom,       # Specify the data used
               REML = FALSE,
               control = lmerControl(optimizer = "bobyqa")) # use a different optimizer
                                                            # to avoid non convergence
```

4. Check whether `RI_mod` converged using the `converge_ok()` function.

```{r}
converge_ok(RI_mod)
```

5. Perform an LRT using the `anova()` function, by entering both model outputs (`IO_mod`, and `RI_mod`) as arguments. Is the difference significant? What does this mean?

```{r echo = TRUE, eval = FALSE}
# Perform an LRT using the anova() function
anova(XXX, XXX)
```

```{r}
# Perform an LRT using the anova() function
anova(IO_mod, RI_mod)
```

#### Confidence Intervals

This method is not a formal way of testing significance, as it does not produce a p-value. In this method, confidence intervals are obtained and checked if they include zero. If they don't, this is interpreted as a significant result.

6. We can obtain confidence intervals for our fixed effects using the `confint()` function of the `lme4` package. Do this by entering `max_mod` into the `confint()` function.

```{r echo = TRUE, eval = FALSE}
# Compute confidence intervals for the fixed effects
ci_mod <- confint(XXX)
```

```{r}
# Compute confidence intervals for the fixed effects
ci_mod <- confint(max_mod)
```

7. Print the `ci_mod` object. Do the confidence intervals you obtained match the conclusion you drew from inspecting the t-values and the LRT?
  
#### Other Tests

8. Another way of obtaining p-values with the t-statistic and a [Wald test](https://en.wikipedia.org/wiki/Wald_test) by using the `p_value()` function of the `sjstats` package. Enter `max_mod` to the `p_value()` function.

```{r echo = TRUE, eval = FALSE}
# Compute p-values for the fixed effects
p_value(fit = XXX)
```

```{r}
# Compute p-values for the fixed effects
p_value(fit = max_mod)
```

9. As this test may not be appropriate for mixed effects models, we can extend the `p_value()` function with an additional argument, `p.kr = TRUE`. Then the function will compute p-values based on conditional F-tests with [Kenward-Roger approximation](https://www.jstatsoft.org/article/view/v059i09) for the degrees of freedom. Repeat the previous task by extending the function with `p.kr = TRUE`.

```{r}
# Compute p-values for the fixed effects with Kenward-Roger approximation
p_value(fit = max_mod, p.kr = TRUE)
```

10. Compare the output of the tasks E8 and E9 Did things change? The reason why not is that the assumptions were all satisfied as the data was simulated with these assumptions, but with noisier real life data this may look different.

### F - Determining the "Significance" of Random Effects (Model Selection)

You may remember the *keep it maximal* principle, which states that you should always specify the maximal random effects structure justified by the design to guard against type I error inflation ([Barr, Levy, Scheepers, & Tily, 2013](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3881361/)). However, the maximal model may come with a loss of power to detect fixed effects and it may make sense to select which random effects structure to use ([Matuschek, Kliegl, Vasishth, Baayen, & Bates, 2017](https://www.sciencedirect.com/science/article/pii/S0749596X17300013)). To express it with the words of Matuschek and Colleagues: "[A] parsimonious mixed model [...] containing only variance components supported by the data improves the balance between Type I error and power" (p. 305). To select which model to use, we will use LRTs.

When performing this LRT, we cannot use the usual alpha limit of .05. To see why, read the following quote from [Matuschek and colleagues (2017; p. 308)](https://www.sciencedirect.com/science/article/pii/S0749596X17300013)

> Within the context of model selection, it is important to resist the reflex of choosing $\alpha_{LRT} = 0.05$. The $\alpha_{LRT}$ cannot be interpreted as the ‘‘expected model-selection Type I error-rate” but rather as the relative weight of model complexity and goodness-of-fit. For example, choosing $\alpha_{LRT} = 0$, an infinite penalty on the model complexity is implied and consequently the minimal model is always chosen as the best, irrespective of the evidence provided by the data. Choosing $\alpha_{LRT} = 1$ implies an infinite penalty on the goodness-of-fit, and the maximal model is always chosen as the best. Therefore, choosing $\alpha_{LRT} = 0.05$ may imply an overly strong penalty on the model complexity and hence select a reduced model even if data favor a more complex one.

We will follow their example and use $\alpha_{LRT} = 0.2$. As the `anova()` function uses $\alpha = 0.05$ we will have to implement this procedure ourselves.

1. First, we have to fit the model whose complexity is one step lower compared to the maximal model. Think about which model this would be and write down the answer as a comment in your script. Don't cheat by looking at the next task!

```{r echo = TRUE}
# This is just a placeholder to increase the space to the next task to make it 
# easier to not cheat.

# There is nothing to see here...

# Just a random sidenote (which is actually very interesting, so you may want to
# to read on even if this kind of defies the purpose of this part to prevent you from
# looking at the answers first; so consider first completing the task and then reading
# the random but somehow terribly interesting sidenote):
#     Did you know that the reason why R uses the arrow "<-" as assignment
#     operator is because it is based on S, which in turn is based on APL.
#     Now apparently, APL was designed on a specific keyboard that had a
#     "<-" key and there was no "==" implemented to test equality. So equality
#     was tested using "=" and "<-" was chosen as asignment operator
#     (info obtained from this blogpost: https://colinfay.me/r-assignment/)
```


2. The answer to the last task is that you can constrain the correlation of the random intercepts and random slopes to zero. Use the double bar `||` in the random effects structure, that is `(design_matrix||grouping_variable)` to do this. Fit the maximal model but constrain the correlations to zero.

```{r echo = TRUE, eval = FALSE}
# Constrained mixed effects model with by-subject and by-movie random intercepts and slopes
con_mod <- lmer(XXX ~ XXX +             # These are the fixed effects
               (XXX||XXX) + (XXX||XXX), # These are the random effects
               data = XXX,              # Specify the data used
               REML = FALSE,
               control = lmerControl(optimizer = "bobyqa")) # use a different optimizer
                                                            # to avoid non convergence
```

```{r}
# Constrained mixed effects model with by-subject and by-movie random intercepts and slopes
con_mod <- lmer(Tomatometer ~ State +         # These are the fixed effects
               (State||ID) + (State||Movie),  # These are the random effects
               data = tom,                 # Specify the data used
               REML = FALSE,
               control = lmerControl(optimizer = "bobyqa")) # use a different optimizer
                                                            # to avoid non convergence
```

3. Check whether the model converged using `converge_ok()`.

```{r}
converge_ok(con_mod)
```

4. This model didn't converge. This can happen and it's not always easy to find out how to get the model to converge. One way would be to use different Bayesian methods with the `rstanarm` package. This is rather advanced and we will ignore it for now, but should this happen to you in your work you may not want to interpret this model.

5. Now let's prepare the setup for the LRT. The difference in the deviances of two models (defined as negative two times the log-likelihood) approximately follows a $\chi^2$ distribution. Therefore we can test the significance by testing whether the $\chi^2$ value is larger than a threshold value. To do this we set up our $\alpha_{LRT}$ and derive the threshold value. Do this by running the following code.

```{r echo = TRUE}
alpha <- .2 # set up the alpha value
st <- qchisq(1 - alpha, df = 2) # derive the critical value above which the difference
                                # is significant. We have two degrees of freedom because
                                # the constrained model has two more degrees of freedom 
```

6. Now we can compute the difference in the deviances by subtracting the deviance from the more complex model from the constrained model. Extract the deviances from the two models using the `deviance()` function from the `stats` package and save the difference as `d_diff`.

```{r echo = TRUE, eval = FALSE}
d_diff <- deviance(XXX) - deviance(XXX)
```

```{r}
d_diff <- deviance(con_mod) - deviance(max_mod)
```

7. Test whether the difference in deviance is large enough to be classified as a significant deviance according to our $\alpha_{LRT}$ level. *Hint*: Test whether `d_diff` is larger than the significance threshold `st`.

```{r}
# Test whether the difference in the deviance between the two models is large enough
# to be considered significant
d_diff > st
```

8. The output of the previous task was `FALSE`. What does that mean? Do you keep the maximal model in this case? What would it mean if the result were `TRUE`, how would your next steps look?

```{r}
# FALSE as output in this case means that the difference between the models was
# not large enough that we can consider the maximal model to fit the data better.
```

### G - Residual Variances

1. We may be interested in knowing the residual variance (the error; denoted as within group variance in the output), and the random effects variances. Extract this information from the maximal model output by using the `re_var()` function from the `sjstats` package.

```{r}
re_var(max_mod)
```

2. Look at the output from the last task. Are the different variances large? Is the within group variance or the between group variance larger and how do you interpret this information?



### X - Challenges

#### More on p-Values for Fixed Effects: Parametric Bootstrap

Another test of the significance of fixed effects, and the one you should prefer in an uncertain case (i.e., if p-values are on the border of your alpha-level), is to use parametric bootstrap, where, many times, data is resampled with replacement and then the model is fitted again, to obtain an empirical distribution of the effect or parameter of interest. The drawback of this technique is that it can take rather long.

1. For linear mixed effects models (fit using `lmer()`) you can use the `PBmodcomp()` function from the `pbkrtest` package. Check out the help function of `PBmodcomp()` like this.

```{r eval = FALSE, echo=TRUE}
?PBmodcomp
```

2. Now use the `PBmodcomp()` function to get p-values for the model. You will have to specify a less complex model against which to test. Use `IO_mod` for this. Also, because this procedure can take rather long, only use 100 simulations.

```{r eval = FALSE, echo=TRUE}
# perform parametric bootstrap
pb_mod <- PBmodcomp(largeModel = XXX, 
                    smallModel = XXX, 
                    nsim = XXX)
```


```{r}
# perform parametric bootstrap
pb_mod <- PBmodcomp(largeModel = max_mod,
                    smallModel = IO_mod,
                    nsim = 100)
```

3. Print the `pb_mod` object and inspect the results. Are the results different from the other tests you've performed before? How does the p-value from the parametric bootstrap test compare to the one obtained from the likelihood test?

#### Intra-Class Correlation

The intra-class correlation (ICC) is "the proportion of the variance explained by the grouping structure in the population" (Hox 2002, p.15). It is calculated by dividing the between-group variance by the sum of the within-group and the between-group variance (i.e., the total variance).

1. Compute the ICC of the maximal model using the `icc()` function from the `sjstats` package.

```{r}
icc(max_mod)
```

2. Take a look at the output. Did you notice the warning? This is because the ICC cannot be interpreted in the same way for random slopes and intercepts models. Add the argument `adjusted = TRUE` to your ICC call to take care of the warning.

```{r}
icc(max_mod, adjusted = TRUE)
```

3. Check the `icc()` help page to understand what happened now that you added the `adjusted = TRUE` argument.

4. Often, the ICC is computed on the intercept only model with random intercepts. In task C1. you have already fitted this model and saved it as `IO_mod`. Compute the ICC form this model. *Hint*: no need to use the `adjusted` argument, as no random slopes are involved.

```{r}
icc(IO_mod)
```

4. Now do the same for the `RI_mod`, that differs from the `IO_mod` you've just used only in that it also contains the fixed effect.

```{r}
icc(RI_mod)
```

5. Compare the outputs of the last two tasks. Did things change? Think about what these changes mean.

#### Crossed Versus Nested Random Effects

So far we only considered crossed random effects. Now we will also look at data with *nested random effects*, where every level of a nested factor only appears within a single level of a higher order factor. A popular example is the case of classes within schools. Every class is only part of a single school and thus class is nested within schools.

To know whether random effects are crossed or nested, you usually need to know the design, as the structure is often not obvious from the factor levels. For example, even though class may be nested in schools, the classes may be numbered from 1 to the number of classes in that school. So school 1 may have classes 1 to 10, and school 2 may have classes from 1 to 6. Obviously class 1 of school 1 is not the same as class 1 of school 2. However, what is obvious for you is not obvious for R. It doesn't know about the concept of schools and classes, so you have to tell it whether these factors are nested or not, using the appropriate syntax in the formula. Now consider the case where the classes are nested in schools, but numbered from 1 to total number of classes, that is, such that every class has a unique label. In this case it does not matter what structure you specify.

1. Now we will work with the `school` data set you have already loaded in section A. Using the `table()` function, create a cross table of school and class. (*Hint*: You'll have to enter the variables separately.)

```{r eval = FALSE, echo = TRUE}
table(XXX, XXX)
```

```{r}
table(schools$school, schools$class)
```

2. Fit a mixed effects model predicting extraversion (`extra`) with openness (`open`) and the social score (`social`) with random intercepts for classes nested in school and save it as `hier_mod`. *Hint*: The nested random effects structure is specified with the following syntax `(1|higher_level_variable/lower_level_variable)`.

```{r}
# Hierarchical mixed effects model
hier_mod <- lmer(extra ~ open + social + # These are the fixed effects
               (1|school/class),        # These are the nested random effects
               data = schools,          # Specify the data used
               REML = FALSE)
```

3. Did the model converge? Check with `converge_ok()`

```{r}
converge_ok(hier_mod)
```

4. Inspect the model output using `summary()`.

```{r}
summary(hier_mod)
```

5. Now fit the same model but with crossed random intercepts as in the sections before, rather than with nested random intercepts. Save the output as `cross_mod`.

```{r}
# Hierarchical mixed effects model
cross_mod <- lmer(extra ~ open + agree +  # These are the fixed effects
                 (1|school) + (1|class),  # These are the crossed random effects
                 data = schools,          # Specify the data used
                 REML = FALSE)
```

6. Check whether the model converged.

```{r}
converge_ok(cross_mod)
```

7. Inspect the output using the `summary()` function.

```{r}
summary(cross_mod)

```

8. Did the results change compared to the ones for the nested random effects structure?

```{r}
# There are changes in t-values, and estimates of the fixed effects. Furthermore,
# the variance estimates of the random effects changed, as well as the goodness of fit,
# that is the model with the crossed random effects structure fits way worse.
```

#### More Random Effects Selection

1. In task D6 you found out that the constrained model with correlations between random effects set to zero is not significantly worse than the maximal model. Thus, if you were testing a confirmatory hypothesis, you should choose this model rather than the maximal model. What we didn't do in section F is to proceed with our backward selection procedure. This is now your task. Fit a model with by-subject random intercepts and slopes, but without their correlation, and with by-movie random intercepts. 

```{r}
# Constrained model with by-subjects random intercepts and slopes, without their
# correlation, and with by-movie random intercepts
con_mod2 <- lmer(Tomatometer ~ State +         # These are the fixed effects
                (State||ID) + (1|Movie),      # These are the random effects
                data = tom,                 # Specify the data used
                REML = FALSE,
                control = lmerControl(optimizer = "bobyqa")) # use a different optimizer
                                                             # to avoid non convergence
```

2. Now perform the procedure of Section D. to evaluate which of the two models to keep. *Note*: You will have to set the `df` argument in the `qchisq()` function to `df = 1` because these two models only differ in one degree of freedom.

```{r}
# alpha is still the same
alpha <- .2 # set up the alpha value
st1 <- qchisq(1 - alpha, df = 1) # derive the critical value above which the difference
                                 # is significant. This time we have only one degree of
                                 # freedom because the more complex model only has one
                                 # parameter more than the the simpler model

# get the difference in deviance between the two models
d_diff <- deviance(con_mod2) - deviance(con_mod)

# Test whether this difference is large enough to be considered significant
d_diff > st1
```

3. What is your conclusion which model to keep and work with?

```{r}
# con_mod, with correlations constrained to zero would be selected as the maximal model
# does not perform siginificantly better, and the less complex models perform significantly
# worse. As this model did not converge and the maximal model did, it may in this case still
# make sense to go with the maximal model.
```

#### Generalized Linear Mixed Effects Models

1. Now we will look at generalized linear mixed effects model. Load the `cancer_remission.csv` data into R and save it as `cr`.

```{r}
cr <- read_csv("1_Data/cancer_remission.csv")
```

2. Take a look at the data. It is a subset of simulated data from [UCLA Institute for Digital Research and Education Search this website](https://stats.idre.ucla.edu/).

```{r}
cr
summary(cr)
View(cr)
```

3. Predict cancer remission (`remission`) by the cancer stage (`CancerStage`), the length of the patients stay (`LengthofStay`), the doctors experience (`Experience`), and by specifying by doctor (`DID`) random intercepts. Use the `glmer` function with `family = binomial` to do this. Also, use the "bobyqa" optimizer as in some examples before.

```{r}
cr_mod <- glmer(XXX ~ XXX + XXX + XXX +
                (1 | XXX),
                data = XXX,
                family = binomial, # with this you tell glmer to run a logistic regression
                control = glmerControl(optimizer = "bobyqa"))
```


```{r}
cr_mod <- glmer(remission ~ CancerStage + LengthofStay + Experience +
                (1 | DID),
                data = cr,
                family = binomial, # with this you tell glmer to run a logistic regression
                control = glmerControl(optimizer = "bobyqa"))
```

4. Did the model converge?

```{r}
converge_ok(cr_mod)
```

5. Inspect the model output.

```{r}
summary(cr_mod)
```


6. Extract the $R^2$ value for this model, once using the `r2()` and once using the `cod()` function. Are the results the same? Find out what the differences are by checking the help pages.

```{r}
r2(cr_mod)
cod(cr_mod)
```


**Note**:
Running and interpreting generalized linear mixed effects models can be quite challenging and we don't have the time here to cover them in more detail. You can check out this [tutorial](https://stats.idre.ucla.edu/other/mult-pkg/introduction-to-generalized-linear-mixed-models/) or the references listed under resources if you want to learn more about this.


## Examples

```{r, eval = FALSE, echo = TRUE}

# load packages
library(lme4)
library(sjstats)

# look at sleepstudy data contained in lme4
head(sleepstudy)

# look at describtion of sleepstudy
?sleepstudy

# fixed effects only model predicting the reaction time
# from the number of days with sleep deprivation
sleep_FE <- glm(Reaction ~ Days, data = sleepstudy)

# inspect model
summary(sleep_FE)

# intercept only model mixed effects model
sleep_IO <- lmer(Reaction ~ 1 + (1|Subject),
                 data = sleepstudy,
                 REML = FALSE)

# inspect model
summary(sleep_IO)

# run random interceptes model
sleep_RI <- lmer(Reaction ~ Days + (1|Subject),
                 data = sleepstudy,
                 REML = FALSE)

# inspect output
summary(sleep_RI)

# check whether model converged
converge_ok(sleep_RI)

# get p_values for fixed effects using Kenward-Roger approximation
p_value(sleep_RI, p.kr = TRUE)

# alternatively, get p-value using anova() function
anova(sleep_IO, sleep_RI)

# run mixed effects model without correlations between random effects parameters
sleep_nc <- lmer(Reaction ~ Days + (Days||Subject),
                 data = sleepstudy,
                 REML = FALSE)

# check whether the model converged
converge_ok(sleep_nc)

# check model output
summary(sleep_nc)

# run maximal model (by-subjects random slopes and intercepts)
sleep_max <- lmer(Reaction ~ Days + (Days|Subject),
                 data = sleepstudy,
                 REML = FALSE)

# check whether the model converged
converge_ok(sleep_max)

# check model output
summary(sleep_max)

### select the random effects structure

# set alpha to .2
alpha <- .2

# derive the critical value above which the difference is significant.
st1 <- qchisq(1 - alpha, df = 1)

# get the difference in deviance between the two models
d_diff <- deviance(sleep_nc) - deviance(sleep_max)

# Test whether this difference is large enough to be considered significant
d_diff > st1

# correlation is not necessary. Now test whether the random slopes are necessary
d_diff <- deviance(sleep_RI) - deviance(sleep_nc)

# Test whether this difference is large enough to be considered significant
d_diff > st1 # -> keep sleep_nc model

# compute r-squared
r2(sleep_nc)

# compute icc
icc(sleep_nc, adjusted = TRUE)

### Visualize Model

# extract fixed effects
m_line <- fixef(sleep_nc)

# extract random effects
ranefs <- ranef(sleep_nc)
predicted <- tibble(
  intercept = ranefs$Subject[,1] + fixef(sleep_nc)[1],
  slope = ranefs$Subject[,2] + fixef(sleep_nc)[2])

# randomly draw 10 subjects to plot the fitted lines
rand10 <- sample(1:nrow(predicted), 10)

LMM_plot <- ggplot(sleepstudy, aes(Days, Reaction)) +
  geom_point(colour= "#606061", alpha = .15, size = 2.5)+
  geom_segment(aes(x = 0, y = intercept, xend = 9, yend = intercept + slope * 9),
               data = predicted %>% slice(rand10), colour = "#EA4B68", size = 1.5,
               alpha = .8) +
  geom_segment(aes(x = 0, y = m_line[1], xend = 9, yend = m_line[1] + 9 * m_line[2]),
               colour = "black", size = 2, alpha = 1) +
  theme(axis.title.x = element_text(vjust = -1),
        axis.title.y = element_text(vjust = 1)) +
  theme_bw() +
  theme(
    strip.text = element_text(size = 12, face = "bold"),
    axis.text = element_text(size = 16),
    axis.title = element_text(size = 18,face = "bold")
  )

LMM_plot
```


## Datasets

|File | Rows | Columns |
|:----|:-----|:------|
|[tomatometer.csv](https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/1_Data/tomatometer.csv?token=AIFo1JUJrMd9LfEcBY6u9ZhqlCcMaTvGks5csP1AwA%3D%3D) | `r nrow(tom)` | `r ncol(tom)` |
|[schools.csv](https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/1_Data/schools.csv?token=AZQb9mdlio2aNiTgfaddDHxpENKWm3Hhks5cmQ9jwA%3D%3D) | `r nrow(schools)` | `r ncol(schools)` |
|[cancer_remission.csv](https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/1_Data/cancer_remission.csv?token=AZQb9ic1yv1-UdOtMITu0HyaW1xRMdV9ks5cmQ-bwA%3D%3D) | `r nrow(cr)` | `r ncol(cr)` |

The *tomatometer.csv* dataset contains tomatometer ratings from 200 raters who each rated 15 movies, once while they were sober and once while they were drunk.

The *schools.csv* dataset contains ratings of extraversion, openness to experience, agreeableness, and a social score of 1200 students from different classes of 6 different schools.

The *cancer_remission.csv* dataset contains data on lung cancer remission from patients nested within doctors

#### tomatometer.csv

| Name | Description |
|:-------------|:-------------------------------------|
| ID | Participant id code |
| Movie | Movie ID from M1 to M15 |
|State| the state in which the rating was done ("sober", or "drunk") |
|Tomatometer| tomatometer rating from 0 to 100|

#### schools.csv

| Name | Description |
|:-------------|:-------------------------------------|
| id | pupil id |
| extra | extraversion rating of pupil from 0 to 100|
|open| openness to experience rating of pupil from 0 to 100|
|agree| agreeableness rating of pupil from 0 to 100|
|social| social score of pupil |
|class| the class a pupil is in (nested under school; levels "a", "b", "c", and "d") |
|school| the school a pupil / class is in (levels "I", "II", "III", "IV") |

#### cancer_remission.csv

| Name | Description |
|:-------------|:-------------------------------------|
| remission | whether the cancer is in remission (0 = No, 1 = Yes) |
| CancerStage | four different cancer stages (levels "I", "II", "III", "IV") |
|LengthofStay| how long a participant stayed in hospital (score ranging from 1 to 10) |
|Experience| experience of the doctor (ranging from 7 to 29, probably years of experience)|
|DID| doctor id |


## Functions

### Packages

|Package| Installation|
|:------|:------|
|`tidyverse`|`install.packages("tidyverse")`|
| `lme4` | `install.packages("lme4")` |
| `sjstats` | `install.packages("sjstats")` |
| `pbkrtest` | `install.packages("pbkrtest")` |

### Functions

| Function| Package | Description |
|:---|:------|:---------------------------------------------|
|     `lmer`|`lme4`| Fit a linear mixed effects model | 
|     `glmer`|`lme4`| Fit a generalized linear mixed effects model | 
|     `fixef`|`lme4`| Extract fixed effects coefficients from lmer or glmer output | 
|     `ranef`|`lme4`| Extract random effects coefficients from lmer or glmer output | 
|     `anova`|`stats`| Generic function to run (in this case) a likelihood ratio test | 
|     `confint`|`stats`| Compute confidence intervals for various statistical outputs | 
|     `deviance`|`stats`| Extract the deviance of various statistical outputs | 
|     `qchisq`|`stats`| Quantile function of the $\chi^2$ distribution to get critical $\chi^2$ values | 
|     `PBmodcomp`|`pbkrtest`| Perform parametric bootstrap to obtain p-values from linear mixed effects models | 
|     `converge_ok`|`sjstats`| Test whether a model converged or not (some warnings produced by lmer and glmer may be informative but not necessarily mean you need to worry, so this function is a good proxy to use. If it yields `FALSE` you may want to investigate things and try to take measures to improve your model) | 
|     `p_value`|`sjstats`| Compute p-values for fixed effects of mixed effects models using Wald's test or conditional F-tests with Kenward-Roger approximation for the degrees of freedom | 
|     `r2`|`sjstats`| Extract $R^2$ value from (generalized) linear (mixed effects) model outputs | 
|     `icc`|`sjstats`| Compute ICCs from (generalized) linear mixed effects model outputs | 
|     `re_var`|`sjstats`| Extract random effects and residual variances from (generalized) linear mixed effects model outputs | 

## Resources

### Vignettes

[sjstats webpage](https://strengejacke.github.io/sjstats/)


[lme4 vignette](https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf)

### Further Reading

A nice and non-technical introduction to mixed effects models can be found in a chapter titled  [**An introduction to linear mixed modeling in experimental psychology**](http://davidkellen.org/wp-content/uploads/2017/04/introduction-mixed-models.pdf) by **Henrik Singmann** and **David Kellen**.

A more technical introduction can be found in the mixed effects models chapter of  [**Analysis of Longitudinal and Cluster-Correlated Data**](https://projecteuclid.org/euclid.cbms/1462106081) by **Nan Laird**.

A freely available tutorial on mixed effects models can be found [here](https://www.ssc.wisc.edu/sscc/pubs/MM/MM_Introduction.html).

An introduction to $R^2$ and ICC in mixed effects model can be found in the article
[The coefficient of determination R2 and intra-class correlation coefficient from generalized linear mixed-effects models revisited and expanded](https://royalsocietypublishing.org/doi/10.1098/rsif.2017.0213) by **Shinichi Nakagawa**, **Paul Johnson**, and **Holger Schielzeth**.

A rather technical introduction to mixed effects model with `lme4` is given in [**Fitting Linear Mixed-Effects Models Using lme4**](https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf) by **Douglas Bates**, **Martin Mächler**, **Benjamin Bolker**, and **Steven Walker**.

One of the classic introductions to regression models, including mixed effects models, is [**Data Analysis Using Regression and Multilevel/Hierarchical Models**](https://www.cambridge.org/ch/academic/subjects/statistics-probability/statistical-theory-and-methods/data-analysis-using-regression-and-multilevelhierarchical-models?format=PB&isbn=9780521686891) by **Andrew Gelman** and **Jennifer Hill**.

Another great introduction to regression, including mixed effects models, is [**Statistical Rethinking**](https://xcelab.net/rm/statistical-rethinking/) by **Richard McElreath**.

