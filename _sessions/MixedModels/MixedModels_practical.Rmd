---
title: "Mixed Models"
author: "<table style='table-layout:fixed;width:100%;border:0;padding:0;margin:0'><col width='10%'><col width='10%'>
  <tr style='border:none'>
    <td style='display:block;width:100%;text-align:left;vertical-align:bottom;padding:0;margin:0;border:none' nowrap>
      <font style='font-style:normal'>Statistics with R</font><br>
      <a href='https://therbootcamp.github.io/SWR_2019Apr/'>
        <i class='fas fa-clock' style='font-size:.9em;' ></i>
      </a>
      <a href='https://therbootcamp.github.io'>
        <i class='fas fa-home' style='font-size:.9em;'></i>
      </a>
      <a href='mailto:therbootcamp@gmail.com'>
        <i class='fas fa-envelope' style='font-size: .9em;'></i>
      </a>
      <a href='https://www.linkedin.com/company/basel-r-bootcamp/'>
        <i class='fab fa-linkedin' style='font-size: .9em;'></i>
      </a>
      <a href='https://therbootcamp.github.io'>
        <font style='font-style:normal'>Basel R Bootcamp</font>
      </a>
    </td>
    <td style='width:100%;vertical-align:bottom;text-align:right;padding:0;margin:0;border:none'>
      <img src='https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/by-sa.png' style='height:15px;width:80px'/>
    </td>
  </tr></table>"
output:
  html_document:
    css: practical.css
    self_contained: no
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(comment = NA, 
                      fig.width = 6, 
                      fig.height = 6,
                      fig.align = 'center',
                      echo = FALSE, 
                      eval = FALSE, 
                      warning = FALSE)

options(digits = 3)

# Load packages
library(tidyverse)
library(lme4)
library(sjstats)
library(pbkrtest)

# Load data
tom_df <- read_csv("1_Data/Tomatometer_dat.csv")
schools <- read_csv("1_Data/schools.csv")
cr <- read_csv("1_Data/cancer_remission.csv")
```

<p align="center" width="100%">
  <img src="image/rottentomatoes.png" alt="Trulli" style="width:100%;height:280px">
  <br>
  <font style="font-size:10px">from <a href="https://www.rottentomatoes.com/">rottentomatoes.com</a></font>
</p>


# {.tabset}

## Overview

In this practical you'll practice "mixed effects modeling" with the `lme4`, `sjstats`, `stats`, and `pbkrtest` packages.

By the end of this practical you will know how to:

1. Run mixed effects models in R.
2. Extract p-values for fixed effects.
3. Select the appropriate random effects structure.
4. Specify crossed vs. nested random effects.
5. Extract variance components and compute the explained variance and intra-class correlation
6. Visualize your linear mixed effects model.
7. Run a generalized mixed effects model.

## Tasks

### A - Setup

1. Open your `BaselRBootcamp` R project. It should already have the folders `1_Data` and `2_Code`. Make sure that the data files listed in the `Datasets` section above are in your `1_Data` folder

```{r}
# Done!
```

2. Open a new R script. At the top of the script, using comments, write your name and the date. Save it as a new file called `MixedModels_practical.R` in the `2_Code` folder.  

```{r}
# Done!
```

3. Using `library()` load the `tidyverse`, `lme4`, and `sjstats` packages (if you don't have them, you'll need to install them with `install.packages()`)!

```{r, echo = TRUE, eval = FALSE}
# Load packages necessary for this script
library(tidyverse)
library(lme4)
library(sjstats)
```

4. Using the following template, load the `Tomatometer_dat.csv` and `schools.csv` data into R and store it as a new object called `tom_df` and `schools`, respectively (Hint: Don't type the path directly! Use the "tab" completion!).

```{r, echo = TRUE, eval = FALSE}
# Load Tomatometer_dat.csv from the 1_Data folder
XX <- read_csv(file = "XX/XX")

# Load school.csv from the 1_Data folder
XX<- read_csv("XX/XX")
```


```{r}
# Load Tomatometer_dat.csv from the 1_Data folder
tom_df <- read_csv(file = "1_Data/Tomatometer_dat.csv")

# Load school.csv from the 1_Data folder
schools <- read_csv("1_Data/schools.csv")
```

5. Take a look at the first few rows of the datasets by printing them to the console.

```{r, echo = TRUE, eval = FALSE}
# Print the tom_df object
XXX
XX
```

```{r}
# Print the tom_df object
tom_df
schools
```

7. Use the the `summary()` function to print more details on the columns of the datasets.

```{r, echo=TRUE, eval = FALSE}
summary(XXX)
summary(XXX)
```

```{r}
summary(tom_df)
summary(schools)
```



8. Use the `View()` function to view the entire dataframe in a new window.

```{r, echo = TRUE, eval = FALSE}
View(XXX)
View(XXX)
```

```{r}
View(tom_df)
View(schools)
```

### B - Running a Linear Mixed Effects Model

In the first part of this practical we will work with the `tom_df` data example from the slides and test the effect of a person's `State` ("Sober" vs. "Drunk") on his or her `Tomatometer` rating.

1. Run fixed effects only model predicting `Tomatometer` with `State` and save the result as `FE_mod`. Then inspect the results.

```{r, echo = TRUE, eval = FALSE}
# Use lm, as lmer only works if at least one random effect is specified
FE_mod <- glm(XXX ~ XXX, data = XXX)

# Inspect the resuts
summary(FE_mod)
```

```{r}
# Use lm, as lmer only works if at least one random effect is specified
FE_mod <- glm(Tomatometer ~ State, data = tom_df)

# look at the resuts
summary(FE_mod)
```
2. In your model, it took "Drunk" as base state, so the regression coefficient you obtained shows how much lower the ratings in `State == "Sober"` are. It may be more intuitive to have "Sober" as the base level. To do this we have to coerce the `State` variable into a factor and set the levels. Do this by running the following code.

```{r, echo = TRUE}
# Coerce the State variable into a factor
tom_df <- tom_df %>%
  mutate(State = factor(State, levels = c("Sober", "Drunk")))
```

3. Now rerun your model from task B1 (the only thing that should change is that your coefficient is now called `StateDrunk` and now has a positive sign, and that your Intercept now shows the mean of the Sober condition).

```{r}
# Use lm, as lmer only works if at least one random effect is specified
FE_mod <- glm(Tomatometer ~ State, data = tom_df)

# look at the resuts
summary(FE_mod)
```


4. In the talk, you have learned that if we have repeated measures, the indepence assumption is violated. We can account for this by running a mixed effects model. Run a model with by-subjects random intercepts (subjects identifiers are stored in the `ID` variable). *Hint*: Random effects are specified in parenthesis in the formula in the following way `(`[design_matrix](https://en.wikipedia.org/wiki/Design_matrix)`|grouping_variable)`. **Note**: Do you see the `REML = FALSE` in the model specification? It tells R to fit the model using maximum likelihood (ML), rather than restricted maximul likelihood (REML; for more information on this in a technical approach see [here](https://projecteuclid.org/euclid.cbms/1462106081) and [here](http://www.stats.net.au/Maths_REML_manual.pdf), for a less technical approach see [here](https://en.wikipedia.org/wiki/Restricted_maximum_likelihood)). This will later be important for certain model comparisons that only work if the model was fitted using ML.


```{r echo = TRUE, eval = FALSE}
# Mixed effects model with by-subject random intercepts
subj_RI_mod <- lmer(XXX ~ XXX +           # These are the fixed effects
                    (1|XXX),              # These are the random effects
                    data = XXX,           # Specify the data used
                    REML = FALSE)
```

```{r}
# Mixed effects model with by-subject random intercepts
subj_RI_mod <- lmer(Tomatometer ~ State + # These are the fixed effects
                    (1|ID),               # These are the random effects
                    data = tom_df,        # Specify the data used
                    REML = FALSE)
```


5. Using `summary()`, inspect the results of the mixed effects model. Did the effect of `State` change, now that you incorporated the random effects? Can you find out what did change from the model outputs? (Did you note that there were no p-values in the output? We will look into that later in this practical)


```{r}
summary(subj_RI_mod)
summary(FE_mod)

# While the estimates of the fixed effects did not change, the t-values did change
# substantially (the t-value for StateDrunk in FE_mod is lower than the one in
# subj_RI_mod, and the one for the intercept dropped by 50%).
```


6. Sometimes when we want to make plots or tables, it is usefull if we can extract the estimates from the model output so we don't have to manually type them (which is error prone). The `lme4` package provides functions for this. Extract the fixed effects of your model using the `fixef()` function.

```{r echo = TRUE, eval = FALSE}
fixef(XXX)
```

```{r}
fixef(subj_RI_mod)
```

7. Now extract the random effects using the `ranef()` function.
```{r}
ranef(subj_RI_mod)
```



8. Now expand your mixed effects model from task B4. by adding by-subjects slopes. To do this add the `State` variable to the left side of the bar `|` in the random effects part of the formula.

```{r echo = TRUE, eval = FALSE}
# Mixed effects model with by-subject random intercepts and slopes
subj_RI_RS_mod <- lmer(XXX ~ XXX +          # These are the fixed effects
                      (XXX|XXX),            # These are the random effects
                      data = XXX,           # Specify the data used
                      REML = FALSE)
```

```{r}
# Mixed effects model with by-subject random intercepts and slopes
subj_RI_RS_mod <- lmer(Tomatometer ~ State + # These are the fixed effects
                      (State|ID),            # These are the random effects
                      data = tom_df,         # Specify the data used
                      REML = FALSE)
```

9. Compare the outputs of the fixed effects only model, of the by-subjects random intercepts model, and of the by-subjects random intercepts and slopes model. Did the coefficients change? Why not? What did change?

```{r}
summary(subj_RI_RS_mod)
summary(subj_RI_mod)
summary(FE_mod)

# For FE_mod and subj_RI_mod the answer is the same as in task B5. The coefficients
# again didn't change, as the group means obviously also didn't change. But again
# the t-values changed considerably (e.g. the StateDrunk t-value in subj_RI_mod
# is 98.31, the on in subj_RI_RS_mod is 51.81).
```


10. Because each movie was rated in both states, there is also a repetition in the items. This again leads to a violation of the independence assumption. To account for this, we also have to include by-item (i.e. by-movie) random effects. Expand the model from task B8. by adding by-movie random intercepts and slopes. This is now the *maximal model justified by the design* as it incorporates all possible random effects structures.

```{r echo = TRUE, eval = FALSE}
# Mixed effects model with by-subject and by-movie random intercepts and slopes
max_mod <- lmer(XXX ~ XXX +           # These are the fixed effects
               (XXX|XXX) + (XXX|XXX), # These are the random effects
               data = XXX,            # Specify the data used
               REML = FALSE,
              control = lmerControl(optimizer = "bobyqa")) # use a different optimizer
                                                            # to avoid non convergence
```

```{r}
# Mixed effects model with by-subject and by-movie random intercepts and slopes
max_mod <- lmer(Tomatometer ~ State +       # These are the fixed effects
               (State|ID) + (State|Movie),  # These are the random effects
               data = tom_df,               # Specify the data used
               REML = FALSE,
              control = lmerControl(optimizer = "bobyqa")) # use a different optimizer
                                                            # to avoid non convergence
```

11. Compare the results with those of the earlier models.

```{r}
summary(max_mod)
summary(subj_RI_RS_mod)
summary(subj_RI_mod)
summary(FE_mod)

# oops. See how the t-value for the State effect changed? Now it is "only" 17.28.
# While still substantial, this illustrates the point Barr et al. 2013 make in their
# paper where they argue that for confirmatory hypothesis testing one should always
# specify the maximal random effects structure justified by the design, as otherwise
# the Type I error rate is inflated (i.e., the fixed effects are too often classified
# as being significant, even though they are in truth not related to the outcome
# variable).
```

12. The `converge_ok()` function from the `sjstaty` package let's you check whether the model converged or not. If the functions returns `TRUE` you're good. Test whether `max_mod` converged.

```{r}
converge_ok(max_mod)
```


### C - Computing p-Values for Fixed Effects

As you probably noticed, the `lmer()` summary output does not include p-values. This is not because the authors of `lme4` were lazy, but because how to best compute p-values for mixed effects models is a still ongoing discussion. However, several possibilities exist of how to test whether a variable is a significant predictor (i.e., for now, a significant fixed effect).

#### Likelihood Ratio Test

One possibility to obtain p-values is by running a likelihood ration test ([LRT](https://en.wikipedia.org/wiki/Likelihood-ratio_test)). In an LRT, the model is fitted once with and once without the fixed effect of interest, all else being equal. These two models are then compared in using the LRT. However, LRTs tend to slightly underestimate the p-values of fixed effects, so if the p-value is just below the threshold (often .05), you may want to use a more accurate, more conservative method. This test is why we used `REML = FALSE` when fitting the mixed effects models.

1. For this test, we will use the a model with by-subject and by-item random intercepts, and compare it to the same model without the fixed effect of interest (i.e., without `State`). First, fit the intercept only model (`IO_mod`).

```{r echo = TRUE, eval = FALSE}
# Intercept only mixed effects model with by-subject and by-movie random intercepts
IO_mod <- lmer(XXX ~ 1 +         # There are no fixed effects so add 1 to fit the intercept
              (1|XXX) + (1|XXX), # These are the random effects
              data = XXX,        # Specify the data used
              REML = FALSE)
```

```{r}
# Intercept only mixed effects model with by-subject and by-movie random intercepts
IO_mod <- lmer(Tomatometer ~ 1 +    # There are no fixed effects so add 1 to fit the intercept
               (1|ID) + (1|Movie),  # These are the random effects
               data = tom_df,       # Specify the data used
               REML = FALSE)
```


2. Look at the model output using `summary()`.

```{r}
summary(IO_mod)
```

3. Now fit the by-subject and by-item random intercepts. Save the output as `RI_mod`. Note that we need to use another optiomizer to avoid convergence problems (You don't need to worry about this now, you can use the code template below). 

```{r echo = TRUE, eval = FALSE}
# Intercept only mixed effects model with by-subject and by-movie random intercepts
RI_mod <- lmer(XXX ~ XXX +         # The fixed effects
              (1|XXX) + (1|XXX),   # These are the random effects
              data = XXX,          # Specify the data used
              REML = FALSE,
              control = lmerControl(optimizer = "bobyqa")) # use a different optimizer
                                                            # to avoid non convergence
```

```{r}
# Intercept only mixed effects model with by-subject and by-movie random intercepts
RI_mod <- lmer(Tomatometer ~ State +    # The fixed effects
               (1|ID) + (1|Movie),  # These are the random effects
               data = tom_df,       # Specify the data used
               REML = FALSE,
               control = lmerControl(optimizer = "bobyqa")) # use a different optimizer
                                                            # to avoid non convergence
```

4. Perform an LRT using the `anova()` function, by entering both model outputs as arguments.

```{r echo = TRUE, eval = FALSE}
# Perform an LRT using the anova() function
anova(XXX, XXX)
```


```{r}
# Perform an LRT using the anova() function
anova(IO_mod, RI_mod)
```

 
#### Confidence Intervals

This method is not a formal way of testing significance, as it does not produce a p-value. In this method, confidence intervalls are obtained and checked if they include zero. If they don't, this is interpreted as a significant result.

4. We can obtain confidence intervals for our fixed effects using the `confint()` function of the `lme4` package. Do this by entering your `max_mod` into the `confint()` function.

```{r echo = TRUE, eval = FALSE}
# Compute confidence intervals for the fixed effects
ci_mod <- confint(XXX)
```


```{r}
# Compute confidence intervals for the fixed effects
ci_mod <- confint(max_mod)
```

5. Print the `ci_mod` object. Do the confidence intervals you obtained match the conclusion you drew from inspecting the t-values and the LRT?

#### Parametric Bootstrap

Yet another test of significance, and the one you should prefer in an uncertain case (i.e., if p-values are on the border of your alpha-level), is to use parametric bootstrap, where, many times, data is resampled with replacement and then the model fitted again, to obtain an empirical distribution of the effect or parameter of interest. The drawback of this technique is that it can take rather long because it reestimates the model many times to generate a distribution.


6. For linear mixed effects models (fit using `lmer()`) you can also use the `PBmodcomp()` function from the `pbkrtest` package. Check out the help function of `PBmodcomp()` like this.

```{r eval = FALSE, echo=TRUE}
?PBmodcomp
```

7. Now use the `PBmodcomp()` function to get p-values for the model. As you surely noticed from the help page, you will have to specify a smaller model against which to test. Use `IO_mod` for this. Also, because this procedure can take rather long, only use 100 simulations (when you perform your "real" analyses you may want to increase this number to 1000 or more).

```{r eval = FALSE, echo=TRUE}
# perform parametric bootstrap
pb_mod <- PBmodcomp(XXX, XXX, XXX = XXX)
```


```{r}
# perform parametric bootstrap
pb_mod <- bootMer(max_mod, IO_mod, nsim = 100)
```

8. Print the `pb_mod` object and inspect the results. Are the results different from the other tests you've performed before?
  
#### Other Tests

9. Another way of obtaining p-values with the t-statistic and a [Wald test](https://en.wikipedia.org/wiki/Wald_test) by using the `p_value()` function of the `sjstats` package. Enter `max_mod` to the `p_value()` function.

```{r echo = TRUE, eval = FALSE}
# Compute p-values for the fixed effects
p_value(XXX)
```

```{r}
# Compute p-values for the fixed effects
p_value(max_mod)
```

10. As this test may not be appropriate for mixed effects models, we can extend the `p_value()` function with an additional argument `p.kr = TRUE`. Then the function will compute p-values based on conditional F-tests with [Kenward-Roger approximation](https://www.jstatsoft.org/article/view/v059i09) for the degrees of freedom. Repeat the previous task by extending the function with `p.kr = TRUE` (Note that this approach is computationally rather expensive and thus will take a while to complete).


```{r}
# Compute p-values for the fixed effects with Kenward-Roger approximation
p_value(max_mod, p.kr = TRUE)
```

11. Compare the output of the tasks C9. and C10. Did things change? The reason why not is that the assumptions were all satisfied as the data was simulated with these assumptions, but with noisier real life data this may look different.

### D - Determining the "Significance" of Random Effects (Model Selection)

Just as we may be interested whether the fixed effects are significantly related to the outcome variable, we may also be interested whether the variance in the random effects is substantial, that is, whether it is significantly different from zero. This may help deciding whether a random effect should be included or not. You may remember the *keep it maximal* principle, which states that you should always specify the maximal random effects structure justified by the design to guard against type I error inflation ([Barr, Levy, Scheepers, & Tily, 2013](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3881361/)). However, the maximal model may come with a loss of power to detect fixed effects and it may make sense to select which random effects structure to use ([Matuschek, Kliegl, Vasishth, Baayen, & Bates, 2017](https://www.sciencedirect.com/science/article/pii/S0749596X17300013)). To express it with the words of Matuschek and Colleagues: "[A] parsimonious mixed model [...] containing only variance components supported by the data improves the balance between Type I error and power" (p. 305). To select which model to use, we will use LRTs.

When performing this LRT, we cannot use the usual alpha limit of .05. To see why, read the following quote from [Matuschek and colleagues (2017; p. 308)](https://www.sciencedirect.com/science/article/pii/S0749596X17300013)

> Within the context of model selection, it is important to resist the reflex of choosing $\alpha_{LRT} = 0.05$. The $\alpha_{LRT}$ cannot be interpreted as the ‘‘expected model-selection Type I error-rate” but rather as the relative weight of model complexity and goodness-of-fit. For example, choosing $\alpha_{LRT} = 0$, an infinite penalty on the model complexity is implied and consequently the minimal model is always chosen as the best, irrespective of the evidence provided by the data. Choosing $\alpha_{LRT} = 1$ implies an infinite penalty on the goodness-of-fit, and the maximal model is always chosen as the best. Therefore, choosing $\alpha_{LRT} = 0.05$ may imply an overly strong penalty on the model complexity and hence select a reduced model even if data favor a more complex one.

We will follow their example and use $\alpha_{LRT} = 0.2$. As the `anova()` function uses $\alpha = 0.05$ we will have to implement this procedure ourselves.


1. First, we have to fit the model whose complexity is one step lower compared to the maximal model. Think about which model this would be and write down the answer as a comment in your script. Don't cheat by looking at the next task!

```{r echo = TRUE}
# This is just a placeholder to increase the space to the next task to make it 
# easier to not cheat.

# There is nothing to see here...

# Just a random sidenote (which is actually very interesting, so you may want to
# to read on even if this kind of defies the purpos of this part to prevent you from
# looking at the answers first; so consider first completing the task and then reading
# the random but somehow terribly interesting sidenote):
#     Did you know that the reason why R uses the arrow "<-" as assignment
#     operator is because it is based on S, which in turn is based on APL.
#     Now apparently, APL was designed on a specific keyboard that had a
#     "<-" key and there was no "==" implemented to test equality. So equality
#     was tested using "=" and "<-" was chosen as asignment operator
#     (info obtained from this blogpost: https://colinfay.me/r-assignment/)
```


2. The answer to the last task is that you can constrain the correlation of the random intercepts and random slopes to zero. To do this you can use the double bar `||` in the random effects structure, that is `(design_matrix||grouping_variable)`. Fit the maximal model but constrain the correlations to zero. **Note**: We have to use a different optimizer here because otherwise the model fails to converge.

```{r echo = TRUE, eval = FALSE}
# Constrained mixed effects model with by-subject and by-movie random intercepts and slopes
con_mod <- lmer(XXX ~ XXX +             # These are the fixed effects
               (XXX||XXX) + (XXX||XXX), # These are the random effects
               data = XXX,              # Specify the data used
               REML = FALSE,
               control = lmerControl(optimizer = "bobyqa")) # use a different optimizer
                                                            # to avoid non convergence
```

```{r}
# Constrained mixed effects model with by-subject and by-movie random intercepts and slopes
con_mod <- lmer(Tomatometer ~ State +         # These are the fixed effects
               (State||ID) + (State||Movie),  # These are the random effects
               data = tom_df,                 # Specify the data used
               REML = FALSE,
               control = lmerControl(optimizer = "bobyqa")) # use a different optimizer
                                                            # to avoid non convergence
```

3. Now that we have both model outputs ready we can prepare the setup for the LRT. For this you need to know that the difference in the deviances of two models (which is just negative two times the log-likelihood) approximately follows a $\chi^2$ distribution. Therefore we can test the significance by testing if the $\chi^2$ value is larger than a threshold value. To do this we set up our $\alpha_{LRT}$ and derive the threshold value. Do this using the following code.

```{r echo = TRUE}
alpha <- .2 # set up the alpha value
st <- qchisq(1 - alpha, df = 2) # derive the critical value above which the difference
                                # is significant. We have two degrees of freedom because
                                # the constrained model has two more degrees of freedom 
```


4. No that we have our significance threshold `st` we can compute the difference in the deviances by subtracting the deviance from the more complex model from the constrained model. To do this, extract the deviances from the two models using the `deviance()` function from the `stats` package save the difference as `d_diff`.

```{r echo = TRUE, eval = FALSE}
d_diff <- deviance(XXX) - deviance(XXX)
```


```{r}
d_diff <- deviance(con_mod) - deviance(max_mod)
```

5. Now we can test whether the difference in deviance is large enough to be classified as a significant deviance according to our $\alpha_{LRT}$ level. To do this, we can simply test whether `d_diff` is larger than the significance threshold `st`. Do this and and look at the result.

```{r}
# Test whether the difference in the deviance between the two models is large enough
# to be considered significant
d_diff > st
```


6. The output of the previous task was `FALSE`. What does that mean? Do you keep the maximal model in this case? What would it mean if the result were `TRUE`, how would your next steps look?

```{r}
# FALSE as output in this case means that the difference between the models was
# not large enough that we can consider the maximal model to fit the data better.
```



### E - $R^2$ and Variances

When fitting statistical models, we are often interested in how much systematic variation they can capture. In linear (mixed effects) models this is the $R^2$ value, for generalized (mixed effects) models, we can compute pseudo $R^2$ values. The `sjstats` package comes with methods to obtain such *goodness-of fit measures* for regression models.

1. To obtain the $R^2$ of our model, we will use the `r2()` function from the `sjstats` package. Look at the help menue of the function to get an overview of what models you can use it for.

```{r}
?r2
```


2. Compute the $R^2$ of the maximal model. As you may have read in the help page, the marginal $R^2$ considers only the variance of the fixed effects, while the conditional $R^2$ takes both the fixed and random effects into account.

```{r}
r2(max_mod)
```

3. We may be interested in knowing the residual variance (the error; denoted as within group variance in the output), and the random effects variances. Extract this information from the maximal model output by using the `re_var()` function from the `sjstats` package.

```{r}
re_var(max_mod)
```

4. Look at the output from the last task. Are the different variances large? Is the within group variance or the between group variance larger and how do you interpret this information?


### F - Intra-Class Correlation

The intra-class correlation (ICC) is "the proportion of the variance explained by the grouping structure in the population" (Hox 2002, p.15). It is calculated by dividing the between-group variance by the sum of the within-group and the between-group variance (i.e., the total variance).

1. Compute the ICC of the maximal model using the `icc()` function from the `sjstats` package.

```{r}
icc(max_mod)
```

2. Take a look at the output. Did you notice the warning? This is because the ICC cannot be interpreted in the same way for random slopes and intercepts models. Add the argument `adjusted = TRUE` to your ICC call to take care of the warning (see the icc helppage for explanations).

```{r}
icc(max_mod, adjusted = TRUE)
```

3. Often, the ICC is computed on the intercept only model with random intercepts. In task C1. you have already fitted this model and saved it as `IO_mod`. Compute the ICC form this model. *Hint*: no need to use the `adjusted` argument, as no random slopes are involved.

```{r}
icc(IO_mod)
```

4. Now do the same for the `RI_mod`, that differs from the `IO_mod` you've just used only in that it also contains the fixed effect.

```{r}
icc(RI_mod)
```

5. Compare the outputs of the last two tasks. Did things change?

### G - Crossed Versus Nested Random Effects

So far we only had crossed random effects. Now we will also look at data with *nested random effects*, where every level of a nested factor only appears within a single level of a higher order factor. A popular example is the case of classes within schools. Every class is only part of a single school and thus class is nested within schools. To know whether random effects are crossed or nested, you usually need to know the design, as it the structure is often not obvious from the factor levels. For example, even though class may be nested in schools, the classes may be numbered from 1 to the number of classes in that school. So school 1 may have classes 1 to 10, and school 2 may have classes from 1 to 6. Obviously class 1 of school 1 is not the same as class 1 of school 2. What is obvious for you is not obvious for R. It doesn't know about the concept of schools and classes, so you have to tell it whether these factors are nested or not. Now consider the case where the classes are nested in schools, but numbered from 1 to total number of classes, that is, such that every class has a unique label. In this case it matters not what structur you specify (if this is confusing that's ok, there will be an example later on).

1. For now we will work with the `school` data set you have already loaded in section A. Using the `table()` function, create a cross table of school and class. (*Hint*: You'll have to enter the variables separately.)

```{r eval = FALSE, echo = TRUE}
table(XXX, XXX)
```

```{r}
table(schools$school, schools$class)
```

2. Fit a mixed effects model predicting extraversion ("extra") with openness ("open") and the social score ("social") with random intercepts for classes nested in school and save it as `hier_mod`. *Hint*: The nested random effects structure is specified with the following syntax `(1|higher_level_variable/lower_level_variable)`.

```{r}
# Hierarchical mixed effects model
hier_mod <- lmer(extra ~ open + social + # These are the fixed effects
               (1|school/class),        # These are the nested random effects
               data = schools,          # Specify the data used
               REML = FALSE)
```

3. Inspect the model output using `summary()`.

```{r}
summary(hier_mod)
```

4. Now fit the same model but with crossed random intercepts as in the sections before, rather than with nested random intercepts. Save the output as `cross_mod`.

```{r}
# Hierarchical mixed effects model
cross_mod <- lmer(extra ~ open + agree +  # These are the fixed effects
                 (1|school) + (1|class),  # These are the crossed random effects
                 data = schools,          # Specify the data used
                 REML = FALSE)
```

5. Inspect the output using the `summary()` function. Did the results change compared to the ones for the nested random effects structure.

```{r}
summary(cross_mod)

# There are changes in t-values, estimates and the goodness of fit
```

### H - Visualize Your Model

It is often usefull to visualize your model's output. Run the code below to extract coefficients from `max_mod` and plot the data along with a few sample lines to visualize the variabilita in the slopes and intercepts. As plotting is not the topic of this bootcamp we don't have time to go into details here. If you are interested in learning more about plotting, there are many books and tutorials about it, and we also cover it in the *R for Data Science* bootcamp.

1. Run the following code to visualize your `max_mod` data.


```{r echo = TRUE}
# extract fixed effects
m_line <- fixef(max_mod)

# extract random effects
ranefs <- ranef(max_mod)
predicted <- tibble(
  intercept = ranefs$ID[,1] + fixef(max_mod)[1],
  slope = ranefs$ID[,2] + fixef(max_mod)[2])

# randomly draw 15 subjects to plot the fitted lines
rand15 <- sample(1:nrow(predicted), 15)

LMM_plot <- ggplot(tom_df, aes(State, Tomatometer)) +
  geom_point(colour= "#606061", alpha = .15, size = 2.5)+
  geom_segment(aes(x = 1, y = intercept, xend = 2, yend = intercept + slope),
               data = predicted %>% slice(rand15), colour = "#EA4B68", size = 1.5,
               alpha = .8) +
  geom_segment(aes(x = 1, y = m_line[1], xend = 2, yend = sum(m_line)),
               colour = "black", size = 2, alpha = 1) +
  theme(axis.title.x = element_text(vjust = -1),
        axis.title.y = element_text(vjust = 1)) +
  theme_bw() +
  ylim(0, 100) +
  theme(
    strip.text = element_text(size = 12, face = "bold"),
    axis.text = element_text(size = 16),
    axis.title = element_text(size = 18,face = "bold")
  )

LMM_plot
```


### X - Advanced

1. In task D5. you found out that the constrained model with correlations between random effects set to zero is not significantly worse than the maximal model. Thus, if you were testing a confirmatory hypothesis, you should choose this model rather than the maximal model. What we didn't do in section D is to proceed with our backward selection procedure. This is now your task. Fit a model with by-subject random intercepts and slopes, but without their correlation, and with by-item random intercepts. 

```{r}
# Constrained model with by-subjects random intercepts and slopes, without their
# correlation, and with by-movie random intercepts
con_mod2 <- lmer(Tomatometer ~ State +         # These are the fixed effects
                (State||ID) + (1||Movie),      # These are the random effects
                 data = tom_df,                 # Specify the data used
                 REML = FALSE,
               control = lmerControl(optimizer = "bobyqa")) # use a different optimizer
                                                            # to avoid non convergence
```

2. Now perform the procedure of Section D. to evaluate which of the two models to keep. *Note*: You will have to set the `df` argument in the `qchisq()` function to `df = 1` because these two models only differ in one degree of freedom.

```{r}
# alpha is still the same
alpha <- .2 # set up the alpha value
st1 <- qchisq(1 - alpha, df = 1) # derive the critical value above which the difference
                                 # is significant. This time we have only one degree of
                                 # freedom because the more complex model only has one
                                 # parameter more than the the simpler model

# get the difference in deviance between the two models
d_diff <- deviance(con_mod2) - deviance(con_mod)

# Test whether this difference is large enough to be considered significant
d_diff > s1
```

3. So far we have only looked at linear mixed effects models. Let's change that and look at generalized linear mixed effects model. Load the `cancer_remission.csv` data into R and save it as `cr`.

```{r}
cr <- read_csv("1_Data/cancer_remission.csv")
```

4. Take a look at the data. It is simulated data from [UCLA Institute for Digital Research and Education Search this website](https://stats.idre.ucla.edu/).

```{r}
cr
summary(cr)
View(cr)
```

5. Predict cancer remission (`remission`) by the cancer stage (`CancerStage`), the length of the patients stay (`LengthofStay`), the doctors experience (`Experience`), and by specifying by doctor (`DID`) random intercepts. Use the `glmer` function with `family = binomial` to do this. Also, use the "bobyqa" optimizer as in some examples before.

```{r}
cr_mod <- glmer(remission ~ CancerStage + LengthofStay + Experience +
                (1 | DID), data = cr, family = binomial,
                control = glmerControl(optimizer = "bobyqa"), nAGQ = 10)
```

6. Extract the $R^2$ value for this model, once using the `r2()` and once using the `cod()` function. Are the results the same? Find out what the differences are by checking the help pages.

**Note**:
Running and interpreting generalized linerar mixed effects models can be quite challenging and we don't have the time here to cover tem. You can check out this [tutorial](https://stats.idre.ucla.edu/other/mult-pkg/introduction-to-generalized-linear-mixed-models/) or the references listed under resources if you want to learn more about this.


## Examples

```{r, eval = FALSE, echo = TRUE}

# load packages
library(lme4)
library(sjstats)

# look at sleepstudy data contained in lme4
head(sleepstudy)

# look at describtion of sleepstudy
?sleepstudy

# fixed effects only model predicting the reaction time
# from the number of days with sleep deprivation
sleep_FE <- glm(Reaction ~ Days, data = sleepstudy)

# inspect model
summary(sleep_FE)

# intercept only model mixed effects model
sleep_IO <- lmer(Reaction ~ 1 + (1|Subject),
                 data = sleepstudy,
                 REML = FALSE)

# inspect model
summary(sleep_IO)

# run random interceptes model
sleep_RI <- lmer(Reaction ~ Days + (1|Subject),
                 data = sleepstudy,
                 REML = FALSE)

# inspect output
summary(sleep_RI)

# check whether model converged
converge_ok(sleep_RI)

# get p_values for fixed effects using Kenward-Roger approximation
p_value(sleep_RI, p.kr = TRUE)

# alternatively, get p-value using anova() function
anova(sleep_IO, sleep_RI)

# run mixed effects model without correlations between random effects parameters
sleep_nc <- lmer(Reaction ~ Days + (Days||Subject),
                 data = sleepstudy,
                 REML = FALSE)

# check whether the model converged
converge_ok(sleep_nc)

# check model output
summary(sleep_nc)

# run maximal model (by-subjects random slopes and intercepts)
sleep_max <- lmer(Reaction ~ Days + (Days|Subject),
                 data = sleepstudy,
                 REML = FALSE)

# check whether the model converged
converge_ok(sleep_max)

# check model output
summary(sleep_max)

### select the random effects structure

# set alpha to .2
alpha <- .2

# derive the critical value above which the difference is significant.
st1 <- qchisq(1 - alpha, df = 1)

# get the difference in deviance between the two models
d_diff <- deviance(sleep_nc) - deviance(sleep_max)

# Test whether this difference is large enough to be considered significant
d_diff > st1

# correlation is not necessary. Now test whether the random slopes are necessary
d_diff <- deviance(sleep_RI) - deviance(sleep_nc)

# Test whether this difference is large enough to be considered significant
d_diff > st1 # -> keep sleep_nc model

# compute r-squared
r2(sleep_nc)

# compute icc
icc(sleep_nc, adjusted = TRUE)

### Visualize Model

# extract fixed effects
m_line <- fixef(sleep_nc)

# extract random effects
ranefs <- ranef(sleep_nc)
predicted <- tibble(
  intercept = ranefs$Subject[,1] + fixef(sleep_nc)[1],
  slope = ranefs$Subject[,2] + fixef(sleep_nc)[2])

# randomly draw 10 subjects to plot the fitted lines
rand10 <- sample(1:nrow(predicted), 10)

LMM_plot <- ggplot(sleepstudy, aes(Days, Reaction)) +
  geom_point(colour= "#606061", alpha = .15, size = 2.5)+
  geom_segment(aes(x = 0, y = intercept, xend = 9, yend = intercept + slope * 9),
               data = predicted %>% slice(rand10), colour = "#EA4B68", size = 1.5,
               alpha = .8) +
  geom_segment(aes(x = 0, y = m_line[1], xend = 9, yend = m_line[1] + 9 * m_line[2]),
               colour = "black", size = 2, alpha = 1) +
  theme(axis.title.x = element_text(vjust = -1),
        axis.title.y = element_text(vjust = 1)) +
  theme_bw() +
  theme(
    strip.text = element_text(size = 12, face = "bold"),
    axis.text = element_text(size = 16),
    axis.title = element_text(size = 18,face = "bold")
  )

LMM_plot
```


## Datasets

|File | Rows | Columns |
|:----|:-----|:------|
|[Tomatometer_dat.csv](https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/1_Data/Tomatometer_dat.csv?token=AZQb9iWcc8yUCbFwBliP5UfVzAooM3okks5cmQ96wA%3D%3D) | `r nrow(tom_df)` | `r ncol(tom_df)` |
|[schools.csv](https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/1_Data/schools.csv?token=AZQb9mdlio2aNiTgfaddDHxpENKWm3Hhks5cmQ9jwA%3D%3D) | `r nrow(schools)` | `r ncol(schools)` |
|[cancer_remission.csv](https://raw.githubusercontent.com/therbootcamp/SwR_2019Apr/master/1_Data/cancer_remission.csv?token=AZQb9ic1yv1-UdOtMITu0HyaW1xRMdV9ks5cmQ-bwA%3D%3D) | `r nrow(cr)` | `r ncol(cr)` |

The *Tomatometer_dat.csv* dataset contains tomatometer ratings from 200 raters who each rated 15 movies, once while they were sober and once while they were drunk.

The *schools.csv* dataset contains ratings of extraversion, openness to experience, agreeableness, and a social score of 1200 students from different classes of 6 different schools.

The *cancer_remission.csv* dataset contains data on lung cancer remission from patients nested within doctors

#### Tomatometer_dat.csv

| Name | Description |
|:-------------|:-------------------------------------|
| ID | Participant id code |
| Movie | Movie ID from M1 to M15 |
|State| the state in which the rating was done ("sober", or "drunk") |
|Tomatometer| tomatometer rating from 0 to 100|

#### schools.csv

| Name | Description |
|:-------------|:-------------------------------------|
| id | pupil id |
| extra | extraversion rating of pupil from 0 to 100|
|open| openness to experience rating of pupil from 0 to 100|
|agree| agreeableness rating of pupil from 0 to 100|
|social| social score of pupil |
|class| the class a pupil is in (nested under school; levels "a", "b", "c", and "d") |
|school| the school a pupil / class is in (levels "I", "II", "III", "IV") |

#### cancer_remission.csv

| Name | Description |
|:-------------|:-------------------------------------|
| remission | whether the cancer is in remission (0 = No, 1 = Yes) |
| CancerStage | four different cancer stages (levels "I", "II", "III", "IV") |
|LengthofStay| how long a participant stayed in hospital (score ranging from 1 to 10) |
|Experience| experiene of the doctor (ranging from 7 to 29, probably years of experience)|
|DID| doctor id |


## Functions

### Packages

|Package| Installation|
|:------|:------|
|`tidyverse`|`install.packages("tidyverse")`|
| `lme4` | `install.packages("lme4")` |
| `sjstats` | `install.packages("sjstats")` |
| `pbkrtest` | `install.packages("pbkrtest")` |

### Functions

| Function| Package | Description |
|:---|:------|:---------------------------------------------|
|     `lmer`|`lme4`| Fit a linear mixed effects model | 
|     `glmer`|`lme4`| Fit a generalized linear mixed effects model | 
|     `fixef`|`lme4`| Extract fixed effects coefficients from lmer or glmer output | 
|     `ranef`|`lme4`| Extract random effects coefficients from lmer or glmer output | 
|     `anova`|`stats`| Generic function to run (in this case) a likelihood ratio test | 
|     `confint`|`stats`| Compute confidence intervals for various statistical outputs | 
|     `deviance`|`stats`| Extract the deviance of various statistical outputs | 
|     `qchisq`|`stats`| Quantile function of the $\chi^2$ distribution to get critical $\chi^2$ values | 
|     `PBmodcomp`|`pbkrtest`| Perform parametric bootstrap to obtain p-values from linear mixed effects models | 
|     `converge_ok`|`sjstats`| Test whether a model converged or not (some warnings produced by lmer and glmer may be informative but not necessarily mean you need to worry, so this function is a good proxy to use. If it yields `FALSE` you may want to investigate things and try to take measures to improve your model) | 
|     `p_value`|`sjstats`| Compute p-values for fixe effects of mixed effects models using Wald's test or conditional F-tests with Kenward-Roger approximation for the degrees of freedom | 
|     `r2`|`sjstats`| Extract $R^2$ value from (generalized) linear (mixed effects) model outputs | 
|     `icc`|`sjstats`| Compute ICCs from (generalized) linear mixed effects model outputs | 
|     `re_var`|`sjstats`| Extract random effects and residual variances from (generalized) linear mixed effects model outputs | 

## Resources

### Further Reading

A nice and non-technical introduction to mixed effects models can be found in a chapter titled  [**An introduction to linear mixed modeling in experimental psychology**](http://davidkellen.org/wp-content/uploads/2017/04/introduction-mixed-models.pdf) by **Henrik Singmann** and **David Kellen**.

A more technical introduction can be found in the mixed effects models chapter of  [**Analysis of Longitudinal and Cluster-Correlated Data**](https://projecteuclid.org/euclid.cbms/1462106081) by **Nan Laird**.

A freely available tutorial on mixed effects models can be found [here](https://www.ssc.wisc.edu/sscc/pubs/MM/MM_Introduction.html).

An introduction to $R^2$ and ICC in mixed effects model can be found in the article
[The coefficient of determination R2 and intra-class correlation coefficient from generalized linear mixed-effects models revisited and expanded](https://royalsocietypublishing.org/doi/10.1098/rsif.2017.0213) by **Shinichi Nakagawa**, **Paul Johnson**, and **Holger Schielzeth**.

A rather technical introduction to mixed effects model with `lme4` is given in [**Fitting Linear Mixed-Effects Models Using lme4**](https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf) by **Douglas Bates**, **Martin Mächler**, **Benjamin Bolker**, and **Steven Walker**.

One of the classic introductions to regression models, including mixed effects models, is [**Data Analysis Using Regression and Multilevel/Hierarchical Models**](https://www.cambridge.org/ch/academic/subjects/statistics-probability/statistical-theory-and-methods/data-analysis-using-regression-and-multilevelhierarchical-models?format=PB&isbn=9780521686891) by **Andrew Gelman** and **Jennifer Hill**.

Another great introduction to regression, including mixed effects models, is [**Statistical Rethinking**](https://xcelab.net/rm/statistical-rethinking/) by **Richard McElreath**.

